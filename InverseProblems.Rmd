---
title: "The Structure of Inverse Problems in Experimental Particle Physics"
author: "Sean Gilligan"
output: 
  pdf_document:
    citation_package: biblatex
    number_sections: TRUE
keep_tex: TRUE
bibliography: citations.bib
header-includes:
  - \usepackage{setspace}\onehalfspacing
  - \usepackage{xcolor}
  - \usepackage{bm}
  - \usepackage{amsmath}
  - \usepackage{parskip}
  - \usepackage{hyperref}
  - \usepackage{csquotes}
  - \usepackage{float}
  - \usepackage{wrapfig}
  - \hypersetup{colorlinks=TRUE,linkcolor=red,citecolor=blue,filecolor=magenta,urlcolor=blue}
  - \newcommand{\comment}[1]{}
abstract: \singlespacing This report provides a survey of some of the common methods used by the high energy physics community to understand and solve ill-posed inverse problems as they pertain to signal distortions that result from imperfect measuring devices and processes. These methods are in general collectively referred to as unfolding. The specifics of data and data collection methods are generalized. Common features are discussed insofar as they contribute to the necessary understanding of the data and implementation of any covered unfolding methods. In order to construct a slightly more wholistic picture some additional topics are briefly touched upon if they relate to other common aspects of data analysis in particle physics, but only during parts of relevant discussions where they would otherwise normally appear.
fontsize: 12pt
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

```{r, message = F, echo=F}
library(tidyverse)
library(gridExtra)
library(grid)
library(gridtext)
library(gtable)
library(knitr)
library(RColorBrewer)
library(viridisLite)
library(viridis)
library(gtools)
library(cowplot)
library(egg)
library(scales)
library(ggforce)
library(latex2exp)
```

\section{Introduction} \label{intro}

A common problem faced in the quantitative sciences and their associated technologies is the introduction of errors during the data collection process. While the possible sources of these errors are as varied as the possible events which the data might describe, significant work has been done to develop methods the can help would-be analysts reconcile them. The requisite understanding of a scenario's underlying systematic and stochastic processes might not allow researchers to truly reverse entropy or make up for the finite resolution of a detector, but it can approximate them with a quantifiable degree of certainty. 

The applied mathematics that this involves falls within the general category of \textbf{inverse problems}, and there are a variety of labels used to refer to the procedures in its arsenal. There is the colloquially vague \textbf{unsmearing}, but there are also names that reference specific applications and methods. For the sake of simplicity, and any necessary physical constraints, the manner of inverse problems addressed here will only have satisfactory solutions that involve linear operations that map from one Hilbert space\footnote{The definition of a Hilbert space is provided in Appendix \ref{appHilbert} for convenience.} to another. Symbolically this can be expressed by the equation
\begin{align}
Az=u,\nonumber
\end{align} 
where $A$ is a linear operator acting on an element $z\in Z$, the sought solution, to produce an element $u\in U$, the observed data. Within the context of the methods described herein $z$
and $u$ take the form of continuous or discrete distributions that when integrated or summed over the domain of their arguments result in finite real quantities.

The difficulty of solving for $z$ can be classified into one of two camps. The easiest cases involve conditions that create a \textbf{well-posed} problem, which requires that \cite{Yagola2011ch2}
\begin{enumerate}
  \item a solution exists $\forall u\in U$,
  \item the solution is unique,
  \item and if $u_n\longrightarrow u$, $Az_n\longrightarrow u_n$, and $Az\longrightarrow u$, then $z_n\longrightarrow z$.
\end{enumerate}
Conditions 1 and 2 work together to imply that the inverse operator
$A^{-1}$ exists, and Condition 3 is often worded to describe the inverse
as continuous, which implies that small deviations in $u$ should
correspond to similar deviations in $z$. When one or more of
these conditions are not met, the problem is said to be
\textbf{ill-posed}, and some of the consequences of assuming otherwise
should hopefully become clear in the coming pages.

Entire books have been written on this subject that do not begin to cover the full scope
of the methods developed to deal with ill-posed problems. With that in mind the hope for this short paper is for it to serve as an introduction to ill-posed problems while providing some degree of direction for those who would like to know more.

\subsection{The Deconvolution}\label{deconvolution}

One way to characterize a basic example of a situation suitable for
being treated as a convolution would be one that should be very familiar
to anyone who has ever taken statistics course. Assume that data
collected regarding $n$ statistical events represent the measurement of
$n$ independent and identically distributed (i.i.d.) random variables
$\bm{X}=\{X_1,X_2,\dots,X_n\}$ from a distribution of possible values
represented by the probability density function (PDF) $f_X(x)$, such
that the probability of a random variable $X_i$ having a value between
$x_a$ and $x_b$ is $$P(\:x_a<X_i<x_b\:)=\int_{x_a}^{x_b}f_X(x)\,dx$$ and
$$\int_\mathcal{X}f_X(x)\,dx=1,$$ where $\mathcal{X}$ represents the
domain of $x$. The error introduced during the measurement process is
similarly represented by a set of i.i.d. random variables
$\bm{\varepsilon}=\{\varepsilon_1,\varepsilon_2,\dots,\varepsilon_n\}$
with a PDF $f_\varepsilon(\varepsilon)$, where the sets
$\bm{\varepsilon}$ and $\bm{X}$ are typically assumed to be independent
of each other. The set of measured/reconstructed values
$\bm{Y}=\{Y_1,Y_2,\dots,Y_n\}$ then are also i.i.d. and can be defined
in terms of the preceding sets of variables such that for event
$i\in\{1,\dots,n\}$,
\begin{align}Y_i&=g(X_i,\varepsilon_i)\nonumber\\&=X_i+\varepsilon_i.\label{eq:meas}\end{align}
In light of this relationship, the corresponding PDF $f_Y(y)$ can be
found explicitly through an operation on $f_X(x)$ and
$f_\varepsilon(\varepsilon)$ using the mathematics of functional
analysis. Stated in more general terms, the empirical density function
$f_Y$ is formed from the \textbf{convolution} of the true density
function $f_X$ and the error density function $f_\varepsilon$, and is
defined by \cite{Panaretos2011}
\begin{align}f_Y&\equiv f_X*f_\varepsilon\label{eq:conv1}\\f_Y(y)&\equiv\int_\mathcal{X}f_X(x)f_\varepsilon(\varepsilon)\,dx\nonumber\\&=\int_\mathcal{X}f_X(x)f_\varepsilon\big(g_x^{-1}(y)\big)\left\vert J_{g_x^{-1}}(y)\right\vert dx\nonumber\\&=\int_\mathcal{X}f_X(x)f_\varepsilon(y-x)\,dx,\label{eq:conv2}\end{align}
where $J$ represents the Jacobian of the transformation involved in
performing the change of basis on $f_\varepsilon$ from $\varepsilon$ to
$x$, which is necessary for the evaluation of the integral for a given
$y$. The magnitude of the Jacobian for transformation of $\varepsilon$
to $y-x$ through the manipulation of Equation \eqref{eq:meas} happens to
be $1$.

As the collection of measured values $\bm{Y}$ accumulates an estimate of
empirical density $\hat{f}_Y$ can readily be formed. However, a major
goal in an analysis of data like this is typically to develop an
accurate estimate of the true density $\hat{f}_X$. Using the information
contained in $\hat{f}_Y$ to accomplish this necessarily requires some
attempt at finding an inverse process to the convolution, i.e. the
\textbf{deconvolution}.

For cases in the form of this particular example there are a variety
approaches, but they commonly involve the Fourier transform of the
density functions $\left\{f_X,f_\varepsilon,f_Y\right\}$ into their
corresponding characteristic functions
$\left\{\phi_X,\phi_\varepsilon,\phi_Y\right\}$
\cite{Meister2009}\cite{Panaretos2011}. Minor aspects of the definition
for the Fourier transform can vary slightly between applications,
resulting primarily from the use of different scale factors and sign
conventions. Here it will be defined for some random variable
$U\in\mathbb{R}$ with density function $f_U(u)$ and random variable
$T\in\mathbb{R}$ as
\begin{align}\phi_T(t)=\int_{-\infty}^\infty f_U(u)\,e^{itu}\,du.\label{eq:ft}\end{align}
When conditions permit the inverse Fourier transform can be found via
\begin{align}f_U(u)=\int_{-\infty}^\infty \phi_T(t)\,e^{-itu}\,dt.\label{eq:ift}\end{align}
The Fourier transform is important in deconvolution methods because when
you apply it to the convolution of two density functions the link
between their respective characteristic functions becomes purely
multiplicative, i.e.
$$f_Y=f_X*f_\varepsilon\implies\phi_Y=\phi_X\phi_\varepsilon.$$ An
instructional proof of this result is provided on page 447 of
\cite{Boas2005}. The steps so far characterize a typical deconvolution
scheme, with later steps consisting of various ways to perform density
estimation and addressing issues similar to those that will be seen
ahead \cite{Meister2009}.

\subsection{Generalizing}\label{general}

The remainder of this paper is dedicated to a more generalized study of
these type of problems. With the understanding that even experts can be
fairly loose and inconsistent with their vocabulary, this paper will do
its best to provide clear definitions. To begin, while most literature
on deconvolution methods do use the word "convolution", this operation
is also referred to by the German word \textit{faltung}
\cite{Weisstein}. The latter's English translation, \textbf{folding}, is
featured prominently in the particle physics community, but refers to a
more generalized process than what is described by Equation
\eqref{eq:conv2} \cite{DAgostini1994}\cite{Adye2011}\cite{Blobel2013}.
In general, folding and \textbf{unfolding} refer to two sets of
processes within which the sets of convolution and deconvolution
processes form proper respective subsets.

One way to arrive at the desired generalization is with the help of
conditional probability. Thinking of $\{X,Y\}$ as a continuous bivariate
random vector with joint PDF $f(x,y)$ and marginal PDFs $f_X(x)$ and
$f_Y(y)$, we can define the conditional PDF of $Y$ given that $X=x$ as
function of $y$, $f(y\,\vert x)$ \cite{Casella2001}. The relationship
between these PDFs is sufficient to define any one of them in terms of
operations involving one or more of the others. As such, for $f_Y(y)$ it
can be shown
\begin{align}f_Y(y)&=\int_\mathcal{X}f(x,y)\,dx\nonumber\\&=\int_\mathcal{X}f(y\,\vert x)f_X(x)\,dx\nonumber\\&=\int_\mathcal{X}K(x,y)f_X(x)\,dx.\label{eq:fred}\end{align}
While integrating over $x$, $f(y\,\vert x)$ is implicitly treated as a
function of both $x$ and $y$. Acknowledging this allows for
understanding Equation \eqref{eq:fred} as a Fredholm integral of the
first kind with a Kernel function $K(x,y)$ that reflects the physical
measurement process \cite{Blobel2011}. The relationship between $x$ and
$y$ in $K(x,y)$ is not defined, but when the kernel is a function of the
difference of its arguments, such that $K(x,y)=K(y-x)$, Equation
\eqref{eq:fred} becomes the convolution described in Equation
\eqref{eq:conv2}.

In particle physics experiments, analysts make use of Monte-Carlo (MC)
simulations to estimate detector response to randoms samples from some
true distribution $f_X(x)^{\text{MC}}$, which is itself estimated by way
of MC simulations using models that typically contain theory being
tested by the experiment in question. The resulting measured
distribution $f_Y(y)^{\text{MC}}$ grants implicit knowledge of $K(x,y)$
by way of Equation \eqref{eq:fred} \cite{Blobel2013}. Finding the
inverse of this Kernel is then the goal, as it should in theory allow
for the mapping of experimental observations $\bm Y$, as randomly
sampled from $f_Y(y)$, back to their true values $\bm X$.

\subsection{Discretization} \label{discret}

In practice researchers are only ever dealing with estimates $\hat f_X$,
$\hat f_Y$, $\hat f_X^{\text{MC}}$, and $\hat f_Y^{\text{MC}}$, and the
sets of data that contribute to these estimates are organized by bin
into histograms that form unnormalized granular approximations of their
true distributions. Thinking in terms of these histograms allows for the
reformulation of Equation \eqref{eq:fred} into the linear matrix
equation: \begin{align}\bm\nu = \bm{R}\bm\mu.\label{eq:mat}\end{align}
The vectors $\bm\nu$, $\bm\mu$ and matrix $\bm{R}$ relate to their
continuous counterparts by \cite{Blobel2013}: \begin{align}
  \text{true distribution }f_X(x)&\longrightarrow\bm\mu\,\in\,\{\mathcal{U}\equiv\mathbb{R}^M_{+}\cup\bm{0}\}\text{ the unknown true bin counts,}\nonumber\\
  \text{measured distribution }f_Y(y)&\longrightarrow\bm\nu\,\in\,\mathcal{V}\equiv\{\mathbb{R}^N_{+}\cup\bm{0}\}\text{ the measured bin counts,}\nonumber\\
  \text{Kernel }K(x,y)&\longrightarrow\bm{R}\;\;\text{the rectangular }N\text{-by-}M\text{ \bf response matrix}\text{.}\nonumber
\end{align} The components of vectors $\bm\nu$ and $\bm\mu$ represent
the number of events that have occurred within the regions of $x$ and
$y$ that define the components' corresponding bins. For $i=1,\dots,N$
and $j=1,\dots,M$ the components of matrix $\bm R$ are defined by the
conditional probability \cite{Cowan1998} \begin{align}
  R_{ij}&=P(\text{measured value in bin }i\vert\text{true value in bin }j)\nonumber\\
        &=\frac{P(\text{measured value in bin }i\text{ and true value in bin }j)}{P(\text{true value in bin }j)}\nonumber\\
        &=\frac{\int_{\text{bin }i}\int_{\text{bin }j}K(x,y)f_X(x)dx\,dy}{\int_{\text{bin }j}dx\,f_X(x)}\nonumber\\
        &\equiv P(\nu_i\vert\mu_j).\label{eq:Rij}
\end{align} In terms of $P(\nu_i\vert\mu_j)$ the full response matrix
then has the form \begin{align}
  \bm{R}=\begin{pmatrix}
    P(\nu_1\vert\mu_1)     & P(\nu_1\vert\mu_2)     & \dots  & P(\nu_1\vert\mu_{N})   \\
    P(\nu_2\vert\mu_1)     & P(\nu_2\vert\mu_2)     & \cdots & P(\nu_2\vert\mu_{N})   \\
    \vdots                 & \vdots                 & \ddots & \vdots                 \\
    P(\nu_{M}\vert\mu_1)   & P(\nu_{M}\vert\mu_2)   & \dots  & P(\nu_{M}\vert\mu_{N})
  \end{pmatrix}.\label{eq:Rmat}
\end{align} With these definitions Equation \eqref{eq:mat} tells us that
an event produced in bin $\mu_j$ has some probability $\geq 0$ of being
measured in each of the $N$ bins of $\bm\nu$, and that each bin count
$\nu_i$ receives potential contributions from each of the $M$ bins in
$\bm\mu$, i.e.
\begin{align}
  \nu_i &= \sum_{j=1}^MR_{ij}\mu_j\;\text{     and}\label{eq:bini}\\
  R_{ij}&=\frac{\partial\nu_i}{\partial\mu_j}\label{dnudmu}.
\end{align}
The number of bins are typically set such that $M\leq N$, with the
convention $N=M+1$ being common. A higher number of bins in the measured
distribution reflects that the measuring process is expected to map some
events in $\bm X$ to values of $\bm Y$ that are outside the region of
values that define the initial $M$ bins. These one or more extra bins
are intended to account for all the possible values that a particular
event could be mapped to, such that for a given event starting in bin
$j$ one might expect the probabilities of it being measured in each of
the $N$ final bins to sum to $1$.

However, in practice there are a variety of constraints on events that
can either result in them not being included for analysis or even
prevent them from being detected at all. For example, an analyst might
cut events observed in regions of a detector that result in insufficient
data collection, or maybe some event information carriers miss the
detector entirely, resulting in such events going unseen. In either case
the effect of these missing events is described using the detector
\textbf{efficiency}, and represented mathematically by the $N$-vector
$\bm\epsilon$, where component $\epsilon_j$ is the efficiency of the
$j$th true bin
defined\footnote{In the continuous case it is typically written as $\epsilon(x)$, and understood to be the conditional probability of an event producing any measured value given it has a true value of $x$. It is typically absorbed into $K(x,y)$ where it goes on to manifest within $\bm R$ in the manner shown in Equation \eqref{eq:eff} \cite{Blobel2013}.}
by \cite{Cowan1998}:
\begin{align}\sum_{i=1}^{N}P(\nu_i\vert\mu_j)=\sum_{i=1}^{N}R_{ij}=\epsilon_j\leq 1.\label{eq:eff}\end{align}
In contrast to this are contributions to measured counts from
\textbf{background} processes. Just as events produced in a region of
interest can be smeared out of it, events produced out of it can be
smeared into it. The crossed barrier could correspond to the variable of
interest, but it can also include events excluded from analysis due to
assigned constraints on other variables that describe the event.
Background processes are often studied and dealt with prior to the
unfolding procedures described in the paper. It is briefly mentioned
here to provide a slightly more holistic picture of particle physics
analysese. Mathematically, background would be included by modifying
Equation \eqref{eq:bini} to read
\begin{align}\nu_i = \sum_{j=1}^MR_{ij}\mu_j+\beta_i,\end{align} where
$\beta_i$ is the $i$th component of the $N$-vector $\bm\beta$, which
represents the binned background counts. This leads to equations like
$\nu_i^{\text{sig}}=\nu_i-\beta_i$ in order to specify the expected
number of measured counts that are from the signal of interest. Going
forward background will be assumed to already have been accounted for,
and $\nu_i$ will refer to the expected signal counts of bin $i$.

As all these variables so far have been derived from the exact
continuous distributions $f_X(x)$ and $f_Y(y)$, they correspond to the
expectation values that researchers are estimating during data
collection and analysis. As this is a counting process the components of
the observed number of signal events $\bm{n}$, an $N$-vector, are often
related to the components of the expected number of observed counts
$\bm\nu$ as a collection of $N$ separate and independent Poisson
processes. That is to say the observed counts $n_i$ in bin $i$ are
treated as i.i.d. random variables with the probability mass function
\begin{align}P(n_i\vert\nu_i)=\frac{\nu_i^{n_i}e^{-\nu_i}}{n_i!}.\label{eq:pois}\end{align}
As such counts $n_i$ form the estimate $\hat\nu_i$ of
the expected counts $\nu_i$ by
\begin{align}\nu_i&=\text{E}[\hat\nu_i]=\text{E}[n_i]\nonumber\\&=\text{Cov}[\hat\nu_i]=\text{Cov}[n_i].\nonumber\end{align}
Understanding the probability distribution of $\bm{n}$ allows for
unfolding methods that involve the use of maximum likelihood estimation.
It will be convenient, and necessary for methods based on leaste-squares, to estimate the
covariance matrix $\bm{\hat\Sigma}_{\nu}$ (written $\hat\Sigma^{\nu}_{ij}$ when referring to components) of the observations, which for
independent Poisson processes has components of the form
\begin{align}
  \hat\Sigma^{\nu}_{ij}
      &=\text{Cov}[\hat\nu_i,\hat\nu_j]\nonumber\\
      &=\text{Cov}[n_i,n_j]\nonumber\\
      &=\delta_{ij}n_i,\label{eq:cov}\end{align}
where $\delta_{ij}$ is the Kronecker
delta\footnote{The Kronecker delta $\delta_{ij}$ is a piecewise function of variables $i$ and $j$ defined by $\delta_{ij}=\begin{cases}0\;\;\;\text{if }i\neq j\\1\;\;\;\text{if }i=j\end{cases}.$}.
The path to an estimated covariance matrix of the estimated true distribution
$\bm{\hat\mu}$, itself a function of $\bm{n}$ and $\bm{\nu}$ (or its estimate), can be considered briefly by considering the maximum
log-likelihood, where it can be shown \begin{align}
  \log L(\bm\mu)
    &=\sum_{i=1}^N\log\left(\frac{\nu_i^{n_i}e^{-\nu_i}}{n_i!}\right)\nonumber\\
    &=\sum_{i=1}^N\left(n_i\log\nu_i-\nu_i-\log n_i!\right)\nonumber\\
  \frac{\partial\log L}{\partial\mu_k}
    &=\sum_{i=1}^N\frac{\partial\log L}{\partial\nu_i}\frac{\partial\nu_i}{\partial\mu_k}\nonumber\\
    &=\sum_{i=1}^N\left(\frac{n_i}{\nu_i}-1\right)R_{ik}=0.\nonumber
\end{align} Some minor algebra here thankfully reproduces the estimate
$\bm{\hat\nu}=\bm{n}$, as expected from an earlier observation about $n_i$. Continuing
with an additional derivative shows 
\begin{align}
  \frac{\partial^2\log L}{\partial\mu_k\partial\mu_l}
    &=-\sum_{i=1}^N\left(\frac{n_i}{\nu_i^2}\frac{\partial\nu_i}{\partial\mu_l}\right) R_{ik}\nonumber\\
    &=-\sum_{i=1}^N\frac{n_i R_{il}R_{ik}}{\nu_i^2},\nonumber
\end{align} the negative of the expectation value of which is the Fisher information matrix $\bm{\mathcal{I}}(\bm\mu)$. Since the Fisher information's relationship with the Cram$\acute{\text{e}}$r-Rao lower bound can, as its name implies, be used to determine the lower bound on the covariance matrix of an estimator of $\bm{\mu}$, one can show for one such \emph{unbiased} estimator, say $\bm{T}(\bm{n})$, that
\begin{align}
  \text{Cov}_{\!\bm{\mu}}\!\left[\bm{T}(\bm{n})\right]\geq \bm{\mathcal{I}}(\bm{\mu})^{-1}
    &=\left(-E\left[\frac{\partial^2\log L}{\partial\mu_k\partial\mu_l}\right]\right)^{-1}\nonumber\\
    &=\left(\sum_{i=1}^N\frac{E[n_i] R_{il}R_{ik}}{\nu_i^2}\right)^{-1}\nonumber\\
    &=\left(\sum_{i=1}^N\frac{R_{il}R_{ik}}{\nu_i}\right)^{-1}\nonumber\\
    &=\left(\bm{R}^T\bm{\Sigma}_\nu^{-1}\bm{R}\right)^{-1}\nonumber\\
    &=\bm{R}^{-1}\bm{\Sigma}_\nu(\bm{R}^{-1})^{T}.\label{CRlb}
\end{align}
Indeed, this matrix must then be lower bound for the covariance matrix of any unbiased estimator of $\bm{\mu}$. This is some good insight to have before getting into the weeds of working on actual data.

\subsection{A Simulated Example}\label{simulation}

Consider the following three sets of i.i.d. random variables from separate Cauchy distributions.
\begin{align}
  X_{1,i}&\sim \text{Cauchy}(x_0^{(1)},\gamma_1)\nonumber\\
  X_{2,i}&\sim \text{Cauchy}(x_0^{(2)},\gamma_2)\nonumber\\
  X_{3,i}&\sim \text{Cauchy}(x_0^{(3)},\gamma_3)\nonumber
\end{align} 
Current models predict that some class of physics events observed in past detectors are solely coming from the first two processes (Model 1), such that for $n$ events the number coming from the first process is an i.i.d. random variable from the binomial distribution $B(n,p)$. Meanwhile, a new model (Model 2) has been developed that suggests that the third process has been occurring this whole time but has been incorrectly categorized as one or the other of the first two. Napkin math has estimated a contribution rate that is reflective of some probability $p_3$, such that the binomial distribution is actually a multinomial distribution with probabilities $\bm p=\{p_1,p_2,p_3\}$.

A new experiment is being designed and funded to test this new theory, on top of many others, and simulations are being performed to give analyzers plenty of opportunities to develop their collaboration's analysis framework, perform calibrations, and make ready a myriad of studies that hope to shed light on many an unanswered question. The corresponding MC simulations performed during this time include such simulations for the physics events of interest, but also for the detector. Fpr the purposes of this paper a relatively simple set of simulations have been performed, leading to 400,000 simulated events per theory that are meant to represent the MC simulations, as well as a set of 20,000 simulated events for each theory that are meant to represent hypothetical detector data.

```{r, echo = F}
set.seed(12321)
# Assume 10000 events
nsim_data <- 20000
dmrat <- 20
nsim_mc <- dmrat*nsim_data

# Two possible types of processes with different means but equal variances
loctn <- c(11,18,14)
scale <- c(4,4,5)
p <- 0.25
p1 <- c(0.3,0.7)
p2 <- c((1-p)*p1[1],(1-p)*p1[2],p)

events_data1 <- c(rmultinom(1, nsim_data, p1))
events_data2 <- c(rmultinom(1, nsim_data, p2))

events_mc1 <- c(rmultinom(1, nsim_mc, p1))
events_mc2 <- c(rmultinom(1, nsim_mc, p2))

x1_data1 <- rcauchy(events_data1[1], location = loctn[1], scale = scale[1])
x2_data1 <- rcauchy(events_data1[2], location = loctn[2], scale = scale[2])
x1_data2 <- rcauchy(events_data2[1], location = loctn[1], scale = scale[1])
x2_data2 <- rcauchy(events_data2[2], location = loctn[2], scale = scale[2])
x3_data2 <- rcauchy(events_data2[3], location = loctn[3], scale = scale[3])
x1_mc1 <- rcauchy(events_mc1[1], location = loctn[1], scale = scale[1])
x2_mc1 <- rcauchy(events_mc1[2], location = loctn[2], scale = scale[2])
x1_mc2 <- rcauchy(events_mc2[1], location = loctn[1], scale = scale[1])
x2_mc2 <- rcauchy(events_mc2[2], location = loctn[2], scale = scale[2])
x3_mc2 <- rcauchy(events_mc2[3], location = loctn[3], scale = scale[3])

x_data <- c(x1_data1,x2_data1,
            x1_data2,x2_data2,x3_data2)
x_mc <- c(x1_mc1,x2_mc1,
          x1_mc2,x2_mc2,x3_mc2)

# Efficiency
xdetected_data <- rbernoulli(nsim_data,1-exp(-sqrt(abs(x_data))/4)) == 1
xdetected_mc <- rbernoulli(nsim_mc,1-exp(-sqrt(abs(x_mc))/4)) == 1

# Add Smearing
err_mu_data <- -abs(x_data)^(1/4)
err_sd_data <- log((abs(x_data)+10)/4)
y_data <- x_data + rnorm(2*nsim_data, mean = err_mu_data, sd = err_sd_data)

err_mu_mc <- -abs(x_mc)^(1/4)
err_sd_mc <- log((abs(x_mc)+10)/4)
y_mc <- x_mc + rnorm(2*nsim_mc, mean = err_mu_mc, sd = err_sd_mc)

# Binned representation
sims_data <- tibble("Model" = 
                      rep(c("Model 1","Model 2"),
                          each = nsim_data),
                    "Truth" = x_data,
                    "Reconstructed" = y_data,
                    "Detected" = xdetected_data)
sims_mc <- tibble("Model" = 
                    rep(c("Model 1","Model 2"),
                        each = nsim_mc),
                  "Truth" = x_mc,
                  "Reconstructed" = y_mc,
                  "Detected" = xdetected_mc)

bins_data <- sims_data %>%
  filter(Truth <= 30 & Truth >= 0) %>%
  pivot_longer(c(Reconstructed,Truth), 
               names_to = "Treatment", 
               values_to = "Bin") %>%
  mutate(Bin = ceiling(Bin)) %>%
  filter(Bin <= 30 & Bin >= 1) %>%
  filter((Treatment == "Reconstructed" & Detected == TRUE) | Treatment == "Truth") %>%
  count(Model, Bin, Treatment, name = "Counts") %>%
  complete(Model, Bin=1:30, Treatment, fill = list(Counts=0)) %>%
  mutate(LBin = Bin-1,
         LCounts = cbind(
           rbind(diag(rep(1,62))[c(3,4,3:58,61,62),-c(1,2)],
                 diag(0,60)),
           rbind(diag(0,60),
                 diag(rep(1,62))[c(3,4,3:58,61,62),-c(1,2)])) %*% 
           Counts) %>%
  mutate(Density = Counts/nsim_data,
         LDensity = LCounts/nsim_data) %>%
  select(Model, Treatment,LBin,Bin,LCounts,Counts,LDensity,Density)

bins_mc <- sims_mc %>%
  filter(Truth <= 30 & Truth >= 0) %>%
  pivot_longer(c(Reconstructed,Truth), 
               names_to = "Treatment", 
               values_to = "Bin") %>%
  mutate(Bin = ceiling(Bin)) %>%
  filter(Bin <= 30 & Bin >= 1) %>%
  filter((Treatment == "Reconstructed" & Detected == TRUE) | Treatment == "Truth") %>%
  count(Model, Bin, Treatment, name = "Counts") %>%
  complete(Model, Bin=1:30,  Treatment, fill = list(Counts=0)) %>%
  mutate(LBin = Bin-1,
         LCounts = cbind(
           rbind(diag(rep(1,62))[c(3,4,3:58,61,62),-c(1,2)],diag(0,60)),
           rbind(diag(0,60),diag(rep(1,62))[c(3,4,3:60),-c(1,2)])) %*% 
           Counts) %>%
  mutate(Density = Counts/nsim_data,
         LDensity = LCounts/nsim_data) %>%
  select(Model,Treatment,LBin,Bin,LCounts,Counts,LDensity,Density)

bins_all <- rbind(bins_data,bins_mc) %>%
  mutate(Source = c(rep("Data",120),rep("MC",120)),
         LCounts = LCounts/c(rep(1,120),rep(dmrat,120)),
         Counts = Counts/c(rep(1,120),rep(dmrat,120))) %>%
  mutate(Name = paste(Treatment,Source)) %>%
  filter(Source == "MC" | Treatment == "Reconstructed")

bins_all$Name[which(bins_all$Source == "Data")] <- "Reconstructed Data"

#max(sims_mc$Truth,sims_mc$Reconstructed,sims_data$Truth,sims_data$Reconstructed)
```

```{r, echo = F}
# Continuous representation
X <- seq(0,30,0.01)
nx <- length(X)

fp1x <- dcauchy(X, location = loctn[1], scale = scale[1]) 
fp2x <- dcauchy(X, location = loctn[2], scale = scale[2])
fp3x <- dcauchy(X, location = loctn[3], scale = scale[3])
#fx <- p*dlnorm(X, meanlog = 2.25, sdlog = 0.25) +
#  (1-p)*dlnorm(X, meanlog = 2.8, sdlog = 0.15)

fpx_truth1 <- tibble(X = rep(X,2), Density = c(p1[1]*fp1x,p1[2]*fp2x), 
                     Process = rep(c("Process 1","Process 2"), each = nx),
                     Model = rep("Model 1",2*nx))
fpx_truth2 <- tibble(X = rep(X,3), Density = c(p2[1]*fp1x,p2[2]*fp2x,p2[3]*fp3x), 
                     Process = rep(c("Process 1","Process 2","Process 3"), each = nx),
                     Model = rep("Model 2",3*nx))
fx_truth1 <- tibble(X = X, Density = p1[1]*fp1x + p1[2]*fp2x, 
                    Model = rep("Model 1",nx),Treatment = rep("Truth",nx))
fx_truth2 <- tibble(X = X, Density = p2[1]*fp1x + p2[2]*fp2x + p2[3]*fp3x,
                    Model = rep("Model 2",nx),Treatment = rep("Truth",nx))
fy_truth1 <- read.csv("f1yEstimate.csv") %>% 
  mutate(Model = "Model 1", Treatment = "Reconstructed")
fy_truth2 <- read.csv("f2yEstimate.csv") %>% 
  mutate(Model = "Model 2", Treatment = "Reconstructed")
names(fy_truth1)[1] <- names(fx_truth1)[1] <- 
  names(fy_truth2)[1] <- names(fx_truth2)[1] <- "XY"

# Binned expected counts
bins_expected1 <- read.csv("hist1Expected.csv") %>%
  mutate(LDensity = LCounts,
         Density = Counts,
         LCounts = LCounts*nsim_data,
         Counts = Counts*nsim_data)
bins_expected2 <- read.csv("hist2Expected.csv") %>%
  mutate(LDensity = LCounts,
         Density = Counts,
         LCounts = LCounts*nsim_data,
         Counts = Counts*nsim_data)

# Counts max
fmax_density <- 1.05*max(c(bins_data$Density,bins_mc$Density,
                           fx_truth1$Density,fx_truth2$Density,
                           fy_truth1$Density,fy_truth2$Density,
                           bins_expected1$Density,bins_expected2$Density))
fmax_count <- 1.05*max(c(bins_data$Counts,bins_mc$Counts/dmrat,
                         fx_truth1$Density*nsim_data,fx_truth2$Density*nsim_data,
                         fy_truth1$Density*nsim_data,fy_truth2$Density*nsim_data,
                         bins_expected1$Counts,bins_expected2$Counts))
```

```{r, fig.height=7, fig.width=7.5, fig.align='center', echo = F}
continuous2 <- ggplot() + 
  theme_bw() +
  geom_line(data = rbind(fx_truth1,fx_truth2,
                         fy_truth1,fy_truth2) %>%
              filter(Model == "Model 1"),
            mapping = aes(x = XY, 
                          y = nsim_data*Density,
                      color = Treatment),
            alpha = 0.7) +
  geom_line(data = rbind(fx_truth1,fx_truth2,
                         fy_truth1,fy_truth2) %>%
              filter(Model == "Model 2"),
            mapping = aes(x = XY, 
                          y = nsim_data*Density,
                   linetype = Treatment),
            color = rep(c("#8fb032","#eb6235"), each=nx),
            alpha = 0.7) +
  scale_color_manual(name = "Model 2 MC",
                     labels = c("Truth",
                                "Reconstructed"),
                     breaks = c("Truth",
                                "Reconstructed"),
                     values = c("Truth" = "#5e81b5",
                                "Reconstructed" = "#e19c24")) + 
  scale_linetype_manual(name = NA,
                        labels = c("Truth",
                                   "Reconstructed"),
                        breaks = c("Truth",
                                   "Reconstructed"),
                        values = c("Truth" = "solid",
                                   "Reconstructed" = "solid")) +
  guides(color = guide_legend(title = "Model 1 MC", order = 1, 
                              override.aes = list(shape = NA,
                                                  alpha = 0.7),
                              title.vjust = unit(-0.2, "pt")),
         linetype = guide_legend(title = "Model 2", order = 2, 
                                 override.aes = list(color = c("#8fb032","#eb6235"),
                                                     alpha = c(0.8,0.8)),
                                 title.vjust = unit(-0.2, "pt"))) +
  scale_x_continuous("X (Truth), Y (Reconstructed)",
                     breaks = seq(0,30,3), 
                     limits = c(0,30), 
                     expand = c(0,0)) +
  scale_y_continuous("Counts",
                     limits = c(0,fmax_count), expand = c(0,0),
                     breaks = seq(0,ceiling(fmax_count),100)) +
  labs(title = "Scaled Exact Distributions") +
  theme(legend.text = element_text(size = 8),
        legend.title = element_text(size = 9),
        legend.justification = c("left", "top"),
        legend.box.just = "left",
        legend.spacing.y = unit(1, "pt"),
        legend.key.height = unit(10,"points"),
        legend.margin = margin(1,5,5,5,"pt"),
        aspect.ratio = 1,
        axis.title.y = element_blank())

```

```{r, fig.height=4, fig.width=7.5, fig.align='center', echo = F}
################### MC and Data #################################

discrete_DataMC <- ggplot() + 
  theme_bw() +
  geom_point(data = bins_all %>% 
               filter(LBin >= 0 & Bin <= 30 &
                        Source == "Data" & Model == "Model 2"),
             mapping = aes(x = LBin+0.5,
                           y = Counts,
                           fill = "Reconstructed"),
             shape = 20, color = "black") +
  geom_segment(data = filter(bins_all,
                               LBin >= 0 &
                               Bin <= 30 &
                               Source == "MC" &
                               Model == "Model 1"),
               mapping = aes(x = LBin,
                             y = Counts,
                          xend = Bin,
                          yend = Counts,
                         color = Name),
               alpha = 0.7) +
  geom_segment(data = filter(bins_all,
                               LBin >= 0 &
                               Bin <= 30 &
                               Source == "MC" &
                               Model == "Model 1"),
               mapping = aes(x = LBin,
                             y = LCounts,
                          xend = LBin,
                          yend = Counts,
                         color = Name),
               alpha = 0.7) +
  geom_segment(data = bins_all %>% 
                 filter(LBin >= 0 &
                          Bin <= 30 &
                          Source == "MC" &
                          Model == "Model 2") %>%
                 arrange(desc(Treatment)),
               mapping = aes(x = LBin,
                             y = Counts,
                          xend = Bin,
                          yend = Counts,
                      linetype = Name),
                         color = rep(c("#eb6235","#8fb032"),each=30),
               alpha = 0.7) +
  geom_segment(data = bins_all %>% 
                 filter(LBin >= 0 &
                          Bin <= 30 &
                          Source == "MC" &
                          Model == "Model 2") %>% 
                 arrange(desc(Treatment)),
               mapping = aes(x = LBin,
                             y = LCounts,
                          xend = LBin,
                          yend = Counts,
                      linetype = Name),
                         color = rep(c("#eb6235","#8fb032"),each=30),
               alpha = 0.7) +
  scale_x_continuous("X (Truth), Y (Reconstructed)",
                     breaks = seq(0,30,3), 
                     limits = c(0,30), 
                     expand = c(0,0)) +
  scale_y_continuous("Counts",
                     limits = c(0,fmax_count), expand = c(0,0),
                     breaks = seq(0,ceiling(fmax_count),100)) +
  scale_color_manual(name = NA,
                     labels = c("Truth",
                                "Reconstructed"),
                     breaks = c("Truth MC",
                                "Reconstructed MC"),
                     values = c("Truth MC" = "#5e81b5",
                                "Reconstructed MC" = "#e19c24")) + #"#eb6235","#8fb032"
  scale_linetype_manual(name = NA,
                        labels = c("Truth",
                                   "Reconstructed"),
                        breaks = c("Truth MC",
                                   "Reconstructed MC"),
                        values = c("Truth MC" = "solid",
                                   "Reconstructed MC" = "solid")) +
  guides(color = guide_legend(title = "Model 1 MC", order = 1, 
                              override.aes = list(shape = NA,
                                                  alpha = 0.7),
                              title.vjust = unit(-0.5, "pt")),
         linetype = guide_legend(title = "Model 2 MC", order = 2, 
                                 override.aes = 
                                   list(color = c("#eb6235","#8fb032"),
                                        alpha = 0.7),
                                 title.vjust = unit(-0.5, "pt")),
         fill = guide_legend(title = "Data", order = 3, 
                             title.vjust = unit(-0.5, "pt"))) +
  labs(title = "MC Simulations and ''Data''") +
  theme(legend.title = element_text(size = 9),
        legend.text = element_text(size = 8),
        legend.box.just = "left",
        legend.spacing.y = unit(-2.5, "pt"),
        legend.key.height = unit(10,"points"),
        aspect.ratio = 1,
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.title.y = element_blank())
```

\begin{figure}[!ht]
    \centering
```{r, fig.height=3, fig.width=8, fig.align='center', echo = F}
plot1 <- ggplotGrob(continuous2 + theme(legend.position = "none",
                                        axis.title.x = element_blank()))
plot2 <- ggplotGrob(discrete_DataMC + theme(legend.position = "none",
                                        axis.title.x = element_blank()))


egg::ggarrange(continuous2 + 
                 theme(legend.position = "none", 
                       axis.title.x = element_blank()), 
               discrete_DataMC + 
                 theme(axis.title.x = element_blank()), nrow = 1,
               left = textGrob("Counts", rot = 90, vjust = 0.25),
               bottom = textGrob("X (Truth), Y (Reconstructed)", hjust = 0.8))
```
  \caption{\emph{\small The distinguishing characteristics between Model 1 and Model 2 become greatly diminished as their respective distributions are smeared and diffused by mechanisms common to both. Ignoring the knowledge granted in this figure's legend, one would be hardpressed to predict which Truth distribution a particular Reconstructed distribution came from.}}
  \label{plots1}
\end{figure}

Please see Appendix \ref{simuApp} for more specific information about the performed simulations. Referring to Figure \ref{plots1}, the blue and red colored plots correspond to the true distributions of Model 1 and Model 2 respectively. The colored lines represent the MC simulations. They have been rescaled to represent the same number of possible events as the experimental data. The impact of the third contributing Cauchy process in Model 2 can be seen by way of the notably diminished peak and the partial filling of the dividing indent between the two processes of Model 1. While it is clear for both the continuous and discrete representations that the two larger distributions are distinct, less can be said about them once finite detector resolutions and other inefficiencies have had their effects, as one can see in the similarities between their yellow and green reconstructed distributions.

\begin{figure}[!ht]
    \centering
```{r, fig.align='center', echo = F, fig.height=3, fig.align='center'}
########## MIGRATION ###########################################################

migration <- sims_mc %>%
  filter(Detected == TRUE &
           Truth <= 30 & Truth >= 0) %>%
  ggplot() +
    scale_y_continuous(breaks = seq(0,30,by=3),
                       limits = c(0,30), 
                       expand = c(0,0)) +
    scale_x_continuous(breaks = seq(-3,36,by=3), 
                       limits = c(-5,37), 
                       expand = c(0,0)) +
    theme_bw() +
    stat_bin2d(geom="raster",
               mapping = aes(x = Reconstructed, 
                             y = Truth), 
               breaks = list(x = seq(-4,36,1),
                             y = seq(0,30,1))) +
    labs(x = "Y (Reconstructed)", y = "X (Truth)",
         title = "MC Event Migration") +
    geom_abline(intercept=0,slope=1,lty="dashed",col="red") +
    facet_grid(~Model) + coord_equal(ratio = 1)

gradient_max <- max(ggplot_build(migration)$data[[1]]$count)
gmins <- abs(floor(gradient_max/10^(floor(log10(gradient_max))-1))/
               c(5,10,20,25,50)-1)
gradient_step <- c(5,10,20,25,50)[which(gmins == min(gmins))]*
  10^(floor(log10(gradient_max))-2)

migration <- migration +
  scale_fill_gradientn(colours = viridis(11),
                       breaks = seq(0,gradient_max,gradient_step),
                       guide = guide_colourbar(barwidth = 1, 
                                               barheight = 8.5,
                                               frame.colour = "black",
                                               ticks.colour = "black",
                                               title = "Counts"))

migration
```
  \caption{\emph{\small The added dimensional perspective offered by this migration heat map reveals additional structure and dependencies concerning the smearing process.}}
  \label{plots2}
\end{figure}

Figure \ref{plots2} provides an alternative perspective to Figure \ref{plots1} by providing a visual representation of the two models' migration matrices. The red dashed lines indicate where events would live if there were no skewing or smearing processes. Note that the Reconstructed axis extends past points covered by the Truth axis, providing insight into the behavior with which events produced in the region of interest ($X\in(0,30)$ here) end up measured or reconstructed outside of it, but still in a detector's \textbf{fiducial volume}, which represents the reliable, central region of the detector or other relevant region of the events' \textbf{phase space}, which is the space of all possible states.


```{r, echo=F}
### Data 1
dataReco1 <- sims_data %>%
  filter(Model == "Model 1" &
           Truth <= 30 & Truth >= 0 &
           Detected == TRUE) %>%
  mutate(Bin = ceiling(Reconstructed)) %>%
  count(Bin, name = "Counts") %>%
  select(Bin, Counts)
sidedata1 <- dataReco1 %>%
  filter(Bin < 1 | Bin > 30) %>% pull(Bin)
dataReco1 <- dataReco1 %>%
  filter(Bin >= 1 & Bin <= 30) %>%
  rbind(dataReco1 %>% filter(Bin < 1 | Bin > 30) %>%
          mutate(Counts = sum(Counts)) %>% 
          filter(Bin == 31))

dataTruth1 <- sims_data %>%
  filter(Model == "Model 1" &
           Truth <= 30 & Truth >= 0) %>%
  mutate(Bin = ceiling(Truth)) %>%
  count(Bin, name = "Counts") %>%
  select(Bin, Counts)

### Data 2
dataReco2 <- sims_data %>%
  filter(Model == "Model 2" &
           Truth <= 30 & Truth >= 0 &
           Detected == TRUE) %>%
  mutate(Bin = ceiling(Reconstructed)) %>%
  count(Bin, name = "Counts") %>%
  select(Bin, Counts)
sidedata2 <- dataReco2 %>%
  filter(Bin < 1 | Bin > 30) %>% pull(Bin)
dataReco2 <- dataReco2 %>%
  filter(Bin >= 1 & Bin <= 30) %>%
  rbind(dataReco2 %>% filter(Bin < 1 | Bin > 30) %>%
          mutate(Counts = sum(Counts)) %>% 
          filter(Bin == 31))

dataTruth2 <- sims_data %>%
  filter(Model == "Model 2" &
           Truth <= 30 & Truth >= 0) %>%
  mutate(Bin = ceiling(Truth)) %>%
  count(Bin, name = "Counts") %>%
  select(Bin, Counts)

### MC 1
mcReco1 <- sims_mc %>%
  filter(Model == "Model 1" &
           Truth <= 30 & Truth >= 0 &
           Detected == TRUE) %>%
  mutate(Bin = ceiling(Reconstructed)) %>%
  count(Bin, name = "Counts") %>%
  select(Bin, Counts)

sidemc1 <- mcReco1 %>%
  filter(Bin < 1 | Bin > 30) %>% pull(Bin)
mcmat1 <- matrix(rep(0,31*max(sidemc1-min(sidemc1)+1)),ncol=31)
mcmat1[(1:30-min(sidemc1)+1),1:30] <- diag(1,30)
mcmat1[(sidemc1-min(sidemc1)+1),31] <- 1

mcReco1 <- mcReco1 %>%
  filter(Bin >= 1 & Bin <= 30) %>%
  rbind(mcReco1 %>% filter(Bin < 1 | Bin > 30) %>%
          mutate(Counts = sum(Counts)) %>% 
          filter(Bin == 31))

mcTruth1 <- sims_mc %>%
  filter(Model == "Model 1" &
           Truth <= 30 & Truth >= 0) %>%
  mutate(Bin = ceiling(Truth)) %>%
  count(Bin, name = "Counts") %>%
  select(Bin, Counts)

### MC 2
mcReco2 <- sims_mc %>%
  filter(Model == "Model 2" &
           Truth <= 30 & Truth >= 0 &
           Detected == TRUE) %>%
  mutate(Bin = ceiling(Reconstructed)) %>%
  count(Bin, name = "Counts") %>%
  select(Bin, Counts)

sidemc2 <- mcReco2 %>%
  filter(Bin < 1 | Bin > 30) %>% pull(Bin)
mcmat2 <- matrix(rep(0,31*max(sidemc2-min(sidemc2)+1)),ncol=31)
mcmat2[(1:30-min(sidemc2)+1),1:30] <- diag(1,30)
mcmat2[(sidemc2-min(sidemc2)+1),31] <- 1

mcReco2 <- mcReco2 %>%
  filter(Bin >= 1 & Bin <= 30) %>%
  rbind(mcReco2 %>% filter(Bin < 1 | Bin > 30) %>%
          mutate(Counts = sum(Counts)) %>% 
          filter(Bin == 31))

mcTruth2 <- sims_mc %>%
  filter(Model == "Model 2" &
           Truth <= 30 & Truth >= 0) %>%
  mutate(Bin = ceiling(Truth)) %>%
  count(Bin, name = "Counts") %>%
  select(Bin, Counts)


```

\section{Unfolding in particle physics}

The international particle physics community is highly collaborative. Most of this is likely due to necessity, as advances in the field are requiring larger and more expensive experiments that rely on pooled resources of talent, labor, and capital. This extends to digital resources as well. ROOT \cite{Brun1997} is an open-source data analysis framework developed and used primarily by people doing particle physics. Analyses incorporating its resources are primarily written in some combination of Python and C++. While it does contain a strong base level of classes, objects, and methods within its default toolkit, including some for unfolding, extended frameworks are typically built on top of it to meet the needs of specific experiments or to facilitate expanded capabilities that would bloatware for most users. The RooUnfold framework \cite{Adye2011} is one such example. Two methods from the RooUnfold framework will be covered in this section. One is considered naive, as will be shown. The other finds ways past the issues brought up in the first.

\subsection{Inverting the Response Matrix}

In the event of Equation \eqref{eq:mat} being well-posed the obvious
approach would be to construct the unique inverse of the response matrix
$\bm{R}^{-1}$, as estimated from MC simulations, and map the reconstructed counts back to an estimate of
the true counts via \begin{align}
  \bm{\hat\mu}=\bm{R}^{-1}\bm{n}.\label{eq:naiveInv}
\end{align} A statistical justification for this comes from performing
generalized leaste-squares \cite{Johnson2007} fit to estimate $\bm\mu$,
which relies on approximating bin count $n_i$ as normally distributed
with mean $\nu_i$ and variance $1/\nu_i$. Minimizing the sums of squares
yields \begin{align}\min_{\bm\mu}\nabla_{\!\bm\mu}\bm\chi^2(\bm{\mu})
  &=\nabla_{\bm\mu}(\bm{R}\bm{\mu}-\bm{n})^T\bm{\Sigma}_{\nu}^{-1}(\bm{R}\bm{\mu}-\bm{n})\label{eq:leastsq}\\
  &=\nabla_{\bm\mu}(\bm{\mu}^T\bm{R}^T-\bm{n}^T)\bm{\Sigma}_{\nu}^{-1}(\bm{R}\bm{\mu}-\bm{n})\nonumber\\
  &=\nabla_{\bm\mu}(\bm{\mu}^T\bm{R}^T\bm{\Sigma}_{n}^{-1}\bm{R}\bm{\mu}-\bm{\mu}^T\bm{R}^T\bm{\Sigma}_{\nu}^{-1}\bm{n}-\bm{n}^T\bm{\Sigma}_{\nu}^{-1}\bm{R}\bm{\mu}+\bm{n}^T\bm{\Sigma}_{\nu}^{-1}\bm{n})\nonumber\\
  &=\nabla_{\bm\mu}(\bm{\mu}^T\bm{R}^T\bm{\Sigma}_{n}^{-1}\bm{R}\bm{\mu}-2\bm{\mu}^T\bm{R}^T\bm{\Sigma}_{\nu}^{-1}\bm{n}+\bm{n}^T\bm{\Sigma}_{\nu}^{-1}\bm{n})\nonumber\\
  &=2\bm{R}^T\bm{\Sigma}_{\nu}^{-1}\bm{R}\bm{\mu}-2\bm{R}^T\bm{\Sigma}_{\nu}^{-1}\bm{n}\nonumber\\
  &=0\nonumber\\
  \implies\bm{R}^T\bm{\Sigma}_{\nu}^{-1}\bm{R}\bm{\mu}&=\bm{R}^T\bm{\Sigma}_{\nu}^{-1}\bm{n}\nonumber\\
  \implies\bm{\hat\mu}&=(\bm{R}^T\bm{\Sigma}_{\nu}^{-1}\bm{R})^{-1}\bm{R}^T\bm{\Sigma}_{\nu}^{-1}\bm{n}\\
  &=\bm{R}^{+}\bm{n},\label{eq:naivemu}
\end{align} where $\bm{R}^{+}=(\bm{R}^T\bm{\Sigma}_{\nu}^{-1}\bm{R})^{-1}\bm{R}^T\bm{\Sigma}_{\nu}^{-1}$ is the Moore-Penrose generalized inverse
(or pseudo-inverse) \cite{Blobel2013} of $\bm{R}$. Note that multiplying $\bm{R}$ to the left side of $\bm{R}^{+}$ as calculated here fits the form of the hat matrix $\bm{H}$ of ordinary regression, such that $\bm{\hat\nu}=\bm{H}\bm{\nu}$. This has all so far just been basic regression using generalized leaste-squares. As such the inverse of $\bm{R}^T\bm{\Sigma}_{\nu}^{-1}\bm{R}$, using the Moore-Penrose generalized inverse, corresponds to the estimated true counts' covariance matrix:
\begin{align}
  (\bm{R}^T\bm{\Sigma}_{\nu}^{-1}\bm{R})^{-1}=\bm{R}^{+}\bm{\Sigma}_\nu\bm{R}^{+^T}=\bm{R}^{+}\text{Cov}[\bm{n}]\bm{R}^{+^T}=\text{Cov}[\bm{R}^{+}\bm{n}]=\text{Cov}[\bm{\hat\mu}]=\bm{\hat\Sigma}_\mu\nonumber.
\end{align} 
This is the same lower bound covariance matrix calculated in Equation \eqref{CRlb}, which indicates that $\bm{R}^{+}$ is indeed an unbiased estimator with the lowest possible variances among unbiased estimators. For the simulations one such inverse matrix was calculated from the response matrix of each MC Model simulation, and were used to form estimates of their respective $\bm{\Sigma}_\mu$ for each MC-Data combination using each Data model's estimated covariance matrix.

The resulting true counts estimates, with their $\pm1$ estimated standard deviations, are plotted below in  \ref{NaiveRes}. The rescaled true distribution for each MC simulation was provided for comparison. They are in separate plots as their structure is not visible at the scales necessary to show the unfolded Data. The color of the lines indicate the true model behind the simulation and the shaded regions, which represent the calculated uncertainty, are colored to indicate the MC model of the response matrix.

```{r, echo=F, fig.height=3}
R_mc1 <- sims_mc %>%
  filter(Model == "Model 1" &
           Detected == TRUE &
           Truth <= 30 & Truth >= 0) %>%
  mutate(Truth = ceiling(Truth),
         Reconstructed = ceiling(Reconstructed)) %>%
  count(Truth, Reconstructed, name = "Counts") %>%
  complete(Reconstructed=max(Reconstructed):min(Reconstructed),
           Truth=1:30, fill = list(Counts=0)) %>%
  pull(Counts) %>% matrix(byrow = T, ncol = 30) %>% 
  t() %*% mcmat1 %>% t() * matrix(rep(1/mcTruth1$Counts,each=31),ncol=30)

R_mc2 <- sims_mc %>%
  filter(Model == "Model 2" &
           Detected == TRUE &
           Truth <= 30 & Truth >= 0) %>%
  mutate(Truth = ceiling(Truth),
         Reconstructed = ceiling(Reconstructed)) %>%
  count(Truth, Reconstructed, name = "Counts") %>%
  complete(Reconstructed=max(Reconstructed):min(Reconstructed),
           Truth=1:30, fill = list(Counts=0)) %>%
  pull(Counts) %>% matrix(byrow = T, ncol = 30) %>% 
  t() %*% mcmat2 %>% t() * matrix(rep(1/mcTruth2$Counts,each=31),ncol=30)


#Vnu1 <- diag(mcReco1$Counts/dmrat)
#Vnu2 <- diag(mcReco2$Counts/dmrat)
Vnu1 <- diag(dataReco1$Counts)
Vnu2 <- diag(dataReco2$Counts)

inv_R_mc1data1 <- solve(t(R_mc1) %*% solve(Vnu1) %*% R_mc1) %*% 
  t(R_mc1) %*% solve(Vnu1)
inv_R_mc1data2 <- solve(t(R_mc1) %*% solve(Vnu2) %*% R_mc1) %*% 
  t(R_mc1) %*% solve(Vnu2)
inv_R_mc2data1 <- solve(t(R_mc2) %*% solve(Vnu1) %*% R_mc2) %*% 
  t(R_mc2) %*% solve(Vnu1)
inv_R_mc2data2 <- solve(t(R_mc2) %*% solve(Vnu2) %*% R_mc2) %*% 
  t(R_mc2) %*% solve(Vnu2)

Vmu1data1 <- inv_R_mc1data1 %*% Vnu1 %*% t(inv_R_mc1data1)
Vmu1data2 <- inv_R_mc1data2 %*% Vnu2 %*% t(inv_R_mc1data2)
Vmu2data1 <- inv_R_mc2data1 %*% Vnu1 %*% t(inv_R_mc2data1)
Vmu2data2 <- inv_R_mc2data2 %*% Vnu2 %*% t(inv_R_mc2data2)

data1_unfolded_mc1 <- inv_R_mc1data1 %*% dataReco1$Counts
data1_unfolded_mc2 <- inv_R_mc2data1 %*% dataReco1$Counts
data2_unfolded_mc1 <- inv_R_mc1data2 %*% dataReco2$Counts
data2_unfolded_mc2 <- inv_R_mc2data2 %*% dataReco2$Counts

#min(diag(R_mc1 %*%inv_R_mc1data1))/max(diag(R_mc1 %*%inv_R_mc1data1))
#dRm1d1 <- as.data.frame(cbind(dataReco1$Counts,R_mc1))
#summary(lm(V1 ~ . - 1, data = dRm1d1))
#cbind(sqrt(diag(Vmu1data1)),summary(lm(V1 ~ . - 1, data = dRm1d1))$coef[,2])
#cbind(as.vector(lm(V1 ~ .-1, data = dRm1d1)$coef),data1_unfolded_mc1)
```

\begin{figure}[!ht]
    \centering
```{r, echo=F,fig.height=3.5, fig.align='center'}
bins_hat <- tibble(Model = c(rep(c(rep(c("Model 1",
                                         "Model 2"),each=30),
                                   rep(c("Model 1",
                                         "Model 2"),each=30)),2),
                             rep(c("Model 1",
                                   "Model 2"),each=60)),
                   `Assumed Model` = c(rep(c(rep(c("Model 1",
                                                   "Model 2"),each=30),
                                             rep(c("Model 1",
                                                   "Model 2"),each=30)),2),
                                       rep(rep(c("Model 1",
                                                 "Model 2"),each=30),2)),
                   LBin = rep(0:29,12),
                   Bin = rep(1:30,12),
                   LCounts = c(dataReco1$Counts[c(1,1:29)],
                               dataReco2$Counts[c(1,1:29)],
                               mcReco1$Counts[c(1,1:29)]/dmrat,
                               mcReco2$Counts[c(1,1:29)]/dmrat,
                               dataTruth1$Counts[c(1,1:29)],
                               dataTruth2$Counts[c(1,1:29)],
                               mcTruth1$Counts[c(1,1:29)]/dmrat,
                               mcTruth2$Counts[c(1,1:29)]/dmrat,
                               data1_unfolded_mc1[c(1,1:29)],
                               data1_unfolded_mc2[c(1,1:29)],
                               data2_unfolded_mc1[c(1,1:29)],
                               data2_unfolded_mc2[c(1,1:29)]),
                   Counts = c(dataReco1$Counts[1:30],
                              dataReco2$Counts[1:30],
                              mcReco1$Counts[1:30]/dmrat,
                              mcReco2$Counts[1:30]/dmrat,
                              dataTruth1$Counts,
                              dataTruth2$Counts,
                              mcTruth1$Counts/dmrat,
                              mcTruth2$Counts/dmrat,
                              data1_unfolded_mc1,
                              data1_unfolded_mc2,
                              data2_unfolded_mc1,
                              data2_unfolded_mc2),
                   Treatment = c(rep(c("Reconstructed","Truth"),each=4*30),
                                 rep("TruthHat",120)),
                   Source = c(rep("Data",2*30),rep("MC",2*30),
                              rep("Data",2*30),rep("MC",2*30),
                              rep("Data",4*30)),
                   Name = c(rep("Reconstucted Model 1 Data",30),
                            rep("Reconstucted Model 2 Data",30),
                            rep("Reconstucted Model 1  MC",30),
                            rep("Reconstucted Model 2  MC",30),
                            rep("Truth Model 1 Data",30),
                            rep("Truth Model 2 Data",30),
                            rep("MC: Model 1\n   Truth",30),
                            rep("MC: Model 2\n   Truth",30),
                            rep("  MC: Model 1\nData: Model 1",30),
                            rep("  MC: Model 2\nData: Model 1",30),
                            rep("  MC: Model 1\nData: Model 2",30),
                            rep("  MC: Model 2\nData: Model 2",30)),
                   Error = c(sqrt(diag(Vnu1))[1:30],
                             sqrt(diag(Vnu2))[1:30],
                             sqrt(mcReco1$Counts[1:30]/dmrat),
                             sqrt(mcReco2$Counts[1:30]/dmrat),
                             rep(0,30*4),
                             sqrt(diag(Vmu1data1)),
                             sqrt(diag(Vmu1data2)),
                             sqrt(diag(Vmu2data1)),
                             sqrt(diag(Vmu2data2))))


asinh_trans <- function(){
  trans_new(name = 'asinh', transform = function(x) asinh(x), 
            inverse = function(x) sinh(x))
}
fancy_scientific <- function(l) {
     # turn in to character string in scientific notation
     l <- format(l, scientific = TRUE)
     # replace 0e+00 with 0
     l <- gsub("0e\\+00","0",l)
     # remove + after exponent, if exists. E.g.: (3x10^+2 -> 3x10^2)
     l <- gsub("e\\+","e",l)
     # turn the 'e+' into plotmath format
     l <- gsub("e", "\\10^", l)
     parse(text=l)
}

ggplot() + 
  theme_bw() +
  geom_hline(yintercept = 0, linetype = "dashed", alpha = 0.5) +
  geom_rect(data = bins_hat %>%
              filter(Treatment == "TruthHat"),
            mapping = aes(xmin = LBin,
                          ymin = (Counts-Error),
                          xmax = Bin,
                          ymax = (Counts+Error),
                          fill = `Assumed Model`),
            alpha = 0.3) +
  geom_segment(data = bins_hat %>%
                 filter(Treatment == "TruthHat" |
                       (Treatment == "Truth" & Source == "MC")),
               mapping = aes(x = LBin,
                             y = Counts,
                          xend = Bin,
                          yend = Counts,
                         color = Model),
               alpha = 0.9) +
  geom_segment(data = bins_hat %>%
                 filter(Treatment == "TruthHat" |
                       (Treatment == "Truth" & Source == "MC")),
               mapping = aes(x = LBin,
                             y = LCounts,
                          xend = LBin,
                          yend = Counts,
                         color = `Model`),
               alpha = 0.9) +
  scale_x_continuous(breaks = seq(0,30,by=3), 
                     limits = c(0,30), 
                     expand = c(0,0)) +
  scale_color_manual(labels = c("Model 1","Model 2"),
                     breaks = c("Model 1","Model 2"),
                     values = c("#5e81b5","#e19c24")) +
  scale_fill_manual(labels = c("Model 1","Model 2"),
                    breaks = c("Model 1","Model 2"),
                    values = c(alpha("#5e81b5",0.3),
                               alpha("#e19c24",0.3))) +
  facet_wrap(~Name, ncol = 3, scales = "free", dir="v") +
  theme(axis.text = element_text(size=8),
        legend.position = "none") +
  labs(title = "Unfolded Data and MC Truth: Response Matrix Inverse", 
       x = "X (Truth)", 
       y = "Counts")
```
\caption{\emph{\small Plots comparing the unfolded reconstructed data under both model assumptions to the true MC distributions. Note the large uncertainties and the rapidly oscillating positive and negative estimated counts.}}
  \label{NaiveRes}
\end{figure}

The massive, rapid oscillations and large uncertainties indicate that these are not very good results, regardless of whether or not the Data and the MC came from the same Model. The unfolding process used here cannot account for even small random variations in Reconstructed space because it has been over-fitted by the MC results. In search of an unbiased estimator we have unwittingly created a minimum variance that is simply too large to be in any way useful. Some degree of bias in estimating $\bm{\hat\mu}$ will be needed to probe the neighborhood around the unbiased estimate resulting from minimizing the least-squares of Equation \eqref{eq:leastsq} (or equivalent maximum log-likelihood) to find an alternative solution that can actually be used elsewhere. This is done by including an extra term in these equations through a process called \textbf{Regularization}.

\subsection{Regularization}

This section reviews a method of unfolding that utilizes a form of \textbf{Tikhonov regularization}. Tikhonov regularization as discussed here can be described as a form of \textbf{regularized ridge regression} with weighted least-squares, and using a ridge penalty known by some as the \textbf{ridge fused penalty} \cite{vanwieringen2021}. This method is referred to in particle physics as \textbf{Singular Value Decomposition} (SVD) \cite{Adye2011}\cite{Blobel2013}\cite{Cowan1998}, which unfortunately hides much of the underlying concepts. In general, SVD is a matrix factorization method in linear algebra which, to be fair, is featured prominently in this unfolding method. Please refer to Appendix \ref{app:svd} for a brief description of SVD.

As directed in H$\ddot{\text{o}}$cker and Kartvelishvili's seminal paper \cite{Hocker1995} one should shift the location of event count information on the right-hand side of equation \eqref{eq:mat} from the $\bm{\mu}$ to $\bm{R}$, such that its elements are the actual number of corresponding events, much like Figure \ref{plots2}. This new matrix will be referred to by $\bm{X}$. Next, redefine $\bm{\mu}$ by its ratio to the MC true counts $\bm{\mu}^{\text{MC}}$ to produce instead $\bm{\beta}$ such that
\begin{align}
  \beta_i=\mu_i/\mu_i^{\text{MC}}.\nonumber
\end{align}
This naturally requires an inverse rescaling of any estimated $\bm{\hat\beta}$ to get $\bm{\hat\mu}$. The substitution $\bm{Y}=\bm{n}$ will also be used to facilitate broader notation conventions on discussion of this subject. The weighted sum of squares with these changes comes out to be
\begin{align}
  \chi^2(\bm{\beta})=\left(\bm{Y}-\bm{X}\bm{\beta}\right)^T\bm{\hat\Sigma}_\nu^{-1}\left(\bm{Y}-\bm{X}\bm{\beta}\right)\label{regSq}
\end{align}
After rescaling singular value decomposition is then applied to the reconstructed covariance matrix $\bm{\hat\Sigma}_\nu$ which, being symmetric and containing only positive elements, will result in something of the form
\begin{align}
  \bm{\hat\Sigma}_\nu=\bm{QTQ}^T,\;\;\;\;\;\text{ where }\;\;\;\;\;\bm{\hat\Sigma}_\nu^{-1}=\bm{Q}\bm{T}^{-1}\bm{Q}^T\text.
\end{align}
The elements of the diagonal matrix $\bm{T}$ are to be written $T_{ij}=t_i^2\delta_{ij}$. Substituting the inverse of this SVD of $\bm{\hat\Sigma}_\nu$ into Equation \eqref{regSq} the weighted sum of squares becomes an unweighted sum of squares, such that
\begin{align}
  \chi^2(\bm{\beta})=\left(\bm{\tilde{Y}}-\bm{\tilde{X}}\bm{\beta}\right)^T\left(\bm{\tilde{Y}}-\bm{\tilde{X}}\bm{\beta}\right),
\end{align}
where
\begin{align}
  \tilde{X}_{ij}=\frac{1}{t_i}\sum_{k=1}^NQ_{ik}X_{kj}\;\;\;\;\;\;\;\text{and}\;\;\;\;\;\;\tilde{Y}_i=\frac{1}{t_i}\sum_{k=1}^NQ_{ik}Y_{k}.
\end{align}
The bias introduced by regularizing the sum of squares exists by way of the addition of a weighted functional $\tau\,\bm{\Omega}(\bm{\beta})$. As an operator it acts on $\bm{\beta}$ in a fixed way as determined be the needs of the analyst, and $\tau$ (or $\alpha$) is varied to shift the location of the minimum least-squares \cite{Tikh1977}. In this circumstance we are most concerned with reducing the oscillations in our final estimate by encouraging smoothness in the estimated distribution. The general practice is to use an estimate of the second derivative between the ordered elements of $\bm{\beta}$ by way of a matrix operator $\bm{C}$, with the overall goal of minimizing
\begin{align}
  \left(\bm{\tilde{Y}}-\bm{\tilde{X}}\bm{\beta}\right)^T\left(\bm{\tilde{Y}}-\bm{\tilde{X}}\bm{\beta}\right)+\tau (\bm{C}\bm{\beta})^T\bm{C}\bm{\beta}.\label{eq:regular}
\end{align}
The matrix $\bm{C}$ by default assumes a uniform bin width, but if that is not the case the bins can be weighted accordingly by a diagonal matrix $\bm{B}$ whose element $B_{ii}$ equals the width of bin $i$. Plugged in, this changes the regularizing expression in Equation \eqref{eq:regular} to
\begin{align}
  \tau\left(\bm{B}^{1/2}\bm{C}\bm{\beta}\right)^T\bm{B}^{1/2}\bm{C}\bm{\beta}\;\;=\;\;\tau\bm{\beta}^T\left(\bm{C}^T\bm{B}\bm{C}\right)\bm{\beta},\nonumber
\end{align}
where \cite{Cowan1998},
\begin{align}
  \begin{matrix}
    \bm{C}=\begin{pmatrix}
      -1&1&0&\dots&\\
      1&-2&1&\dots&&\\
      0&1&-2&\ddots&\\
      \vdots&\vdots&\vdots&\ddots&\vdots&\vdots\\
      &&&\dots&-2&1\\
      &&&\dots&1&-1
    \end{pmatrix}&
    \text{and}&
    \bm{B}=\begin{pmatrix}
      \Delta x_1&0&\dots&&\\
      0&\Delta x_2 &\dots&&\\
      \vdots&\vdots&\ddots&\vdots&\vdots\\
      &&\dots&\Delta x_{M-1}&0\\
      &&\dots&0&\Delta x_M
    \end{pmatrix}.
  \end{matrix}\nonumber
\end{align}
H$\ddot{\text{o}}$cker and Kartvelishvili work with this to conceptualize an equivalent to Equation \eqref{eq:mat} that involves stacking $N\times M$ matrix $\sqrt{\tau}\,\bm{C}$ below the response matrix and lengthening the Reconstructed vector by $N$ zeros, such that
\begin{align}
  \begin{bmatrix}
    \bm{\tilde{X}}\\\sqrt{\tau}\,\bm{C}
  \end{bmatrix}\bm{\beta} = 
  \begin{bmatrix}
    \bm{\tilde{Y}}\\
    \bm{0}_N
  \end{bmatrix},\nonumber
\end{align}
From here they implement the method of \textbf{damped least squares} to express the solution for $\tau>0$ in terms of the solution for $\tau=0$. They start with absorbing the matrix $\bm{C}$ into $\bm{\beta}$ to get $\begin{bmatrix}\bm{\tilde{X}}\bm{C}^{-1}\\\sqrt{\tau}\,\bm{I}_{N\times M}\end{bmatrix}\bm{C}\bm{\beta}$. The matrix $\bm{C}$ is notably singular, so $\bm{C}^{-1}$ does not exist. The writers work around this by adding a negligible value to its diagonal elements and creating the invertable matrix $\bm{\tilde{C}}=\bm{C}+\xi\bm{I}$, where typically $\xi=10^{-3}$ or $10^{-4}$.

Setting $\tau=0$ and applying singular value decomposition to the remaining matrix on the left-hand, such that $\bm{\tilde{X}\tilde{C}}^{-1}=\bm{USV}^T$, modifies the least-squares equation to 
\begin{align}\chi^2
  &=\left(\bm{\tilde{Y}}-\bm{\tilde{X}}\bm{\beta}\right)^T\left(\bm{\tilde{Y}}-\bm{\tilde{X}}\bm{\beta}\right)\nonumber\\
  &=\left(\bm{\tilde{Y}}-\bm{\tilde{X}}\bm{\tilde{C}}^{-1}\bm{\tilde{C}}\bm{\beta}\right)^T\left(\bm{\tilde{Y}}-\bm{\tilde{X}}\bm{\tilde{C}}^{-1}\bm{\tilde{C}}\bm{\beta}\right)\nonumber\\
  &=\left(\bm{\tilde{Y}}-\bm{USV}^T\bm{\tilde{C}\beta}\right)^T\left(\bm{\tilde{Y}}-\bm{USV}^T\bm{\tilde{C}\beta}\right)\nonumber\\
  &=\left(\bm{\tilde{Y}}-\bm{USb}\right)^T\left(\bm{\tilde{Y}}-\bm{USb}\right)\nonumber\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\{\bm{b}=\bm{V}^T\bm{\tilde{C}\beta}\}\\
  &=\left(\bm{\tilde{Y}}^T-\bm{b}^T\bm{S}^T\bm{U}^T\right)\left(\bm{\tilde{Y}}-\bm{USb}\right)\nonumber\\
  &=\bm{\tilde{Y}}^T\bm{UU}^T\bm{\tilde{Y}}-\bm{b}^T\bm{S}^T\bm{U}^T\bm{\tilde{Y}}-\bm{\tilde{Y}}^T\bm{USb}+\bm{b}^T\bm{S}^T\bm{U}^T\bm{USb}\;\;\;\;\;\;\{\bm{UU}^T=\bm{U}^T\bm{U}=\bm{I}_{N\times N}\}\nonumber\\
  &=\bm{y}^T\bm{y}-\bm{b}^T\bm{S}^T\bm{y}-\bm{y}^T\bm{Sb}+\bm{b}^T\bm{S}^T\bm{S}\bm{b}\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\{\bm{y}=\bm{U}^T\bm{Y}\}\nonumber\\
  &=(\bm{y}-\bm{Sb})^T(\bm{y}-\bm{Sb}).\nonumber
\end{align}
Since $\bm{S}$ is a diagonal matrix with elements $S_{ii}=s_i>0$ the solution can be found easily enough to be 
\begin{align}
  \hat{b}_i^{(0)}=\frac{y_i}{s_i}\;\;\;\;\text{followed by}\;\;\;\;\bm{\hat\beta}^{(0)}=\bm{\tilde{C}}^{-1}\bm{V\hat{b}}^{(0)}.
\end{align}
The estimated Truth counts would then come from reweighting $\bm{\hat\beta}$ such that $\hat\mu_i=\hat\beta_i\mu_i^{\text{MC}}$. Now since $\tau=0$ in this case there is yet no regularization. H$\ddot{\text{o}}$cker and Kartvelishvili provide a source regarding this part of damped least-squares that explains that introducing $\tau>0$ is accomplished just by modifying the non-regularized result to
\begin{align}
  y_i^{(\tau)}&=y_i^{(0)}\frac{s_i^2}{s_i^2+\tau}\nonumber\\
  \implies \hat{b}_i^{(\tau)}&=\frac{y_i^{(0)}s_i}{s_i^2+\tau},\nonumber
\end{align}
and so forth for $\bm{\hat\mu}$. The description of this last equation as a low-pass filter is fairly apt, as the extreme influence of small singular values, represented by diagonals $\{s_i\}$ in the diagonal matrix from the SVD of $\bm{\tilde{X}\tilde{C}}^{-1}$, are muffled by the presence of a nonzero $\tau$.

The analysts work out the resulting covariance matrices for these estimates, which are dependent on the value of $\tau$ such that
\begin{align}
  \hat\Sigma_{ij}^{b}&=\frac{s_i^2}{(s_i^2+\tau)^2}\delta_{ij},\nonumber\\
  \bm{\hat\Sigma}_{\beta}&=\bm{C}^{-1}\bm{V}\bm{\hat\Sigma}_b\bm{V}^T(\bm{C}^{-1})^T,\;\;\text{ and}\nonumber\\
  \hat\Sigma_{ij}^{\mu}&=\mu_i^{\text{MC}}\hat\Sigma_{ij}^\beta\mu_j^{\text{MC}}.\nonumber
\end{align}

As far as selecting a value of $\tau$, H$\ddot{\text{o}}$cker and Kartvelishvili recommend first plotting $\vert y_i\vert$ with respect to $i$. They point at that $\bm{y}$ makes up the coefficients of the decomposition of the reconstructed counts $\bm{n}$ ($\bm{Y}$ above), with an orthogonal basis given by the columns of $\bm{U}$. It is expected that $\vert d_i\vert$ will decay exponentially with $i$ until some critical value $i=k$ in which all following coefficients will be indistinguishable from random samples from an $N(0,1)$ distribution. Once that value of $k$ has been identified set $\tau=s_k^2$.

```{r, echo = F}
C <- rbind(rep(0,30),cbind(diag(1,29),rep(0,29))) + 
  rbind(cbind(rep(0,29),diag(1,29)),rep(0,30)) +
  diag(c(-1,rep(-2,28),-1))

tC <- C + diag(10^(-3),30)

X1 <- R_mc1 %*% diag(mcTruth1$Counts/dmrat)
X2 <- R_mc2 %*% diag(mcTruth2$Counts/dmrat)

SigNu1 <- diag(dataReco1$Counts)
SigNu2 <- diag(dataReco2$Counts)

Y1 <- dataReco1$Counts
Y2 <- dataReco2$Counts

Q1 <- svd(SigNu1)$u
Q2 <- svd(SigNu2)$u
T1 <- diag(svd(SigNu1)$d)
T2 <- diag(svd(SigNu1)$d)

#tXm1d1 <- sqrt(solve(T1)) %*% t(Q1) %*% X1
#tXm1d2 <- sqrt(solve(T2)) %*% t(Q2) %*% X1
#tXm2d1 <- sqrt(solve(T1)) %*% t(Q1) %*% X2
#tXm2d2 <- sqrt(solve(T2)) %*% t(Q2) %*% X2

tXm1d1 <- tXm1d2 <- tXm2d1 <- tXm2d2 <- matrix(0,nrow=31,ncol=30)
tY1 <- tY2 <- rep(0,31)
for(i in 1:31){
  for(j in 1:30){
    tXm1d1[i,j] <- (Q1[i,] %*% X1[,j])/sqrt(T1[i,i])
    tXm1d2[i,j] <- (Q2[i,] %*% X1[,j])/sqrt(T2[i,i])
    tXm2d1[i,j] <- (Q1[i,] %*% X2[,j])/sqrt(T1[i,i])
    tXm2d2[i,j] <- (Q2[i,] %*% X2[,j])/sqrt(T2[i,i])
  }
  tY1[i] <- (Q1[i,] %*% Y1)/sqrt(T1[i,i])
  tY2[i] <- (Q2[i,] %*% Y2)/sqrt(T2[i,i])
}

#tY1 <- sqrt(solve(T1)) %*% t(Q1) %*% Y1
#tY2 <- sqrt(solve(T2)) %*% t(Q2) %*% Y2

Um1d1 <- svd(tXm1d1 %*% solve(tC))$u
Um1d2 <- svd(tXm1d2 %*% solve(tC))$u
Um2d1 <- svd(tXm2d1 %*% solve(tC))$u
Um2d2 <- svd(tXm2d2 %*% solve(tC))$u
Vm1d1 <- svd(tXm1d1 %*% solve(tC))$v
Vm1d2 <- svd(tXm1d2 %*% solve(tC))$v
Vm2d1 <- svd(tXm2d1 %*% solve(tC))$v
Vm2d2 <- svd(tXm2d2 %*% solve(tC))$v
Sm1d1 <- diag(svd(tXm1d1 %*% solve(tC))$d)
Sm1d2 <- diag(svd(tXm1d2 %*% solve(tC))$d)
Sm2d1 <- diag(svd(tXm2d1 %*% solve(tC))$d)
Sm2d2 <- diag(svd(tXm2d2 %*% solve(tC))$d)

s11 <- diag(Sm1d1)
s12 <- diag(Sm1d2)
s21 <- diag(Sm2d1)
s22 <- diag(Sm2d2)

y11 <- t(Um1d1) %*% tY1
y12 <- t(Um1d2) %*% tY2
y21 <- t(Um2d1) %*% tY1
y22 <- t(Um2d2) %*% tY2
```

```{r, echo = FALSE, fig.height=5}
findTau <- tibble(x = rep(c(0,rep(1:29,each=2),30),4),
                  mag_y = abs(rep(c(y11,y12,y21,y22),each=2)),
                  label = c(rep("  MC: Model 1\nData: Model 1",60),
                            rep("  MC: Model 1\nData: Model 2",60),
                            rep("  MC: Model 2\nData: Model 1",60),
                            rep("  MC: Model 2\nData: Model 2",60)))

sTau <- rep(c(s11,s12,s21,s22)^2,each=2)/
  (rep(c(s11,s12,s21,s22)^2,each=2) + 
     rep(c(s11[2],s12[11],s21[11],s22[11])^2,each=60))

findTau <- rbind(findTau,
                 findTau %>% mutate(mag_y = mag_y*sTau)) %>%
  mutate(dcat = rep(c("di0","diTau10"),each=240))

tauArrow <- tibble(x=c(4.5,3.5,5.5,3.5),y=5e-04,xend=c(4.5,3.5,5.5,3.5),
                   yend=0.5*findTau$mag_y[c(10,78,140,68)],
                   label = c("  MC: Model 1\nData: Model 1",
                             "  MC: Model 1\nData: Model 2",
                             "  MC: Model 2\nData: Model 1",
                             "  MC: Model 2\nData: Model 2"))
ggplot() + theme_bw() +
  geom_hline(yintercept = 1, lty = "dotdash") +
  geom_line(data = findTau,
            mapping = aes(x=x,
                          y=mag_y,
                          color=label,
                          linetype=dcat)) +
  scale_color_manual(values = c("#5e81b5","#e19c24","#eb6235","#8fb032"),
                     guide = "none") +
  scale_fill_manual(values = c("#5e81b5","#e19c24","#eb6235","#8fb032"),
                    guide = "none") +
  scale_y_log10(name = expression(paste("|",y,"|"))) +
  scale_x_continuous(name = ,
                     limits = c(0,31),
                     breaks = seq(0,30,5),
                     expand = c(0,0)) +
  scale_linetype_manual(name = element_blank(),
                        breaks = c("di0","diTau10"),
                        labels = c(expression(paste("|",d[i]^(0),"|")),
                                   expression(paste("|",d[i]^(tau),"|"))),
                        values = c("di0"="solid","diTau10"="longdash")) +
  geom_segment(data = tauArrow,
               mapping = aes(x=x,y=y,xend=xend,yend=yend),
               arrow = arrow(length = unit(4, "pt"))) +
  geom_text(data = tibble(x=c(5,4,6,4),
                          y=c(1e-05),
                          lab=c("k==5","k==4","k==6","k==4"),
                          label = c("  MC: Model 1\nData: Model 1",
                             "  MC: Model 1\nData: Model 2",
                             "  MC: Model 2\nData: Model 1",
                             "  MC: Model 2\nData: Model 2")),
            mapping = aes(x=x,y=y,label=lab), size = 3,
            parse = TRUE) +
    facet_wrap(~label, ncol=2) + theme(legend.position = "bottom")

```


```{r, echo = FALSE}

tau11 <- s11[2:20]^2
tau12 <- s12[2:20]^2
tau21 <- s21[2:20]^2
tau22 <- s22[2:20]^2
#tau11 <- c(seq(0,1,0.01),seq(1.5,3.5,0.5),4:25,s11[10]^2)
#tau12 <- c(seq(0,1,0.01),seq(1.5,3.5,0.5),4:25,s12[10]^2)
#tau21 <- c(seq(0,1,0.01),seq(1.5,3.5,0.5),4:25,s21[10]^2)
#tau22 <- c(seq(0,1,0.01),seq(1.5,3.5,0.5),4:25,s22[10]^2)

#c(s11[10]^2,s12[10]^2,s21[10]^2,s22[10]^2)

mu1 <- mcTruth1$Counts
mu2 <- mcTruth2$Counts

chis11 <- rep(0,length(tau11))
chis12 <- rep(0,length(tau12))
chis21 <- rep(0,length(tau21))
chis22 <- rep(0,length(tau22))

hats <- tibble(X = rep(seq(0.5,29.5,1),4*length(tau11)),
               CVCounts = rep(0,30*4*length(tau11)),
               label = rep(rep(c("MC 1 / Data 1",
                                 "MC 1 / Data 2",
                                 "MC 2 / Data 1",
                                 "MC 2 / Data 2"),each=30),
                           length(tau11)),
               SD = rep(0,30*4*length(tau11)),
               MC = rep(rep(c("MC 1","MC 2"),each=2*30),
                        length(tau11)),
               Data = rep(rep(c("Data 1","Data 2",
                            "Data 1","Data 2"),each=30),
                          length(tau11)),
               Taus = c(rep(c(t(cbind(tau11,tau12,tau21,tau22))),each=30)))

truth <- tibble(X = rep(c(0,rep(1:29,each=2),30),4),
                TruthCounts = c(rep(rep(mu1,each=2)/dmrat,2),
                                rep(rep(mu2,each=2)/dmrat,2)),
                MC = rep(c("MC 1","MC 2"),each=120),
                Data = rep(rep(c("Data 1","Data 2"),each=60),2))



inv_sigMuHat11 <- inv_sigMuHat12 <- inv_sigMuHat21 <- 
  inv_sigMuHat22 <- matrix(0, nrow = 30, ncol = 30)

for(i in 1:30){
  for(j in 1:30){
    inv_sigMuHat11[i,j] <- 
      sum(tXm1d1[,i]*tXm1d1[,j])/((mu1[i]*mu1[j])/dmrat^2)
    inv_sigMuHat12[i,j] <- 
      sum(tXm1d2[,i]*tXm1d2[,j])/((mu1[i]*mu1[j])/dmrat^2)
    inv_sigMuHat21[i,j] <- 
      sum(tXm2d1[,i]*tXm2d1[,j])/((mu2[i]*mu2[j])/dmrat^2)
    inv_sigMuHat22[i,j] <- 
      sum(tXm2d2[,i]*tXm2d2[,j])/((mu2[i]*mu2[j])/dmrat^2)
  }
}

for(t in 1:length(tau11)){
  
  bTauHat11 <- bTauHat12 <- 
    bTauHat21 <- bTauHat22 <- rep(0,30)
  for(i in 1:30){
    bTauHat11[i] <- (y11[i]*s11[i])/(s11[i]^2 + tau11[t])
    bTauHat12[i] <- (y12[i]*s12[i])/(s12[i]^2 + tau12[t])
    bTauHat21[i] <- (y21[i]*s21[i])/(s21[i]^2 + tau21[t])
    bTauHat22[i] <- (y22[i]*s22[i])/(s22[i]^2 + tau22[t])
  }
  
  betaTauHat11 <- solve(tC) %*% Vm1d1 %*% bTauHat11
  betaTauHat12 <- solve(tC) %*% Vm1d2 %*% bTauHat12
  betaTauHat21 <- solve(tC) %*% Vm2d1 %*% bTauHat21
  betaTauHat22 <- solve(tC) %*% Vm2d2 %*% bTauHat22
  
  muTauHat11 <- betaTauHat11*(mu1/dmrat)
  muTauHat12 <- betaTauHat12*(mu1/dmrat)
  muTauHat21 <- betaTauHat21*(mu2/dmrat)
  muTauHat22 <- betaTauHat22*(mu2/dmrat)
  
  hats$CVCounts[((t-1)*120 + 1):(t*120)] <-
    c(muTauHat11,muTauHat12,muTauHat21,muTauHat22)
  
  sig_b_hat11 <- sig_b_hat12 <- sig_b_hat21 <- 
    sig_b_hat22 <- matrix(0, nrow = 30, ncol = 30)
  for(i in 1:30){
    sig_b_hat11[i,i] <- s11[i]^2/(s11[i]^2 + tau11[t])^2
    sig_b_hat12[i,i] <- s12[i]^2/(s12[i]^2 + tau12[t])^2
    sig_b_hat21[i,i] <- s21[i]^2/(s21[i]^2 + tau21[t])^2
    sig_b_hat22[i,i] <- s22[i]^2/(s22[i]^2 + tau22[t])^2
  }
  
  sig_beta_hat11 <- solve(tC) %*% Vm1d1 %*% 
    sig_b_hat11 %*% t(Vm1d1) %*% t(solve(tC))
  sig_beta_hat12 <- solve(tC) %*% Vm1d2 %*% 
    sig_b_hat12 %*% t(Vm1d2) %*% t(solve(tC))
  sig_beta_hat21 <- solve(tC) %*% Vm2d1 %*% 
    sig_b_hat21 %*% t(Vm2d1) %*% t(solve(tC))
  sig_beta_hat22 <- solve(tC) %*% Vm2d2 %*% 
    sig_b_hat22 %*% t(Vm2d2) %*% t(solve(tC))
  
  sigMuHat11 <- sigMuHat12 <- sigMuHat21 <- 
    sigMuHat22 <- matrix(0, nrow = 30, ncol = 30)
  for(i in 1:30){
    for(j in 1:30){
      sigMuHat11[i,j] <- 
        (mu1[i]/dmrat)*sig_beta_hat11[i,j]*(mu1[j]/dmrat)
      sigMuHat12[i,j] <- 
        (mu1[i]/dmrat)*sig_beta_hat12[i,j]*(mu1[j]/dmrat)
      sigMuHat21[i,j] <- 
        (mu2[i]/dmrat)*sig_beta_hat21[i,j]*(mu2[j]/dmrat)
      sigMuHat22[i,j] <- 
        (mu2[i]/dmrat)*sig_beta_hat22[i,j]*(mu2[j]/dmrat)
    }
  }
  
  hats$SD[((t-1)*120 + 1):(t*120)] <- 
    sqrt(c(diag(sigMuHat11),diag(sigMuHat12),
           diag(sigMuHat21),diag(sigMuHat22)))
  
  chis11[t] <- t(muTauHat11 - mu1/dmrat) %*% 
    inv_sigMuHat11 %*% (muTauHat11 - mu1/dmrat)
  chis12[t] <- t(muTauHat12 - mu1/dmrat) %*% 
    inv_sigMuHat12 %*% (muTauHat12 - mu1/dmrat)
  chis21[t] <- t(muTauHat21 - mu2/dmrat) %*% 
    inv_sigMuHat21 %*% (muTauHat21 - mu2/dmrat)
  chis22[t] <- t(muTauHat22 - mu2/dmrat) %*% 
    inv_sigMuHat22 %*% (muTauHat22 - mu2/dmrat)
  
}

compareChis <- 
  tibble(tau = c(tau11,tau12,tau21,tau22), 
         Chi2 = c(chis11,chis12,chis21,chis22),
         col=c(rep("MC 1 / Data 1",
                   length(tau11)),
               rep("MC 1 / Data 2",
                   length(tau12)),
               rep("MC 2 / Data 1",
                   length(tau21)),
               rep("MC 2 / Data 2",
                   length(tau22))))

ggplot() + theme_bw() +
  geom_line(data = compareChis, 
            mapping = aes(x=tau, y=Chi2, color=col)) +
  scale_y_continuous(expand=c(0,1),
                     name = expression(Chi^2),
                     limits=c(0,max(compareChis$Chi2))) +
  scale_x_log10(name = expression(tau),
                     expand = c(0,0.1)) +
  geom_point(data = compareChis,
             mapping = aes(x=tau, y=Chi2, fill=col),
             shape = 21) +
  scale_color_manual(name = "Pairings",
                     values = c("#5e81b5","#e19c24",
                                "#eb6235","#8fb032")) +
  scale_fill_manual(name = "Pairings",
                    values = c("#5e81b5","#e19c24",
                               "#eb6235","#8fb032"))

k11 <- 5
k12 <- 4
k21 <- 6
k22 <- 4
ggplot() + theme_bw() + geom_hline(yintercept = 0, lty = 2) +
  geom_line(data = truth,
            mapping = aes(x=X, y=TruthCounts, color = MC)) +
  geom_errorbar(data = hats %>% 
                 filter((MC == "MC 1" & Data == "Data 1" & Taus == s11[k11]^2) |
                          (MC == "MC 1" & Data == "Data 2" & Taus == s12[k12]^2) |
                          (MC == "MC 2" & Data == "Data 1" & Taus == s21[k21]^2) |
                          (MC == "MC 2" & Data == "Data 2" & Taus == s22[k22]^2)),
               mapping = aes(x=X,ymin=CVCounts-SD,ymax=CVCounts+SD),
               width = 0.2) +
  geom_point(data = hats %>% 
                 filter((MC == "MC 1" & Data == "Data 1" & Taus == s11[k11]^2) |
                          (MC == "MC 1" & Data == "Data 2" & Taus == s12[k12]^2) |
                          (MC == "MC 2" & Data == "Data 1" & Taus == s21[k21]^2) |
                          (MC == "MC 2" & Data == "Data 2" & Taus == s22[k22]^2)),
             mapping = aes(x=X, y=CVCounts, fill = Data), shape = 21) +
  scale_fill_manual(values = c("Data 1" = "#5e81b5",
                               "Data 2" = "#e19c24")) +
  scale_color_manual(values = c("MC 1" = "#5e81b5",
                                "MC 2" = "#e19c24")) +
  scale_x_continuous(limits = c(0,30),expand = c(0,0)) +
  facet_wrap(~label)

```

```{r, echo = F, eval = F}
compareMuHats <- tibble(x=c(rep(c(0,rep(1:29,each=2),30),2),
                            rep(seq(0.5,29.5,1),4),
                            rep(seq(0.5,29.5,1)+0.000001,2),
                            rep(c(0,rep(1:29,each=2),30),2),
                            rep(seq(0.5,29.5,1),4),
                            rep(seq(0.5,29.5,1)+0.000001,2)),
                        y=c(rep(mu1/dmrat,each=2),
                            rep(dataTruth1$Counts,each=2),
                            c(mu11hats[[2]]),c(mu11hats[[11]]),
                            c(mu12hats[[2]]),c(mu12hats[[11]]),
                            c(mu12hats[[2]]),c(mu12hats[[11]]),
                            rep(mu2/dmrat,each=2),
                            rep(dataTruth2$Counts,each=2),
                            c(mu21hats[[2]]),c(mu21hats[[11]]),
                            c(mu22hats[[2]]),c(mu22hats[[11]]),
                            c(mu22hats[[2]]),c(mu22hats[[11]])),
                        col=c(rep("MC1",60),rep("Data1",60),
                              rep(c("MC1 Data1","MC1 Data2",
                                    "Incorrect Model"),each=(30*2)),
                              rep("MC2",60),rep("Data2",60),
                              rep(c("MC2 Data1","MC2 Data2",
                                  "Incorrect Model"),each=(30*2))))

ggplot() +
  geom_line(data = compareMuHats %>%
              filter(col == "MC1" | col == "MC2" |
                      col == "Data1" | col == "Data2"),
            mapping = aes(x=x, y=y, color=col)) +
  geom_point(data = compareMuHats %>%
                 filter(col == "MC1 Data1" | col == "MC1 Data2" |
                          col == "MC2 Data1" | col == "MC2 Data2" |
                          col == "Incorrect Model"),
             mapping = aes(x=x, y=y, fill=col, shape=col),
             size=2) + 
  scale_color_manual(name = "True Distribution",
                     breaks = c("MC1","MC2","Data1","Data2"),
                     labels = c("MC1"="MC Model 1",
                                "MC2"="MC Model 2",
                                "Data1"="Data Model 1",
                                "Data2"="Data Model 2"),
                     values = c("MC1"="#000000",
                                "MC2"="555555",
                                "Data1"="#5e81b5",
                                "Data2"="#e19c24")) +
  scale_fill_manual(name = "Models",
                    breaks = c("MC1 Data1","MC1 Data2",
                               "MC2 Data1","MC2 Data2",
                               "Incorrect Model"),
                    labels = c("MC1 Data1"="Model 1",
                               "MC1 Data2"="Model 2",
                               "MC2 Data1"="Model 1",
                               "MC2 Data2"="Model 2",
                               "Incorrect Model"="Incorrect Model"),
                    values = c("MC1 Data1"="#5e81b5",
                               "MC1 Data2"="#e19c24",
                               "MC2 Data1"="#5e81b5",
                               "MC2 Data2"="#e19c24",
                               "Incorrect Model"="#FFFFFF")) +
  scale_shape_manual(name = "Models",
                    breaks = c("MC1 Data1","MC1 Data2",
                               "MC2 Data1","MC2 Data2",
                               "Incorrect Model"),
                    labels = c("MC1 Data1"="Model 1",
                               "MC1 Data2"="Model 2",
                               "MC2 Data1"="Model 1",
                               "MC2 Data2"="Model 2",
                               "Incorrect Model"="Incorrect Model"),
                    values = c("MC1 Data1"=21,
                               "MC1 Data2"=21,
                               "MC2 Data1"=21,
                               "MC2 Data2"=21,
                               "Incorrect Model"=13)) +
  scale_y_continuous(expand=c(0,0),
                     limits = c(0,1600),
                     name = "counts") +
  scale_x_continuous(expand=c(0,0),
                     name = expression(tau))

ggplot(data = tibble(x=c(2,4,6,4.000001,6.000001),y=rep(1,5)),
       mapping = aes(x=x,y=y,shape=as.factor(x),
                     fill=as.factor(y),size=as.factor(y))) +
  geom_point() + theme_bw() +
  scale_shape_manual(labels = c(2,4,6,4,6),
                     breaks = c(2,4,6,4.000001,6.000001),
                     values = c(21,21,21,4,13)) +
  scale_size_manual(values = 3)

```

```{r, echo = FALSE, eval=FALSE}
MC1toData1 <- tibble(x=rep(1:30-0.5,12),
                     y=c(mu1/dmrat,unlist(mu11hats)),
                     shape=c(rep(20,30),rep(1,30*11)),
                     alpha=c(rep(1,30),rep((1:30)/30,each=11)))
ggplot(MC1toData1, aes(x=x,y=y,shape=as.factor(shape),alpha=alpha,
                       color=as.factor(shape))) +
  geom_point() +
  scale_shape_manual(values = c(1,20)) +
  scale_color_manual(values = c("red","black")) +
  scale_y_continuous(limits = c(0,1500))

```

\newpage

\appendix

\section{Hilbert Spaces}\label{appHilbert}

Hilbert spaces are a prominent feature in the field of
functional analysis. They see significant application in partial
differential equations, quantum mechanics, and signal processing, where
they are commonly implemented in the performance of Fourier analysis.
Mathematically they represent an extension beyond the real and complex
geometric-like vector spaces developed by earlier generalizations of
Euclidean spaces in the 19th century. Developments in real analysis at
the beginning of the 20th century lead the spaces of functions and
sequences to being conceptualized as linear spaces in their own right.

As extensions of previously understood spaces they necessarily exist at
the intersection of several other important spaces that aught to be
understood beforehand. With that said, the following definitions come
from Rudin in \cite{Rudin1991}. To start, a \textbf{vector space}, as
defined here, consists of a set $X$ of vectors for which addition and
scalar multiplication are defined such that for all $x,y,z\in X$ and any
complex number $\alpha\in\mathbb{C}$
\begin{enumerate}
  \item there exists a vector in $X$ such that
    \begin{enumerate}
      \item addition is commutative: $x+y=y+x$,
      \item addition is associative: $x+(y+z)=(x+y)+z$,
    \end{enumerate}
  \item $\alpha x$ exists in $X$ such that $1x=x$, $0x=0$ (the zero vector), and multiplication is distributive:
    \begin{enumerate}
      \item $\alpha(\beta x)=(\alpha\beta x)$,
      \item $\alpha(x+y)=\alpha x+\alpha y$, and
      \item $(\alpha +\beta)x=\alpha x+\beta x$.
    \end{enumerate}
\end{enumerate}
The range of $\alpha$ above describes a complex vector space. If
$\alpha$ is restricted to the reals $\mathbb{R}$, then $X$ is considered
a real vector space. Note that vector spaces include more than just
traditional coordinate-style vectors, but also include function spaces
such as the vector space of all polynomials with degree of at most $n$,
which has the basis $\{1,x,x^2,\dots,x^{n-1},x^n\}$.

Typically associated in applications, metric spaces form a another
relevant set of spaces that has some significant overlap with the vector
spaces. A space $X$ is said to be a \textbf{metric space} if for all
$x,y\in X$ there exists an operator $d(x,y)$ that maps them to a
nonnegative real number that defines their distance from each other
within $X$. The properties of this operator are
\begin{enumerate}
  \item $0\leq d(x,y)<\infty$ for all $x$ and $y\in X$,
  \item $d(x,y)=0$ iff $x=y$,
  \item $d(x,y)=d(y,x)$ for all $x$ and $y\in X$,
  \item $d(x,z)\leq d(x,y)+d(y,z)$ for all $x$, $y$, $z\in X$.
\end{enumerate}
For a metric space $X$, the distance operator $d$ is referred to as the
metric on $X$. The intersection of the vector and metric spaces form the
set of normed spaces. As an extension of the conditions thus far, a
space $X$ is a \textbf{normed space} if $\forall x\in X$ there exists a
nonnegative real number $\vert\vert x\vert\vert$, called the
\textbf{norm} of $x$ such that
\begin{enumerate}
  \item $\vert\vert x+y\vert\vert\leq\vert\vert x\vert\vert+\vert\vert y\vert\vert\;\forall x,y\in X$,
  \item $\vert\vert\alpha x\vert\vert = \vert\alpha\vert\,\vert\vert x\vert\vert$ if $x\in X$ and $\alpha$ is a scalar,
  \item $\vert\vert x\vert\vert>0$ if $x\neq 0$.
\end{enumerate}
Such a set is said to be \textbf{complete} if every
\textbf{Cauchy sequence} in $X$ converges to a point in $X$. A Cauchy
sequence in a metric space $X$ is any sequence $\{x_n\}$ that
$\forall\varepsilon>0$ there exists an integer $N$ such that
$d(x_m,x_n)<\varepsilon$ when $m>N$ and $n>N$. A quick example of this
is the sequence defined by $x_n=\sqrt{n}$. For some starting $x_m$ and
$x_n$ where $m-n=\delta$, we have \begin{align}
d(x_m,x_n)
  &=\sqrt{m}-\sqrt{n}\nonumber\\
  &=\sqrt{n+\delta}-\sqrt{n}\nonumber\\
  &=(\sqrt{n+\delta}-\sqrt{n})\frac{\sqrt{n+\delta}+\sqrt{n}}{\sqrt{n+\delta}+\sqrt{n}}\nonumber\\
  &=\frac{n+\delta-n}{\sqrt{n+\delta}+\sqrt{n}}\nonumber\\
  &=\frac{\delta}{\sqrt{n}(\sqrt{1+\delta/n}+\sqrt{1})}\nonumber\\
  &<\frac{1}{\sqrt{n}}\left(\frac{\delta}{2}\right)<\varepsilon\nonumber\\
  \implies n &> \left(\frac{\delta}{2\varepsilon}\right)^2.\nonumber
\end{align} Noting that for constant $\delta$ the limit of
$\frac{1}{\sqrt{n}}\left(\frac{\delta}{2}\right)$ as
$n\longrightarrow\infty$ is the zero vector (the point of convergence)
would also be sufficient to show that $x_n=\sqrt{n}$ is a Cauchy
sequence.

\begin{wrapfigure}{r}{0.55\textwidth}
  \centering
```{r, echo=F, fig.height=3.6, fig.width=3.6, fig.align='center'}
venndiagram <- tibble(X0 = c( 0.0,0.0,
                             -1.3,0.9),
                      Y0 = c(-0.75, 0.75,
                              0.00,-1.05),
                      R = c(4.50,4.5,
                            2.75,2.3),
                      space = c("Vector Space",
                                "Metric Space",
                                "Inner Product Space",
                                "Banach Space")) %>%
  mutate(across(space, factor, 
                levels = c("Vector Space",
                           "Metric Space",
                           "Inner Product Space",
                           "Banach Space")))
venntext <- tibble(X = c(0.0, 1.90,
                         0.0,-2.25,
                         1.7, 0.00),
                   Y = c( 4.45, 2.10,
                         -4.45, 0.75,
                         -1.70,-0.60),
                   theta = c( 0,-35,
                              0, 50,
                             55,  0),
                   text = c("Vector Space\n(vectors)",
                            "Normed Space\n(length)",
                            "Metric Space\n(distance)", 
                            "Inner Product\nSpace\n(angle/orthogonality)",
                            "Banach Space\n(completeness)",
                            "Hilbert\nSpace"))
                           

ggplot() +
  coord_fixed() + theme_void() +
  geom_circle(data = venndiagram,
              mapping = aes(x0 = X0, y0 = Y0, r = R,
                            fill = space)) +
  geom_text(data = venntext,
            mapping = aes(x = X, y = Y, angle = theta,
                          label = text), size = 3.5) +
  theme(legend.position = "none") +
  scale_fill_manual(values = c(alpha("#b91500",0.35),
                               alpha("#45a500",0.35),
                               alpha("#ef6800",0.25),
                               alpha("#0000e0",0.25)))
  
```
  \caption{\emph{A Venn diagram representing the intersection and nesting of the spaces described in Appendix \ref{appHilbert}.}}
  \label{spaceVenn}
  \vspace{-30pt}
\end{wrapfigure}

Incidentally, a normed vector space that is complete as defined here
meets the definition of a \textbf{Banach space}. An additional subset of
the normed vector spaces consists of those spaces in which for all
$x,y\in X$ there exists a real or complex number $\langle x,y\rangle$
defined by an operator called the \textbf{inner product}. For all
$x,y,z\in X$ this operation must satisfy
\begin{enumerate}
  \item $\langle x,y\rangle=\langle y,x\rangle^*$ (where the ${}^*$ represents the complex conjugate),
  \item $\langle x+y,z\rangle=\langle x,z\rangle+\langle y,z\rangle$,
  \item $\langle \alpha x,y\rangle=\alpha\langle x,y\rangle$ (for $\alpha\in\mathbb{C}$),
  \item $\langle x,x\rangle\geq0$, and
  \item $\langle x,x\rangle=0$ iff $x=0$.
\end{enumerate}
A space that satisfies these requirements forms an
\textbf{inner product space}, and the inner product defined in such a
space relates to the form of its norm, such that
$\vert\vert x\vert\vert=\langle x,x\rangle^{1/2}$. Finally, at the
intersection of Banach spaces and inner product spaces are the Hilbert
spaces. I.e. a \textbf{Hilbert space} is a complete vector space with an
inner product defined by its norm.

A commonly presented example is the $L^2$ function space, which consists
of functions that are square integrable, i.e. if
$f(x)\in L^2\implies \vert\vert f(x)\vert\vert^2=\int_\chi\vert f(x)\vert^2dx<\infty$,
where $\chi$ is the domain of $x$. The subset $L^2[-\pi,\pi]$, where
$\chi=[-\pi,\pi]$, has the well known Fourier series as a basis, which
is commonly written such that for $f(x)\in L^2[-\pi,\pi]$ \begin{align}
  f(x)=\frac{a_0}{2} + \sum_{n=1}^\infty\left[a_n\cos(nx)+b_n\sin(nx)\right],\nonumber
\end{align} where $$\begin{matrix}
  a_n=\frac{1}{\pi}\int_{-\pi}^\pi f(x)\cos(nx)dx \\ \text{and} \\ b_n=\frac{1}{\pi}\int_{-\pi}^\pi f(x)\sin(nx)dx.
\end{matrix}$$ Verification that this basis meets all the requirements
laid out so far is beyond the scope of this paper.

\section{Description of simulations} \label{simuApp}

The Cauchy processes specifically are of the form
\begin{align}
  X_{1,i}&\sim \text{Cauchy}(11,4)\nonumber\\
  X_{2,i}&\sim \text{Cauchy}(18,4)\nonumber\\
  X_{3,i}&\sim \text{Cauchy}(14,5)\nonumber
\end{align} 
The probabilities under Model 1 (in which only the first two processes take place) is $\bm{p}=\{0.3,0.7\}$. The probabilities governing Model 2 are generated from $\bm{p}$ by
\begin{align}
  \bm{p'}
    &=\bm{\Lambda}\bm{p}
    =\begin{pmatrix}0.75 & 0 \\ 0 & 0.75 \\ 0.6 & 0.1\end{pmatrix}\begin{pmatrix}0.3\\0.7\end{pmatrix}
    =\begin{pmatrix}0.225 \\ 0.525 \\ 0.25 \end{pmatrix}\nonumber
\end{align}
the effects of detector smearing is represented by i.i.d random variables generated by the conditional Gaussian process
$$\varepsilon_i\sim N\left(\mu(X_i),\sigma(X_i)^2\right),$$
the mean and variance of which are functions defined by 
\begin{align}
  \mu(X_i=x)&=-x^{1/4}\;\;\text{and}\nonumber\\
  \sigma(X_i=x)&=\log\left(\frac{x+10}{4}\right).\nonumber
\end{align} 
The efficiency is similarly conditional on $X_i$, and is
modeled here as a Bernoulli process with i.i.d random variables
$\epsilon_i\sim\text{Bernoulli}\big(p(X_i)\big)$, where the average
detection rate (when $\epsilon_i=1$) is a function of the form
\begin{align}
  p(X_i=x)=1-e^{-\sqrt{x}/4}.\nonumber
\end{align}

\section{Singular Value Decomposition} \label{app:svd}

The below definitions were provided by the Wikipedia page for Singular Value Decomposition \cite{wiki:svd} or by a page it directly links to.

Singular value decomposition involves the factorization of an $N\times M$ matrix $\bm{A}$ into
\begin{align}
  \bm{A}=\bm{USV}^T,\label{eq:svd}
\end{align}
where $\bm{U}$ and $\bm{V}$ are respectively $N\times N$ and $M\times M$ unitary matrices and $\bm{S}$ is an $N\times M$ rectangular diagonal matrix consisting of non-negative real numbers. 

A \textbf{unitary} matrix is any matrix in which its conjugate transpose is also its inverse, i.e. $\bm{U}^\dagger=\bm{U}^{-1}$. When the contents of a unitary matrix are all real it is also referred to as an \textbf{orthogonal} or \textbf{orthonormal} matrix. A \textbf{rectangular diagonal matrix} is simply a rectangular (not necessarily square) in which all of its off diagonal components are $0$, i.e. $S_{ij}=0$ if $i\neq j$. The matrices $\bm{U}$ and $\bm{V}$ can be thought of as two rotation matrices that sandwich a rescaling matrix $\bm{S}$.

\section{Bin-by-bin unfolding}

In this approach a multiplicative \textbf{correction factor} $C_i$ is
applied to the observed number of signal events $n_i$ for each bin to
produce the estimator of $\mu_i$ \cite{Cowan1998}, 
\begin{align}
  \hat{\mu}_i &= C_in_i.\label{eq:binest}
\end{align} 
The correction factors are determined by taking the
respective ratios of a bin's MC simulated truth signal event counts
$\mu_i^{\text{MC}}$ to its MC simulated reconstructed signal event
counts $\nu_i^{\text{MC}}$,
\begin{align}
  C_i=\frac{\mu_i^{\text{MC}}}{\nu_i^{\text{MC}}}.\label{eq:cfact}
\end{align}
The covariance matrix $\bm{\Sigma}_\mu$ of this estimator derives
naturally from Equations \eqref{eq:cov} and \eqref{eq:binest}, with
components 
\begin{align}
  \Sigma^\mu_{ij}
    &=\text{Cov}[\hat\mu_i,\hat\mu_j]\nonumber\\
    &=C_iC_j\text{Cov}[n_i,n_j]\nonumber\\
    &=C_i^2\delta_{ij}\nu_i\nonumber\\
    &=\left(\frac{\mu_i^{\text{MC}}}{\nu_i^{\text{MC}}}\right)^2\delta_{ij}\nu_i.\label{eq:bincov}
\end{align} 
The expectation value of the estimate can be calculated
easily enough as well, and with it the bias 
\begin{align}
  \text{Bias}[\hat{\mu}_i]
    &=E_i[\hat{\mu}_i]-\mu_i\nonumber\\ 
    &=C_iE[n_i]-\mu_i\nonumber\\ 
    &=\frac{\mu_i^{\text{MC}}}{\nu_i^{\text{MC}}}\nu_i-\mu_i\nonumber\\
    &=\left(\frac{\mu_i^{\text{MC}}}{\nu_i^{\text{MC}}}-\frac{\mu_i}{\nu_i}\right)\nu_i.\label{eq:binbias}
\end{align}

```{r, echo = F, fig.align='center',fig.height=3}
BinByBin_bins <- tibble(Model = c(rep(c("Model 1","Model 2"),each=60),
                              rep(c("Model 1","Model 2"),each=30)),
                   `Assumed Model` = rep(rep(c("Model 1","Model 2"),
                                              each=30),3),
                   LBin = rep(0:29,6), Bin = rep(1:30,6),
                   LCounts = c(((mcTruth1$Counts/mcReco1$Counts[1:30])*
                                  dataReco1$Counts[1:30])[c(1,1:29)],
                               ((mcTruth2$Counts/mcReco2$Counts[1:30])*
                                  dataReco1$Counts[1:30])[c(1,1:29)],
                               ((mcTruth1$Counts/mcReco1$Counts[1:30])*
                                  dataReco2$Counts[1:30])[c(1,1:29)],
                               ((mcTruth2$Counts/mcReco2$Counts[1:30])*
                                  dataReco2$Counts[1:30])[c(1,1:29)],
                               mcTruth1$Counts[c(1,1:29)]/dmrat,
                               mcTruth2$Counts[c(1,1:29)]/dmrat),
                   Counts = c((mcTruth1$Counts/mcReco1$Counts[1:30])*
                                dataReco1$Counts[1:30],
                              (mcTruth2$Counts/mcReco2$Counts[1:30])*
                                dataReco1$Counts[1:30],
                              (mcTruth1$Counts/mcReco1$Counts[1:30])*
                                dataReco2$Counts[1:30],
                              (mcTruth2$Counts/mcReco2$Counts[1:30])*
                                dataReco2$Counts[1:30],
                              mcTruth1$Counts/dmrat,
                              mcTruth2$Counts/dmrat),
                   Name = c(rep(c("Model 1 Data",
                                  "Model 2 Data"),each=60),
                            rep("MC",60)))



BinByBin_bins_res <- BinByBin_bins %>% 
  filter(Name == "Model 1 Data" | Name == "Model 2 Data") %>%
  mutate(`Assumed Model` = rep(rep(c("Model 1 Residuals",
                                      "Model 2 Residuals"),each=30),2),
         LCounts = (rep(c(0,mcTruth1$Counts[1:29]/dmrat,
                          0,mcTruth2$Counts[1:29]/dmrat),2) - LCounts),
         Counts = rep(c(mcTruth1$Counts/dmrat,
                        mcTruth2$Counts/dmrat),2) - Counts)


ggplot() + 
  theme_bw() + 
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_segment(data = BinByBin_bins %>% 
                 filter(Name == "MC"),
               mapping = aes(x = LBin,
                             y = Counts,
                          xend = Bin,
                          yend = Counts),
               alpha = 0.6) +
  geom_segment(data = BinByBin_bins %>% 
                 filter(Name == "MC"),
               mapping = aes(x = LBin,
                             y = LCounts,
                          xend = LBin,
                          yend = Counts),
               alpha = 0.6) +
  geom_rect(data = BinByBin_bins %>%
              mutate(ymin = Counts - 
                       rep(sqrt(c((mcTruth1$Counts^2/
                                 mcReco1$Counts[1:30]),
                              (mcTruth2$Counts^2/
                                 mcReco2$Counts[1:30]))),3),
                     ymax = Counts + 
                       rep(sqrt(c((mcTruth1$Counts^2/
                                 mcReco1$Counts[1:30]),
                              (mcTruth2$Counts^2/
                                 mcReco2$Counts[1:30]))),3),
                     xmin = LBin, xmax = Bin),
            mapping = aes(xmin=xmin,ymin=ymin,
                          xmax=xmax,ymax=ymax,
                          fill=Name),alpha=0.2) +
  geom_segment(data = BinByBin_bins %>% 
                 filter(Name == "Model 1 Data" | Name == "Model 2 Data"),
               mapping = aes(x = LBin,
                             y = Counts,
                          xend = Bin,
                          yend = Counts,
                         color = Name),
               alpha = 0.7) +
  geom_segment(data = BinByBin_bins %>% 
                 filter(Name == "Model 1 Data" | Name == "Model 2 Data"),
               mapping = aes(x = LBin,
                             y = LCounts,
                          xend = LBin,
                          yend = Counts,
                         color = Name),
               alpha = 0.7) +
  scale_x_continuous("X (Truth)",
                     breaks = seq(0,30,by=3), 
                       limits = c(0,30), 
                       expand = c(0,0)) +
  scale_color_manual(labels = c("Model 1","Model 2"),
                     breaks = c("Model 1 Data","Model 2 Data"),
                     values = c("#5e81b5","#e19c24")) +
  scale_fill_manual(labels = c("Model 1","Model 2"),
                     breaks = c("Model 1 Data","Model 2 Data"),
                     values = c(alpha("#5e81b5",0.2),
                                alpha("#e19c24",0.2),NA)) +
  guides(fill = guide_legend("Data", override.aes = list(fill=c(alpha("#5e81b5",0.2),alpha("#e19c24",0.2)))),
         color = guide_legend("Data")) +
  facet_wrap(`Assumed Model` ~ ., ncol = 2, dir = "v")
```

```{r, echo = F, fig.align='center',fig.height=3}
ggplot() + 
  theme_bw() + 
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_rect(data = BinByBin_bins_res %>%
              mutate(ymin = Counts - 
                       rep(sqrt(c((mcTruth1$Counts^2/
                                 mcReco1$Counts[1:30]),
                              (mcTruth2$Counts^2/
                                 mcReco2$Counts[1:30]))),2)/dmrat,
                     ymax = Counts + 
                       rep(sqrt(c((mcTruth1$Counts^2/
                                 mcReco1$Counts[1:30]),
                              (mcTruth2$Counts^2/
                                 mcReco2$Counts[1:30]))),2)/dmrat,
                     xmin = LBin, xmax = Bin),
            mapping = aes(xmin=xmin,ymin=ymin,
                          xmax=xmax,ymax=ymax,
                          fill=Name),alpha=0.2) +
  geom_segment(data = BinByBin_bins_res[1:120,],
               mapping = aes(x = LBin,
                             y = Counts,
                          xend = Bin,
                          yend = Counts,
                         color = Name),
               alpha = 0.7) +
  geom_segment(data = BinByBin_bins_res[1:120,],
               mapping = aes(x = LBin,
                             y = LCounts,
                          xend = LBin,
                          yend = Counts,
                         color = Name),
               alpha = 0.7) +
  scale_x_continuous("X (Truth)",
                     breaks = seq(0,30,by=3), 
                       limits = c(0,30), 
                       expand = c(0,0)) +
  scale_color_manual(labels = c("Model 1","Model 2"),
                     breaks = c("Model 1 Data","Model 2 Data"),
                     values = c("#5e81b5","#e19c24")) +
  scale_fill_manual(labels = c("Model 1","Model 2"),
                     breaks = c("Model 1 Data","Model 2 Data"),
                     values = c(alpha("#5e81b5",0.3),
                                alpha("#e19c24",0.3))) +
  guides(fill = guide_legend("Data", override.aes = 
                               list(fill=c(alpha("#5e81b5",0.2),
                                           alpha("#e19c24",0.2)))),
         color = guide_legend("Data")) +
  facet_wrap(`Assumed Model` ~ ., ncol = 2, dir = "v")
```

```{r, echo = F, fig.align='center',fig.height=3}

biases <- 
  c((mcTruth1$Counts/mcReco1$Counts[1:30] - 
       (bins_expected1 %>% filter(Treatment == "Truth") %>% pull(Counts))/
       (bins_expected1 %>% filter(Treatment == "Measured") %>% pull(Counts))) *
      (bins_expected1 %>% filter(Treatment == "Measured") %>% pull(Counts))/
      (bins_expected1 %>% filter(Treatment == "Truth") %>% pull(Counts)),
    (mcTruth2$Counts/mcReco2$Counts[1:30] - 
       (bins_expected1 %>% filter(Treatment == "Truth") %>% pull(Counts))/
       (bins_expected1 %>% filter(Treatment == "Measured") %>% pull(Counts))) *
      (bins_expected1 %>% filter(Treatment == "Measured") %>% pull(Counts))/
      (bins_expected1 %>% filter(Treatment == "Truth") %>% pull(Counts)),
    (mcTruth1$Counts/mcReco1$Counts[1:30] - 
       (bins_expected2 %>% filter(Treatment == "Truth") %>% pull(Counts))/
       (bins_expected2 %>% filter(Treatment == "Measured") %>% pull(Counts))) *
      (bins_expected2 %>% filter(Treatment == "Measured") %>% pull(Counts))/
      (bins_expected2 %>% filter(Treatment == "Truth") %>% pull(Counts)),
    (mcTruth2$Counts/mcReco2$Counts[1:30] - 
       (bins_expected2 %>% filter(Treatment == "Truth") %>% pull(Counts))/
       (bins_expected2 %>% filter(Treatment == "Measured") %>% pull(Counts))) *
      (bins_expected2 %>% filter(Treatment == "Measured") %>% pull(Counts))/
      (bins_expected2 %>% filter(Treatment == "Truth") %>% pull(Counts)),
    ((bins_expected1 %>% filter(Treatment == "Truth") %>% pull(Counts))/
       (bins_expected1 %>% filter(Treatment == "Measured") %>% pull(Counts))-
       (bins_expected2 %>% filter(Treatment == "Truth") %>% pull(Counts))/
       (bins_expected2 %>% filter(Treatment == "Measured") %>% pull(Counts))) *
      (bins_expected2 %>% filter(Treatment == "Measured") %>% pull(Counts))/
      (bins_expected2 %>% filter(Treatment == "Truth") %>% pull(Counts)),
    ((bins_expected2 %>% filter(Treatment == "Truth") %>% pull(Counts))/
       (bins_expected2 %>% filter(Treatment == "Measured") %>% pull(Counts))-
       (bins_expected1 %>% filter(Treatment == "Truth") %>% pull(Counts))/
       (bins_expected1 %>% filter(Treatment == "Measured") %>% pull(Counts))) *
      (bins_expected1 %>% filter(Treatment == "Measured") %>% pull(Counts))/
      (bins_expected1 %>% filter(Treatment == "Truth") %>% pull(Counts)))

biasSD <- c(sqrt(((mcTruth1$Counts/mcReco1$Counts[1:30])*
                    (bins_expected1 %>% 
                       filter(Treatment == "Measured") %>% 
                       pull(Counts))))/
                   (bins_expected1 %>% 
                      filter(Treatment == "Truth") %>% 
                      pull(Counts)),
            sqrt(((mcTruth2$Counts/mcReco2$Counts[1:30])*
                    (bins_expected1 %>% 
                       filter(Treatment == "Measured") %>% 
                       pull(Counts))))/
                   (bins_expected1 %>% 
                      filter(Treatment == "Truth") %>% 
                      pull(Counts)),
            sqrt(((mcTruth1$Counts/mcReco1$Counts[1:30])*
                    (bins_expected2 %>% 
                       filter(Treatment == "Measured") %>% 
                       pull(Counts))))/
                   (bins_expected2 %>% 
                      filter(Treatment == "Truth") %>% 
                      pull(Counts)),
            sqrt(((mcTruth2$Counts/mcReco2$Counts[1:30])*
                    (bins_expected2 %>% 
                       filter(Treatment == "Measured") %>% 
                       pull(Counts))))/
                   (bins_expected2 %>% 
                      filter(Treatment == "Truth") %>% 
                      pull(Counts)))


BinByBin_bias <- tibble(x = c(rep(c(0,rep(1:29,each=2),30,
                                     30,rep(29:1,each=2),0),4),
                               rep(c(0,rep(1:29,each=2),30),4)),
                         y = c(rep(biases[1:30]+biasSD[1:30],each=2),
                               rep(biases[30:1]-biasSD[30:1],each=2),
                               rep(biases[31:60]+biasSD[31:60],each=2),
                               rep(biases[60:31]-biasSD[60:31],each=2),
                               rep(biases[61:90]+biasSD[61:90],each=2),
                               rep(biases[90:61]-biasSD[90:61],each=2),
                               rep(biases[91:120]+biasSD[91:120],each=2),
                               rep(biases[120:91]-biasSD[120:91],each=2),
                               rep(biases[1:120],each=2)),
                         `Expectation Value` = 
                           c(rep("Model 1\nExpectation Value",240),
                             rep("Model 2\nExpectation Value",240),
                             rep("Model 1\nExpectation Value",120),
                             rep("Model 2\nExpectation Value",120)),
                         `MC Truth` = c(rep(c(rep("Model 1",120),
                                              rep("Model 2",120)),2),
                                        rep(c(rep("Model 1",60),
                                              rep("Model 2",60)),2)),
                         MCiEXPj = c(rep(c("MC1EXP1","MC2EXP1",
                                           "MC1EXP2","MC2EXP2"),each=120),
                                     rep(c("MC1EXP1","MC2EXP1",
                                           "MC1EXP2","MC2EXP2"),each=60)),
                         Signal = c(rep("Error",480),rep("CV",240)))

ggplot() + theme_bw() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_polygon(data = BinByBin_bias %>% filter(Signal == "Error"),
               mapping = aes(x=x, y=y, color=`MC Truth`, fill=`MC Truth`),
               linetype = "dotted") +
  geom_line(data = BinByBin_bias %>% filter(Signal == "CV"),
               mapping = aes(x=x, y=y, color=`MC Truth`)) +
  scale_color_manual(values = c("#5e81b5","#e19c24","#eb6235","#8fb032")) +
  scale_fill_manual(values = c(alpha("#5e81b5",0.3),alpha("#e19c24",0.3),
                               alpha("#eb6235",0.3),alpha("#8fb032",0.3))) +
  scale_x_continuous("X (Truth)",
                     breaks = seq(0,30,by=3), 
                       limits = c(0,30), 
                       expand = c(0,0)) +
  scale_y_continuous(
    name = TeX(r"($\frac{\mu_i^{{MC}}/\mu_i}{\nu_i^{{MC}}/\nu_i}-1$)"),
    breaks = seq(-0.15,0.15,0.05),
    labels = c("-0.15","-0.10","-0.05","0.00","0.05","0.10","0.15"),
    limits = c(-0.175,0.175), 
    expand = c(0,0)) +
  facet_wrap(~`Expectation Value`) +
  theme(axis.text.x = element_text(size = 8))

```

\section{R Code}
