---
title: "The Structure of Inverse Problems in Experimental Particle Physics"
author: "Sean Gilligan"
output: 
  pdf_document:
    citation_package: biblatex
    number_sections: TRUE
keep_tex: TRUE
bibliography: citations.bib
header-includes:
  - \usepackage{setspace}\onehalfspacing
  - \usepackage{xcolor}
  - \usepackage{bm}
  - \usepackage{amsmath}
  - \usepackage{parskip}
  - \usepackage{hyperref}
  - \usepackage{csquotes}
  - \usepackage{float}
  - \usepackage{wrapfig}
  - \hypersetup{colorlinks=TRUE,linkcolor=red,citecolor=blue,filecolor=magenta,urlcolor=blue}
  - \newcommand{\comment}[1]{}
abstract: \singlespacing This report provides a survey of some of the common methods used by the high energy physics community to understand and solve ill-posed inverse problems as they pertain to signal distortions that result from imperfect measuring devices and processes. These methods are in general collectively referred to as unfolding. The specifics of data and data collection methods are generalized. Common features are discussed insofar as they contribute to the necessary understanding of the data and implementation of any covered unfolding methods. In order to construct a slightly more wholistic picture some additional topics are briefly touched upon if they relate to other common aspects of data analysis in particle physics, but only during parts of relevant discussions where they would otherwise normally appear.
fontsize: 12pt
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

```{r, message = F, echo=F}
library(tidyverse)
library(gridExtra)
library(grid)
library(gridtext)
library(gtable)
library(knitr)
library(RColorBrewer)
library(viridisLite)
library(viridis)
library(gtools)
library(cowplot)
library(egg)
library(scales)
library(ggforce)
library(latex2exp)
```

\section{Introduction} \label{intro}

A common problem faced in the quantitative sciences and their associated technologies is the introduction of errors during the data collection process. While the possible sources of these errors are as varied as the possible events which the data might describe, significant work has been done to develop methods the can help would-be analysts reconcile them. The requisite understanding of a scenario's underlying systematic and stochastic processes might not allow researchers to truly reverse entropy or make up for the finite resolution of a detector, but it can approximate them with a quantifiable degree of certainty. 

The applied mathematics that this involves falls within the general category of \textbf{inverse problems}, and there are a variety of labels used to refer to the procedures in its arsenal. There is the colloquially vague \textbf{unsmearing}, but there are also names that reference specific applications and methods. For the sake of simplicity, and any necessary physical constraints, the manner of inverse problems addressed here will only have satisfactory solutions that involve linear operations that map from one Hilbert space\footnote{The definition of a Hilbert space is provided in Appendix \ref{appHilbert} for convenience.} to another. Symbolically this can be expressed by the equation
\begin{align}
Az=u,\nonumber
\end{align} 
where $A$ is a linear operator acting on an element $z\in Z$, the sought solution, to produce an element $u\in U$, the observed data. Within the context of the methods described herein $z$
and $u$ take the form of continuous or discrete distributions that when integrated or summed over the domain of their arguments result in finite real quantities.

The difficulty of solving for $z$ can be classified into one of two camps. The easiest cases involve conditions that create a \textbf{well-posed} problem, which requires that \cite{Yagola2011ch2}
\begin{enumerate}
  \item a solution exists $\forall u\in U$,
  \item the solution is unique,
  \item and if $u_n\longrightarrow u$, $Az_n\longrightarrow u_n$, and $Az\longrightarrow u$, then $z_n\longrightarrow z$.
\end{enumerate}
Conditions 1 and 2 work together to imply that the inverse operator
$A^{-1}$ exists, and Condition 3 is often worded to describe the inverse
as continuous, which implies that small deviations in $u$ should
correspond to similar deviations in $z$. When one or more of
these conditions are not met, the problem is said to be
\textbf{ill-posed}, and some of the consequences of assuming otherwise
should hopefully become clear in the coming pages.

In Section \ref{deconvolution} the convolution and deconvolution are
briefly discussed in the capacity of continuously distributed data. They
are then generalized in Section \ref{general}, and discretization
introduced in Section \ref{discret} establishes a data structure that
makes accessible to analysts a host of rigorous statistical and
computational methods. This comes in handy as Section \ref{simulation} presents a number of simulated

Entire books have been written on this subject that do not begin to cover the full scope
of the methods developed to deal with ill-posed problems. With that in mind the hope for this short paper is for it to serve as an introduction to ill-posed problems while providing some degree of direction for those who would like to know more.

\subsection{The Deconvolution}\label{deconvolution}

One way to characterize a basic example of a situation suitable for
being treated as a convolution would be one that should be very familiar
to anyone who has ever taken statistics course. Assume that data
collected regarding $n$ statistical events represent the measurement of
$n$ independent and identically distributed (i.i.d.) random variables
$\bm{X}=\{X_1,X_2,\dots,X_n\}$ from a distribution of possible values
represented by the probability density function (PDF) $f_X(x)$, such
that the probability of a random variable $X_i$ having a value between
$x_a$ and $x_b$ is $$P(\:x_a<X_i<x_b\:)=\int_{x_a}^{x_b}f_X(x)\,dx$$ and
$$\int_\mathcal{X}f_X(x)\,dx=1,$$ where $\mathcal{X}$ represents the
domain of $x$. The error introduced during the measurement process is
similarly represented by a set of i.i.d. random variables
$\bm{\varepsilon}=\{\varepsilon_1,\varepsilon_2,\dots,\varepsilon_n\}$
with a PDF $f_\varepsilon(\varepsilon)$, where the sets
$\bm{\varepsilon}$ and $\bm{X}$ are typically assumed to be independent
of each other. The set of measured/reconstructed values
$\bm{Y}=\{Y_1,Y_2,\dots,Y_n\}$ then are also i.i.d. and can be defined
in terms of the preceding sets of variables such that for event
$i\in\{1,\dots,n\}$,
\begin{align}Y_i&=g(X_i,\varepsilon_i)\nonumber\\&=X_i+\varepsilon_i.\label{eq:meas}\end{align}
In light of this relationship, the corresponding PDF $f_Y(y)$ can be
found explicitly through an operation on $f_X(x)$ and
$f_\varepsilon(\varepsilon)$ using the mathematics of functional
analysis. Stated in more general terms, the empirical density function
$f_Y$ is formed from the \textbf{convolution} of the true density
function $f_X$ and the error density function $f_\varepsilon$, and is
defined by \cite{Panaretos2011}
\begin{align}f_Y&\equiv f_X*f_\varepsilon\label{eq:conv1}\\f_Y(y)&\equiv\int_\mathcal{X}f_X(x)f_\varepsilon(\varepsilon)\,dx\nonumber\\&=\int_\mathcal{X}f_X(x)f_\varepsilon\big(g_x^{-1}(y)\big)\left\vert J_{g_x^{-1}}(y)\right\vert dx\nonumber\\&=\int_\mathcal{X}f_X(x)f_\varepsilon(y-x)\,dx,\label{eq:conv2}\end{align}
where $J$ represents the Jacobian of the transformation involved in
performing the change of basis on $f_\varepsilon$ from $\varepsilon$ to
$x$, which is necessary for the evaluation of the integral for a given
$y$. The magnitude of the Jacobian for transformation of $\varepsilon$
to $y-x$ through the manipulation of Equation \eqref{eq:meas} happens to
be $1$.

As the collection of measured values $\bm{Y}$ accumulates an estimate of
empirical density $\hat{f}_Y$ can readily be formed. However, a major
goal in an analysis of data like this is typically to develop an
accurate estimate of the true density $\hat{f}_X$. Using the information
contained in $\hat{f}_Y$ to accomplish this necessarily requires some
attempt at finding an inverse process to the convolution, i.e. the
\textbf{deconvolution}.

For cases in the form of this particular example there are a variety
approaches, but they commonly involve the Fourier transform of the
density functions $\left\{f_X,f_\varepsilon,f_Y\right\}$ into their
corresponding characteristic functions
$\left\{\phi_X,\phi_\varepsilon,\phi_Y\right\}$
\cite{Meister2009}\cite{Panaretos2011}. Minor aspects of the definition
for the Fourier transform can vary slightly between applications,
resulting primarily from the use of different scale factors and sign
conventions. Here it will be defined for some random variable
$U\in\mathbb{R}$ with density function $f_U(u)$ and random variable
$T\in\mathbb{R}$ as
\begin{align}\phi_T(t)=\int_{-\infty}^\infty f_U(u)\,e^{itu}\,du.\label{eq:ft}\end{align}
When conditions permit the inverse Fourier transform can be found via
\begin{align}f_U(u)=\int_{-\infty}^\infty \phi_T(t)\,e^{-itu}\,dt.\label{eq:ift}\end{align}
The Fourier transform is important in deconvolution methods because when
you apply it to the convolution of two density functions the link
between their respective characteristic functions becomes purely
multiplicative, i.e.
$$f_Y=f_X*f_\varepsilon\implies\phi_Y=\phi_X\phi_\varepsilon.$$ An
instructional proof of this result is provided on page 447 of
\cite{Boas2005}. The steps so far characterize a typical deconvolution
scheme, with later steps consisting of various ways to perform density
estimation and addressing issues similar to those that will be seen
ahead \cite{Meister2009}.

\subsection{Generalizing}\label{general}

The remainder of this paper is dedicated to a more generalized study of
these type of problems. With the understanding that even experts can be
fairly loose and inconsistent with their vocabulary, this paper will do
its best to provide clear definitions. To begin, while most literature
on deconvolution methods do use the word "convolution", this operation
is also referred to by the German word \textit{faltung}
\cite{Weisstein}. The latter's English translation, \textbf{folding}, is
featured prominently in the particle physics community, but refers to a
more generalized process than what is described by Equation
\eqref{eq:conv2} \cite{DAgostini1994}\cite{Adye2011}\cite{Blobel2013}.
In general, folding and \textbf{unfolding} refer to two sets of
processes within which the sets of convolution and deconvolution
processes form proper respective subsets.

One way to arrive at the desired generalization is with the help of
conditional probability. Thinking of $\{X,Y\}$ as a continuous bivariate
random vector with joint PDF $f(x,y)$ and marginal PDFs $f_X(x)$ and
$f_Y(y)$, we can define the conditional PDF of $Y$ given that $X=x$ as
function of $y$, $f(y\,\vert x)$ \cite{Casella2001}. The relationship
between these PDFs is sufficient to define any one of them in terms of
operations involving one or more of the others. As such, for $f_Y(y)$ it
can be shown
\begin{align}f_Y(y)&=\int_\mathcal{X}f(x,y)\,dx\nonumber\\&=\int_\mathcal{X}f(y\,\vert x)f_X(x)\,dx\nonumber\\&=\int_\mathcal{X}K(x,y)f_X(x)\,dx.\label{eq:fred}\end{align}
While integrating over $x$, $f(y\,\vert x)$ is implicitly treated as a
function of both $x$ and $y$. Acknowledging this allows for
understanding Equation \eqref{eq:fred} as a Fredholm integral of the
first kind with a Kernel function $K(x,y)$ that reflects the physical
measurement process \cite{Blobel2011}. The relationship between $x$ and
$y$ in $K(x,y)$ is not defined, but when the kernel is a function of the
difference of its arguments, such that $K(x,y)=K(y-x)$, Equation
\eqref{eq:fred} becomes the convolution described in Equation
\eqref{eq:conv2}.

In particle physics experiments, analysts make use of Monte-Carlo (MC)
simulations to estimate detector response to randoms samples from some
true distribution $f_X(x)^{\text{MC}}$, which is itself estimated by way
of MC simulations using models that typically contain theory being
tested by the experiment in question. The resulting measured
distribution $f_Y(y)^{\text{MC}}$ grants implicit knowledge of $K(x,y)$
by way of Equation \eqref{eq:fred} \cite{Blobel2013}. Finding the
inverse of this Kernel is then the goal, as it should in theory allow
for the mapping of experimental observations $\bm Y$, as randomly
sampled from $f_Y(y)$, back to their true values $\bm X$.

\subsection{Discretization} \label{discret}

In practice researchers are only ever dealing with estimates $\hat f_X$,
$\hat f_Y$, $\hat f_X^{\text{MC}}$, and $\hat f_Y^{\text{MC}}$, and the
sets of data that contribute to these estimates are organized by bin
into histograms that form unnormalized granular approximations of their
true distributions. Thinking in terms of these histograms allows for the
reformulation of Equation \eqref{eq:fred} into the linear matrix
equation: \begin{align}\bm\nu = \bm{R}\bm\mu.\label{eq:mat}\end{align}
The vectors $\bm\nu$, $\bm\mu$ and matrix $\bm{R}$ relate to their
continuous counterparts by \cite{Blobel2013}: \begin{align}
  \text{true distribution }f_X(x)&\longrightarrow\bm\mu\,\in\,\{\mathcal{U}\equiv\mathbb{R}^M_{+}\cup\bm{0}\}\text{ the unknown true bin counts,}\nonumber\\
  \text{measured distribution }f_Y(y)&\longrightarrow\bm\nu\,\in\,\mathcal{V}\equiv\{\mathbb{R}^N_{+}\cup\bm{0}\}\text{ the measured bin counts,}\nonumber\\
  \text{Kernel }K(x,y)&\longrightarrow\bm{R}\;\;\text{the rectangular }N\text{-by-}M\text{ \bf response matrix}\text{.}\nonumber
\end{align} The components of vectors $\bm\nu$ and $\bm\mu$ represent
the number of events that have occurred within the regions of $x$ and
$y$ that define the components' corresponding bins. For $i=1,\dots,N$
and $j=1,\dots,M$ the components of matrix $\bm R$ are defined by the
conditional probability \cite{Cowan1998} \begin{align}
  R_{ij}&=P(\text{measured value in bin }i\vert\text{true value in bin }j)\nonumber\\
        &=\frac{P(\text{measured value in bin }i\text{ and true value in bin }j)}{P(\text{true value in bin }j)}\nonumber\\
        &=\frac{\int_{\text{bin }i}\int_{\text{bin }j}K(x,y)f_X(x)dx\,dy}{\int_{\text{bin }j}dx\,f_X(x)}\nonumber\\
        &\equiv P(\nu_i\vert\mu_j).\label{eq:Rij}
\end{align} In terms of $P(\nu_i\vert\mu_j)$ the full response matrix
then has the form \begin{align}
  \bm{R}=\begin{pmatrix}
    P(\nu_1\vert\mu_1)     & P(\nu_1\vert\mu_2)     & \dots  & P(\nu_1\vert\mu_{N})   \\
    P(\nu_2\vert\mu_1)     & P(\nu_2\vert\mu_2)     & \cdots & P(\nu_2\vert\mu_{N})   \\
    \vdots                 & \vdots                 & \ddots & \vdots                 \\
    P(\nu_{M}\vert\mu_1)   & P(\nu_{M}\vert\mu_2)   & \dots  & P(\nu_{M}\vert\mu_{N})
  \end{pmatrix}.\label{eq:Rmat}
\end{align} With these definitions Equation \eqref{eq:mat} tells us that
an event produced in bin $\mu_j$ has some probability $\geq 0$ of being
measured in each of the $N$ bins of $\bm\nu$, and that each bin count
$\nu_i$ receives potential contributions from each of the $M$ bins in
$\bm\mu$, i.e.
\begin{align}\nu_i = \sum_{j=1}^MR_{ij}\mu_j.\label{eq:bini}\end{align}
The number of bins are typically set such that $M\leq N$, with the
convention $N=M+1$ being common. A higher number of bins in the measured
distribution reflects that the measuring process is expected to map some
events in $\bm X$ to values of $\bm Y$ that are outside the region of
values that define the initial $M$ bins. These one or more extra bins
are intended to account for all the possible values that a particular
event could be mapped to, such that for a given event starting in bin
$j$ one might expect the probabilities of it being measured in each of
the $N$ final bins to sum to $1$.

However, in practice there are a variety of constraints on events that
can either result in them not being included for analysis or even
prevent them from being detected at all. For example, an analyst might
cut events observed in regions of a detector that result in insufficient
data collection, or maybe some event information carriers miss the
detector entirely, resulting in such events going unseen. In either case
the effect of these missing events is described using the detector
\textbf{efficiency}, and represented mathematically by the $N$-vector
$\bm\epsilon$, where component $\epsilon_j$ is the efficiency of the
$j$th true bin
defined\footnote{In the continuous case it is typically written as $\epsilon(x)$, and understood to be the conditional probability of an event producing any measured value given it has a true value of $x$. It is typically absorbed into $K(x,y)$ where it goes on to manifest within $\bm R$ in the manner shown in Equation \eqref{eq:eff} \cite{Blobel2013}.}
by \cite{Cowan1998}:
\begin{align}\sum_{i=1}^{N}P(\nu_i\vert\mu_j)=\sum_{i=1}^{N}R_{ij}=\epsilon_j\leq 1.\label{eq:eff}\end{align}
In contrast to this are contributions to measured counts from
\textbf{background} processes. Just as events produced in a region of
interest can be smeared out of it, events produced out of it can be
smeared into it. The crossed barrier could correspond to the variable of
interest, but it can also include events excluded from analysis due to
assigned constraints on other variables that describe the event.
Background processes are often studied and dealt with prior to the
unfolding procedures described in the paper. It is briefly mentioned
here to provide a slightly more holistic picture of particle physics
analysese. Mathematically, background would be included by modifying
Equation \eqref{eq:bini} to read
\begin{align}\nu_i = \sum_{j=1}^MR_{ij}\mu_j+\beta_i,\end{align} where
$\beta_i$ is the $i$th component of the $N$-vector $\bm\beta$, which
represents the binned background counts. This leads to equations like
$\nu_i^{\text{sig}}=\nu_i-\beta_i$ in order to specify the expected
number of measured counts that are from the signal of interest. Going
forward background will be assumed to already have been accounted for,
and $\nu_i$ will refer to the expected signal counts of bin $i$.

As all these variables so far have been derived from the exact
continuous distributions $f_X(x)$ and $f_Y(y)$, they correspond to the
expectation values that researchers are estimating during data
collection and analysis. As this is a counting process the components of
the observed number of signal events $\bm{n}$, an $N$-vector, are often
related to the components of the expected number of observed counts
$\bm\nu$ as a collection of $N$ separate and independent Poisson
processes. That is to say the observed counts $n_i$ in bin $i$ are
treated as i.i.d. random variables with the probability mass function
\begin{align}P(n_i\vert\nu_i)=\frac{\nu_i^{n_i}e^{-\nu_i}}{n_i!}.\label{eq:pois}\end{align}
The counts $n_i$ would in theory then form the estimate $\hat\nu_i$ of
the expected counts $\nu_i$ by
\begin{align}\nu_i&=\text{E}[\hat\nu_i]=\text{E}[n_i]\nonumber\\&=\text{Var}[\hat\nu_i]=\text{Var}[n_i].\nonumber\end{align}
Understanding the probability distribution of $\bm{n}$ allows for
unfolding methods that involve the use of maximum likelihood estimation.
For methods based on least squares it becomes necessary to find the
covariance matrix $\bm{\Sigma}^{\nu}$ of the observations, which for
independent Poisson processes has components of the form
\begin{align}\Sigma^{\nu}_{ij}&=\text{Cov}[n_i,n_j]\nonumber\\&=\delta_{ij}\nu_i,\label{eq:cov}\end{align}
where $\delta_{ij}$ is the Kronecker
delta\footnote{The Kronecker delta $\delta_{ij}$ is a piecewise function of variables $i$ and $j$ defined by $\delta_{ij}=\begin{cases}0\;\;\;\text{if }i\neq j\\1\;\;\;\text{if }i=j\end{cases}.$}.
The path to the covariance matrix of the estimated true distribution
$\bm{\hat\mu}$ can be considered briefly by considering the maximum
log-likelihood, where it can be shown \begin{align}
  \log L(\bm\mu)
    &=\sum_{i=1}^N\log\left(\frac{\nu_i^{n_i}e^{-\nu_i}}{n_i!}\right)\nonumber\\
    &=\sum_{i=1}^N\left(n_i\log\nu_i-\nu_i-\log n_i!\right)\nonumber\\
  \frac{\partial\log L}{\partial\mu_k}
    &=\sum_{i=1}^N\frac{\partial\log L}{\partial\nu_i}\frac{\partial\nu_i}{\partial\mu_k}\nonumber\\
    &=\sum_{i=1}^N\left(\frac{n_i}{\nu_i}-1\right)R_{ik}=0.\nonumber
\end{align} Some minor algebra here reproduces the estimate
$\bm{\hat\nu}=\bm{n}$, which has been assumed up until now. Continuing
with an additional derivative shows 
\begin{align}
  \frac{\partial^2\log L}{\partial\mu_k\partial\mu_l}
    &=-\sum_{i=1}^N\left(\frac{n_i}{\nu_i^2}\frac{\partial\nu_i}{\partial\mu_l}\right) R_{ik}\nonumber\\
    &=-\sum_{i=1}^N\frac{n_i R_{il}R_{ik}}{\nu_i^2},\nonumber
\end{align} the negative of the expectation value of which is the Fisher information. The Fisher information's relationship with the Cram$\acute{\text{e}}$r-Rao lower bound can, as its name implies, create a lower bound on the covariance matrix of an unbiased estimator of $\bm{\mu}$. For such an unbiased estimator $\bm{\hat T}_{\mu\text{nbiased}}$ we have
\begin{align}
  \text{Cov}\left(\bm{\hat T}_{\mu\text{nbiased}}\right)\geq \bm{I}(\bm{\mu})^{-1}
    &=\left(-E\left[\frac{\partial^2\log L}{\partial\mu_k\partial\mu_l}\right]\right)^{-1}\nonumber\\
    &=\left(\sum_{i=1}^N\frac{E[n_i] R_{il}R_{ik}}{\nu_i^2}\right)^{-1}\nonumber\\
    &=\left(\sum_{i=1}^N\frac{R_{il}R_{ik}}{\nu_i}\right)^{-1}\nonumber\\
    &=\left(\bm{R}^T\bm{\Sigma}_\nu^{-1}\bm{R}\right)^{-1}\nonumber\\
    &=\bm{R}^{-1}\bm{\Sigma}_\nu\bm{R}^{-1^{T}}.\label{CRlb}
\end{align}
Indeed, this matrix must then be lower bound for the covariance matrix of any unbiased estimator of $\bm{\mu}$. This is some good insight to have before getting into the weeds of working on actual data.

\subsection{A Simulated Example}\label{simulation}

Consider the following three sets of i.i.d. random variables from separate Cauchy distributions.
\begin{align}
  X_{1,i}&\sim \text{Cauchy}(x_0^{(1)},\gamma_1)\nonumber\\
  X_{2,i}&\sim \text{Cauchy}(x_0^{(2)},\gamma_2)\nonumber\\
  X_{3,i}&\sim \text{Cauchy}(x_0^{(3)},\gamma_3)\nonumber
\end{align} 
Current models predict that some class of physics events observed in past detectors are solely coming from the first two processes, such that for $n$ events the number coming from the first process is an i.i.d. random variable from the binomial distribution $B(n,p)$. Meanwhile, a new model has been developed that suggests that the third process has been occurring this whole time but has been incorrectly categorized as one or the other of the first two. Napkin math has estimated a contribution rate that is reflective of some probability $p_3$, such that the binomial distribution is actually a multinomial distribution with probabilities $\bm p=\{p_1,p_2,p_3\}$.

A new experiment is being designed and funded to test this new theory, on top of many others, and simulations are being performed to give analyzers plenty of opportunities to develop their collaboration's analysis framework, perform calibrations, and make ready a myriad of studies that hope to shed light on many an unanswered question. The corresponding MC simulations performed during this time include such simulations for the physics events of interest, but also for the detector. The purposes of this paper a relatively simple set of simulations have been performed, leading to 400,000 simulated events per theory that are meant to represent the MC simulations, as well as a set of 20,000 simulated events for each theory the are meant to represent hypothetical data as they might be produced. 

Please see Appendix \ref{simuApp} for information regarding the simulations performed. Please see Figure

```{r, echo = F}
set.seed(12321)
# Assume 10000 events
nsim_data <- 20000
dmrat <- 20
nsim_mc <- dmrat*nsim_data

# Two possible types of processes with different means but equal variances
loctn <- c(11,18,14)
scale <- c(4,4,5)
p <- 0.25
p1 <- c(0.3,0.7)
p2 <- c((1-p)*p1[1],(1-p)*p1[2],p)
p3 <- p2 + c(-0.04,0.03,0.01)

events_data1 <- c(rmultinom(1, nsim_data, p1))
events_data2 <- c(rmultinom(1, nsim_data, p2))
events_data3 <- c(rmultinom(1, nsim_data, p3))

events_mc1 <- c(rmultinom(1, nsim_mc, p1))
events_mc2 <- c(rmultinom(1, nsim_mc, p2))

x1_data1 <- rcauchy(events_data1[1], location = loctn[1], scale = scale[1])
x2_data1 <- rcauchy(events_data1[2], location = loctn[2], scale = scale[2])
x1_data2 <- rcauchy(events_data2[1], location = loctn[1], scale = scale[1])
x2_data2 <- rcauchy(events_data2[2], location = loctn[2], scale = scale[2])
x3_data2 <- rcauchy(events_data2[3], location = loctn[3], scale = scale[3])
x1_data3 <- rcauchy(events_data3[1], location = loctn[1], scale = scale[1])
x2_data3 <- rcauchy(events_data3[2], location = loctn[2], scale = scale[2])
x3_data3 <- rcauchy(events_data3[3], location = loctn[3], scale = scale[3])
x1_mc1 <- rcauchy(events_mc1[1], location = loctn[1], scale = scale[1])
x2_mc1 <- rcauchy(events_mc1[2], location = loctn[2], scale = scale[2])
x1_mc2 <- rcauchy(events_mc2[1], location = loctn[1], scale = scale[1])
x2_mc2 <- rcauchy(events_mc2[2], location = loctn[2], scale = scale[2])
x3_mc2 <- rcauchy(events_mc2[3], location = loctn[3], scale = scale[3])

x_data <- c(x1_data1,x2_data1,
            x1_data2,x2_data2,x3_data2,
            x1_data3,x2_data3,x3_data3)
x_mc <- c(x1_mc1,x2_mc1,
          x1_mc2,x2_mc2,x3_mc2)

# Efficiency
xdetected_data <- rbernoulli(nsim_data,1-exp(-sqrt(abs(x_data))/4)) == 1
xdetected_mc <- rbernoulli(nsim_mc,1-exp(-sqrt(abs(x_mc))/4)) == 1

# Add Smearing
err_mu_data <- -abs(x_data)^(1/4)
err_sd_data <- log((abs(x_data)+10)/4)
y_data <- x_data + rnorm(3*nsim_data, mean = err_mu_data, sd = err_sd_data)

err_mu_mc <- -abs(x_mc)^(1/4)
err_sd_mc <- log((abs(x_mc)+10)/4)
y_mc <- x_mc + rnorm(2*nsim_mc, mean = err_mu_mc, sd = err_sd_mc)

# Binned representation
xymin <- 2
xymax <- 27
xystep <- 3
dxy <- 2
xyaxes <- c(xymin-dxy,xymin,
            seq(xymin+xystep,xymax-xystep,xystep),
            xymax,xymax+dxy)

sims_data <- tibble("Theory" = 
                      rep(c("Theory 1","Theory 2","Theory 3"),
                          each = nsim_data),
                    "Truth" = x_data,
                    "Reconstructed" = y_data,
                    "Detected" = xdetected_data)
sims_mc <- tibble("Theory" = 
                    rep(c("Theory 1","Theory 2"),
                        each = nsim_mc),
                  "Truth" = x_mc,
                  "Reconstructed" = y_mc,
                  "Detected" = xdetected_mc)

bins_data <- sims_data %>%
  filter(Truth <= 30 & Truth >= 0) %>%
  pivot_longer(c(Reconstructed,Truth), 
               names_to = "Treatment", 
               values_to = "Bin") %>%
  mutate(Bin = ceiling(Bin)) %>%
  filter(Bin <= 30 & Bin >= 1) %>%
  filter((Treatment == "Reconstructed" & Detected == TRUE) | Treatment == "Truth") %>%
  count(Theory, Bin, Treatment, name = "Counts") %>%
  complete(Theory, Bin=1:30, Treatment, fill = list(Counts=0)) %>%
  mutate(LBin = Bin-1,
         LCounts = cbind(
           rbind(diag(rep(1,62))[c(3,4,3:58,61,62),-c(1,2)],
                 diag(0,60),
                 diag(0,60)),
           rbind(diag(0,60),
                 diag(rep(1,62))[c(3,4,3:58,61,62),-c(1,2)],
                 diag(0,60)),
           rbind(diag(0,60),
                 diag(0,60),
                 diag(rep(1,62))[c(3,4,3:58,61,62),-c(1,2)])) %*% 
           Counts) %>%
  mutate(Density = Counts/nsim_data,
         LDensity = LCounts/nsim_data) %>%
  select(Theory, Treatment,LBin,Bin,LCounts,Counts,LDensity,Density)

bins_mc <- sims_mc %>%
  filter(Truth <= 30 & Truth >= 0) %>%
  pivot_longer(c(Reconstructed,Truth), 
               names_to = "Treatment", 
               values_to = "Bin") %>%
  mutate(Bin = ceiling(Bin)) %>%
  filter(Bin <= 30 & Bin >= 1) %>%
  filter((Treatment == "Reconstructed" & Detected == TRUE) | Treatment == "Truth") %>%
  count(Theory, Bin, Treatment, name = "Counts") %>%
  complete(Theory, Bin=1:30,  Treatment, fill = list(Counts=0)) %>%
  mutate(LBin = Bin-1,
         LCounts = cbind(
           rbind(diag(rep(1,62))[c(3,4,3:58,61,62),-c(1,2)],diag(0,60)),
           rbind(diag(0,60),diag(rep(1,62))[c(3,4,3:60),-c(1,2)])) %*% 
           Counts) %>%
  mutate(Density = Counts/nsim_data,
         LDensity = LCounts/nsim_data) %>%
  select(Theory,Treatment,LBin,Bin,LCounts,Counts,LDensity,Density)

bins_all <- rbind(bins_data,bins_mc) %>%
  mutate(Source = c(rep("Data",180),rep("MC",120)),
         LCounts = LCounts/c(rep(1,180),rep(dmrat,120)),
         Counts = Counts/c(rep(1,180),rep(dmrat,120))) %>%
  mutate(Name = paste(Treatment,Source)) %>%
  filter(Source == "MC" | Treatment == "Reconstructed")

bins_all$Name[which(bins_all$Source == "Data")] <- "Reconstructed Data"

#max(sims_mc$Truth,sims_mc$Reconstructed,sims_data$Truth,sims_data$Reconstructed)
```

```{r, echo = F}
# Continuous representation
X <- seq(0,30,0.01)
nx <- length(X)

fp1x <- dcauchy(X, location = loctn[1], scale = scale[1]) 
fp2x <- dcauchy(X, location = loctn[2], scale = scale[2])
fp3x <- dcauchy(X, location = loctn[3], scale = scale[3])
#fx <- p*dlnorm(X, meanlog = 2.25, sdlog = 0.25) +
#  (1-p)*dlnorm(X, meanlog = 2.8, sdlog = 0.15)

fpx_truth1 <- tibble(X = rep(X,2), Density = c(p1[1]*fp1x,p1[2]*fp2x), 
                     Process = rep(c("Process 1","Process 2"), each = nx),
                     Theory = rep("Theory 1",2*nx))
fpx_truth2 <- tibble(X = rep(X,3), Density = c(p2[1]*fp1x,p2[2]*fp2x,p2[3]*fp3x), 
                     Process = rep(c("Process 1","Process 2","Process 3"), each = nx),
                     Theory = rep("Theory 2",3*nx))
fx_truth1 <- tibble(X = X, Density = p1[1]*fp1x + p1[2]*fp2x, 
                    Theory = rep("Theory 1",nx),Treatment = rep("Truth",nx))
fx_truth2 <- tibble(X = X, Density = p2[1]*fp1x + p2[2]*fp2x + p2[3]*fp3x,
                    Theory = rep("Theory 2",nx),Treatment = rep("Truth",nx))
fy_truth1 <- read.csv("f1yEstimate.csv") %>% 
  mutate(Theory = "Theory 1", Treatment = "Reconstructed")
fy_truth2 <- read.csv("f2yEstimate.csv") %>% 
  mutate(Theory = "Theory 2", Treatment = "Reconstructed")
names(fy_truth1)[1] <- names(fx_truth1)[1] <- 
  names(fy_truth2)[1] <- names(fx_truth2)[1] <- "XY"

# Binned expected counts
bins_expected1 <- read.csv("hist1Expected.csv") %>%
  mutate(LDensity = LCounts,
         Density = Counts,
         LCounts = LCounts*nsim_data,
         Counts = Counts*nsim_data)
bins_expected2 <- read.csv("hist2Expected.csv") %>%
  mutate(LDensity = LCounts,
         Density = Counts,
         LCounts = LCounts*nsim_data,
         Counts = Counts*nsim_data)

# Counts max
fmax_density <- 1.05*max(c(bins_data$Density,bins_mc$Density,
                           fx_truth1$Density,fx_truth2$Density,
                           fy_truth1$Density,fy_truth2$Density,
                           bins_expected1$Density,bins_expected2$Density))
fmax_count <- 1.05*max(c(bins_data$Counts,bins_mc$Counts/dmrat,
                         fx_truth1$Density*nsim_data,fx_truth2$Density*nsim_data,
                         fy_truth1$Density*nsim_data,fy_truth2$Density*nsim_data,
                         bins_expected1$Counts,bins_expected2$Counts))
```

```{r, fig.height=7, fig.width=7.5, fig.align='center', echo = F}
# Plotting
continuous1 <- ggplot() + 
  theme_bw() +
  geom_line(data = filter(rbind(fpx_truth1,fpx_truth2),
                            X >= xymin-dxy &
                            X <= xymax+dxy),
            mapping = aes(x = X, 
                          y = nsim_data*Density,
                      color = Process,
                   linetype = Theory)) +
  scale_color_manual(name = NA,
                     labels = c("Process 1",
                                "Process 2",
                                "Process 3"),
                     breaks = c("Process 1",
                                "Process 2",
                                "Process 3"),
                     values = c("Process 1" = "#5e81b5",#alpha("#009E73",0.6),
                                "Process 2" = "#e19c24",#alpha("#0072B2",0.6),
                                "Process 3" = "#8fb032")) +#alpha("#0072B2",1))) +
  scale_linetype_manual(name = NA,
                        labels = c("Theory 1",
                                   "Theory 2"),
                        breaks = c("Theory 1",
                                   "Theory 2"),
                        values = c("Theory 1" = "solid",
                                   "Theory 2" = "solid")) +
  scale_x_continuous("X (Truth)",
                     breaks = xyaxes, 
                     limits = c(xymin-dxy,xymax+dxy), 
                     expand = c(0,0)) +
  scale_y_continuous("Counts",
                     limits = c(0,fmax_count), expand = c(0,0),
                     breaks = seq(0,ceiling(fmax_count),100))+
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(), 
        plot.margin = margin(0.15, 0.16, 0.12, 0.1, "cm"),
        legend.text = element_text(size = 7),
        legend.title = element_blank(),
        legend.position = c(.99, 0.99),
        legend.justification = c("right", "top"),
        legend.box.just = "left",
        legend.key.height = unit(10,"points"),
        legend.margin = margin(1,5,5,5,"pt"))

continuous2 <- ggplot() + 
  theme_bw() +
  geom_line(data = rbind(fx_truth1,fx_truth2,
                         fy_truth1,fy_truth2) %>%
              filter(Theory == "Theory 1"),
            mapping = aes(x = XY, 
                          y = nsim_data*Density,
                      color = Treatment),
            alpha = 0.7) +
  geom_line(data = rbind(fx_truth1,fx_truth2,
                         fy_truth1,fy_truth2) %>%
              filter(Theory == "Theory 2"),
            mapping = aes(x = XY, 
                          y = nsim_data*Density,
                   linetype = Treatment),
            color = rep(c("#8fb032","#eb6235"), each=nx),
            alpha = 0.7) +
  scale_color_manual(name = NA,
                     labels = c("Truth",
                                "Reconstructed"),
                     breaks = c("Truth",
                                "Reconstructed"),
                     values = c("Truth" = "#5e81b5",
                                "Reconstructed" = "#e19c24")) + 
  scale_linetype_manual(name = NA,
                        labels = c("Truth",
                                   "Reconstructed"),
                        breaks = c("Truth",
                                   "Reconstructed"),
                        values = c("Truth" = "solid",
                                   "Reconstructed" = "solid")) +
  guides(color = guide_legend(title = "Theory 1", order = 1, 
                              override.aes = list(shape = NA,
                                                  alpha = 0.7),
                              title.vjust = unit(-0.2, "pt")),
         linetype = guide_legend(title = "Theory 2", order = 2, 
                                 override.aes = list(color = c("#8fb032","#eb6235"),
                                                     alpha = c(0.8,0.8)),
                                 title.vjust = unit(-0.2, "pt"))) +
  scale_x_continuous("X (Truth), Y (Reconstructed)",
                     breaks = seq(0,30,3), 
                     limits = c(0,30), 
                     expand = c(0,0)) +
  scale_y_continuous("Counts",
                     limits = c(0,fmax_count), expand = c(0,0),
                     breaks = seq(0,ceiling(fmax_count),100)) +
  labs(title = "Scaled Exact Distributions") +
  theme(legend.text = element_text(size = 8),
        legend.title = element_text(size = 9),
        legend.justification = c("left", "top"),
        legend.box.just = "left",
        legend.spacing.y = unit(1, "pt"),
        legend.key.height = unit(10,"points"),
        legend.margin = margin(1,5,5,5,"pt"),
        aspect.ratio = 1,
        axis.title.y = element_blank())

# eb6235
# 8fb032

```

```{r, fig.height=7, fig.width=7.5, fig.align='center', echo = F, eval = F}
discrete_exp <- ggplot() + 
  theme_bw() +
  geom_segment(data = filter(bins_expected1, 
                             Treatment == "Truth" &
                                  LBin >= xymin-dxy &
                                   Bin <= xymax+dxy),
               mapping = aes(x = LBin,
                             y = Counts,
                          xend = Bin,
                          yend = Counts,
                         color = "E[Theory 1 Truth]",
                      linetype = "E[Theory 1 Truth]")) +
  geom_segment(data = filter(bins_expected1, 
                             Treatment == "Truth" &
                                  LBin >= xymin-dxy &
                                   Bin <= xymax+dxy),
               mapping = aes(x = LBin,
                             y = LCounts,
                          xend = LBin,
                          yend = Counts,
                         color = "E[Theory 1 Truth]",
                      linetype = "E[Theory 1 Truth]")) +
  geom_segment(data = filter(bins_expected1, 
                             Treatment == "Reconstructed" &
                                  LBin >= xymin-dxy &
                                   Bin <= xymax+dxy),
               mapping = aes(x = LBin,
                             y = Counts,
                          xend = Bin,
                          yend = Counts,
                         color = "E[Theory 1 Reconstructed]",
                      linetype = "E[Theory 1 Reconstructed]")) +
  geom_segment(data = filter(bins_expected1, 
                             Treatment == "Reconstructed" &
                                  LBin >= xymin-dxy &
                                   Bin <= xymax+dxy),
               mapping = aes(x = LBin,
                             y = LCounts,
                          xend = LBin,
                          yend = Counts,
                         color = "E[Theory 1 Reconstructed]",
                      linetype = "E[Theory 1 Reconstructed]")) +
  geom_segment(data = filter(bins_expected2, 
                             Treatment == "Truth" &
                                  LBin >= xymin-dxy &
                                   Bin <= xymax+dxy),
               mapping = aes(x = LBin,
                             y = Counts,
                          xend = Bin,
                          yend = Counts,
                         color = "E[Theory 2 Truth]",
                      linetype = "E[Theory 2 Truth]")) +
  geom_segment(data = filter(bins_expected2, 
                             Treatment == "Truth" &
                                  LBin >= xymin-dxy &
                                   Bin <= xymax+dxy),
               mapping = aes(x = LBin,
                             y = LCounts,
                          xend = LBin,
                          yend = Counts,
                         color = "E[Theory 2 Truth]",
                      linetype = "E[Theory 2 Truth]")) +
  geom_segment(data = filter(bins_expected2, 
                             Treatment == "Reconstructed" &
                                  LBin >= xymin-dxy &
                                   Bin <= xymax+dxy),
               mapping = aes(x = LBin,
                             y = Counts,
                          xend = Bin,
                          yend = Counts,
                         color = "E[Theory 2 Reconstructed]",
                      linetype = "E[Theory 2 Reconstructed]")) +
  geom_segment(data = filter(bins_expected2, 
                             Treatment == "Reconstructed" &
                                  LBin >= xymin-dxy &
                                   Bin <= xymax+dxy),
               mapping = aes(x = LBin,
                             y = LCounts,
                          xend = LBin,
                          yend = Counts,
                         color = "E[Theory 2 Reconstructed]",
                      linetype = "E[Theory 2 Reconstructed]")) +
  scale_x_continuous("X (Truth), Y (Reconstructed)",
                     breaks = xyaxes, 
                     limits = c(xymin-dxy,xymax+dxy), 
                     expand = c(0,0)) +
  scale_y_continuous("Counts",
                     limits = c(0,fmax_count), expand = c(0,0),
                     breaks = seq(0,ceiling(fmax_count),
                                  floor(fmax_count/1000)*200)) +
  scale_color_manual(name = NA,
                     labels = c("E[Theory 1 Truth]",
                                "E[Theory 2 Truth]",
                                "E[Theory 1 Reconstructed]",
                                "E[Theory 2 Reconstructed]"),
                     breaks = c("E[Theory 1 Truth]",
                                "E[Theory 2 Truth]",
                                "E[Theory 1 Reconstructed]",
                                "E[Theory 2 Reconstructed]"),
                     values = c("E[Theory 1 Truth]" = alpha("#5e81b5",1),#alpha("#009E73",0.6),
                                "E[Theory 2 Truth]" = alpha("#5e81b5",1),#alpha("#009E73",1),
                                "E[Theory 1 Reconstructed]" = alpha("#e19c24",1),#alpha("#0072B2",0.6),
                                "E[Theory 2 Reconstructed]" = alpha("#e19c24",1))) +#alpha("#0072B2",1))) +
  scale_linetype_manual(name = NA,
                        labels = c("E[Theory 1 Truth]",
                                   "E[Theory 2 Truth]",
                                   "E[Theory 1 Reconstructed]",
                                   "E[Theory 2 Reconstructed]"),
                        breaks = c("E[Theory 1 Truth]",
                                   "E[Theory 2 Truth]",
                                   "E[Theory 1 Reconstructed]",
                                   "E[Theory 2 Reconstructed]"),
                        values = c("E[Theory 1 Truth]" = "solid",
                                   "E[Theory 2 Truth]" = "dashed",
                                   "E[Theory 1 Reconstructed]" = "solid",
                                   "E[Theory 2 Reconstructed]" = "dashed")) +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(), 
        plot.margin = margin(0.15, 0.16, 0.12, 0.1, "cm"),
        legend.text = element_text(size = 7),
        legend.title = element_blank(),
        legend.position = c(.01, 0.99),
        legend.justification = c("left", "top"),
        legend.box.just = "left",
        legend.key.height = unit(10,"points"),
        legend.margin = margin(1,5,5,5,"pt"),
        axis.title.x = element_blank()) +
  guides(color = guide_legend(override.aes = list(alpha = c(1,1,1,1))))

discrete_theory1 <- ggplot() + 
  theme_bw() +
  geom_segment(data = filter(bins_all,
                               Theory == "Theory 1" &
                               LBin >= xymin-dxy &
                               Bin <= xymax+dxy),
               mapping = aes(x = LBin,
                             y = Counts,
                          xend = Bin,
                          yend = Counts,
                         color = Name)) +
  geom_segment(data = filter(bins_all, 
                               Theory == "Theory 1" &
                               LBin >= xymin-dxy &
                               Bin <= xymax+dxy),
               mapping = aes(x = LBin,
                             y = LCounts,
                          xend = LBin,
                          yend = Counts,
                         color = Name)) +
  scale_x_continuous("X (Truth), Y (Reconstructed)",
                     breaks = xyaxes, 
                     limits = c(xymin-dxy,xymax+dxy), 
                     expand = c(0,0)) +
  scale_y_continuous("Counts",
                     limits = c(0,fmax_count), expand = c(0,0),
                     breaks = seq(0,ceiling(fmax_count),
                                  floor(fmax_count/1000)*200)) +
  scale_color_manual(name = "Theory 1",
                     labels = c("MC Truth",
                                "Data Truth",
                                "MC Reconstructed",
                                "Data Reconstructed"),
                     breaks = c("MC Truth",
                                "Data Truth",
                                "MC Reconstructed",
                                "Data Reconstructed"),
                     values = c("MC Truth" = alpha("#5e81b5",1),#alpha("#009E73",0.6),
                                "Data Truth" = alpha("#e19c24",1),#alpha("#009E73",1),
                                "MC Reconstructed" = alpha("#8fb032",1),#alpha("#0072B2",0.6),
                                "Data Reconstructed" = alpha("#eb6235",1))) +#alpha("#0072B2",1))) +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(), 
        plot.margin = margin(0.15, 0.16, 0.12, 0.1, "cm"),
        legend.text = element_text(size = 8),
        legend.title = element_blank(),
        legend.position = c(.01, 0.99),
        legend.justification = c("left", "top"),
        legend.box.just = "left",
        legend.key.height = unit(10,"points"),
        legend.margin = margin(1,5,5,5,"pt"),
        axis.title.x = element_blank()) +
  guides(color = guide_legend(override.aes = list(alpha = c(1,1,1,1))))

discrete_theory2 <- ggplot() + 
  theme_bw() +
  geom_segment(data = filter(bins_all,
                               Theory == "Theory 2" &
                               LBin >= xymin-dxy &
                               Bin <= xymax+dxy),
               mapping = aes(x = LBin,
                             y = Counts,
                          xend = Bin,
                          yend = Counts,
                         color = Name)) +
  geom_segment(data = filter(bins_all, 
                               Theory == "Theory 2" &
                               LBin >= xymin-dxy &
                               Bin <= xymax+dxy),
               mapping = aes(x = LBin,
                             y = LCounts,
                          xend = LBin,
                          yend = Counts,
                         color = Name)) +
  scale_x_continuous("X (Truth), Y (Reconstructed)",
                     breaks = xyaxes, 
                     limits = c(xymin-dxy,xymax+dxy), 
                     expand = c(0,0)) +
  scale_y_continuous("Counts",
                     limits = c(0,fmax_count), expand = c(0,0),
                     breaks = seq(0,ceiling(fmax_count),
                                  floor(fmax_count/1000)*200)) +
  scale_color_manual(name = "Theory 2",
                     labels = c("MC Truth",
                                "Data Truth",
                                "MC Reconstructed",
                                "Data Reconstructed"),
                     breaks = c("MC Truth",
                                "Data Truth",
                                "MC Reconstructed",
                                "Data Reconstructed"),
                     values = c("MC Truth" = alpha("#5e81b5",1),#alpha("#009E73",0.6),
                                "Data Truth" = alpha("#e19c24",1),#alpha("#009E73",1),
                                "MC Reconstructed" = alpha("#8fb032",1),#alpha("#0072B2",0.6),
                                "Data Reconstructed" = alpha("#eb6235",1))) +#alpha("#0072B2",1))) +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(), 
        plot.margin = margin(0.15, 0.16, 0.12, 0.1, "cm"),
        legend.text = element_text(size = 8),
        legend.title = element_blank(),
        legend.position = c(.01, 0.99),
        legend.justification = c("left", "top"),
        legend.box.just = "left",
        legend.key.height = unit(10,"points"),
        legend.margin = margin(1,5,5,5,"pt"),
        axis.title.x = element_blank()) +
  guides(color = guide_legend(override.aes = list(alpha = c(1,1,1,1))))
```

```{r, fig.height=4, fig.width=7.5, fig.align='center', echo = F}
################### MC and Data #################################

discrete_DataMC <- ggplot() + 
  theme_bw() +
  geom_segment(data = filter(bins_all,
                               LBin >= 0 &
                               Bin <= 30 &
                               Source == "MC" &
                               Theory == "Theory 1"),
               mapping = aes(x = LBin,
                             y = Counts,
                          xend = Bin,
                          yend = Counts,
                         color = Name),
               alpha = 0.7) +
  geom_segment(data = filter(bins_all,
                               LBin >= 0 &
                               Bin <= 30 &
                               Source == "MC" &
                               Theory == "Theory 1"),
               mapping = aes(x = LBin,
                             y = LCounts,
                          xend = LBin,
                          yend = Counts,
                         color = Name),
               alpha = 0.7) +
  geom_segment(data = bins_all %>% 
                 filter(LBin >= 0 &
                          Bin <= 30 &
                          Source == "MC" &
                          Theory == "Theory 2") %>%
                 arrange(desc(Treatment)),
               mapping = aes(x = LBin,
                             y = Counts,
                          xend = Bin,
                          yend = Counts,
                      linetype = Name),
                         color = rep(c("#eb6235","#8fb032"),each=30),
               alpha = 0.7) +
  geom_segment(data = bins_all %>% 
                 filter(LBin >= 0 &
                          Bin <= 30 &
                          Source == "MC" &
                          Theory == "Theory 2") %>% 
                 arrange(desc(Treatment)),
               mapping = aes(x = LBin,
                             y = LCounts,
                          xend = LBin,
                          yend = Counts,
                      linetype = Name),
                         color = rep(c("#eb6235","#8fb032"),each=30),
               alpha = 0.7) +
  geom_point(data = bins_all %>% 
               filter(LBin >= 0 & Bin <= 30 &
                        Source == "Data" & Theory == "Theory 2"),
             mapping = aes(x = LBin+0.5,
                           y = Counts,
                           fill = "Reconstructed"),
             shape = 20, color = "black") +
  scale_x_continuous("X (Truth), Y (Reconstructed)",
                     breaks = seq(0,30,3), 
                     limits = c(0,30), 
                     expand = c(0,0)) +
  scale_y_continuous("Counts",
                     limits = c(0,fmax_count), expand = c(0,0),
                     breaks = seq(0,ceiling(fmax_count),100)) +
  scale_color_manual(name = NA,
                     labels = c("Truth",
                                "Reconstructed"),
                     breaks = c("Truth MC",
                                "Reconstructed MC"),
                     values = c("Truth MC" = "#5e81b5",
                                "Reconstructed MC" = "#e19c24")) + #"#eb6235","#8fb032"
  scale_linetype_manual(name = NA,
                        labels = c("Truth",
                                   "Reconstructed"),
                        breaks = c("Truth MC",
                                   "Reconstructed MC"),
                        values = c("Truth MC" = "solid",
                                   "Reconstructed MC" = "solid")) +
  guides(color = guide_legend(title = "Theory 1", order = 1, 
                              override.aes = list(shape = NA,
                                                  alpha = 0.7),
                              title.vjust = unit(-0.5, "pt")),
         linetype = guide_legend(title = "Theory 2", order = 2, 
                                 override.aes = list(color = c("#eb6235","#8fb032"),
                                                     alpha = 0.7),
                                 title.vjust = unit(-0.5, "pt")),
         fill = guide_legend(title = "Data", order = 3, title.vjust = unit(-0.5, "pt"))) +
  labs(title = "MC Simulations and ''Data''") +
  theme(legend.title = element_text(size = 9),
        legend.text = element_text(size = 8),
        legend.box.just = "left",
        legend.spacing.y = unit(-2.5, "pt"),
        legend.key.height = unit(10,"points"),
        aspect.ratio = 1,
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.title.y = element_blank())
```

```{r, fig.align='center', echo = F, fig.height=3.5}
########## MIGRATION ###########################################################

migration <- sims_mc %>%
  filter(Detected == TRUE &
           Truth <= 30 & Truth >= 0) %>%
  ggplot() +
    scale_y_continuous(breaks = seq(0,30,by=3),
                       limits = c(0,30), 
                       expand = c(0,0)) +
    scale_x_continuous(breaks = seq(-3,36,by=3), 
                       limits = c(-5,37), 
                       expand = c(0,0)) +
    theme_bw() +
    stat_bin2d(geom="raster",
               mapping = aes(x = Reconstructed, 
                             y = Truth), 
               breaks = list(x = seq(-4,36,1),
                             y = seq(0,30,1))) +
    labs(x = "Y (Reconstructed)", y = "X (Truth)",
         title = "MC Event Migration") +
    geom_abline(intercept=0,slope=1,lty="dashed",col="red") +
    facet_grid(~Theory) + coord_equal(ratio = 1)

gradient_max <- max(ggplot_build(migration)$data[[1]]$count)
gmins <- abs(floor(gradient_max/10^(floor(log10(gradient_max))-1))/
               c(5,10,20,25,50)-1)
gradient_step <- c(5,10,20,25,50)[which(gmins == min(gmins))]*
  10^(floor(log10(gradient_max))-2)

migration <- migration +
  scale_fill_gradientn(colours = viridis(11),
                       breaks = seq(0,gradient_max,gradient_step),
                       guide = guide_colourbar(barwidth = 1, 
                                               barheight = 8.5,
                                               frame.colour = "black",
                                               ticks.colour = "black",
                                               title = "Counts"))


migration
```


\begin{figure}[!ht]
    \centering
    \hrulefill
```{r, fig.height=3, fig.width=7.5, fig.align='center', echo = F}
plot1 <- ggplotGrob(continuous2 + theme(legend.position = "none",
                                        axis.title.x = element_blank()))
plot2 <- ggplotGrob(discrete_DataMC + theme(legend.position = "none",
                                        axis.title.x = element_blank()))
#list(plot1$widths,plot2$widths)

#plot1$widths[3] <- plot2$widths[9]
#plot1$widths[6] <- plot2$widths[6]

egg::ggarrange(continuous2 + 
                 theme(legend.position = "none", 
                       axis.title.x = element_blank()), 
               discrete_DataMC + 
                 theme(axis.title.x = element_blank()), nrow = 1,
               left = textGrob("Counts", rot = 90, vjust = 0.25),
               bottom = textGrob("X (Truth), Y (Reconstructed)", hjust = 0.8))
```
  \caption{\emph{\small The above plots feature two bimodal gamma distributions depicting events before and after after detector effects. (Top Left) The two continuous distributions correspond to the theoretical PDFs of the two distributions rescaled to correspond with the counts from 20,000 events. The histograms are calculated from the PDFs and correspond to the expected event counts from 20,000 simulated events. (Bottom Left) These four histograms consist of the same expected event counts as well as one instance of actual counts resulting from 20,000 simulated events. (Right) A visual study of simulated detector efficiency is provided by a side-by-side comparison of two heat maps that demonstrate the skewness and dispersion added by a simulated measurement process for detected and undetected events. This study is not intended to meaningfully represent a hypothetical distribution of undetected events in any real detector. Actual detector efficiencies are almost certainly governed by much more complicated collections of parameters.}}
  \label{BasicExample}
  \hrulefill
\end{figure}


```{r, echo=F}
### Data 1
dataReco1 <- sims_data %>%
  filter(Theory == "Theory 1" &
           Truth <= 30 & Truth >= 0 &
           Detected == TRUE) %>%
  mutate(Bin = ceiling(Reconstructed)) %>%
  count(Bin, name = "Counts") %>%
  select(Bin, Counts)
sidedata1 <- dataReco1 %>%
  filter(Bin < 1 | Bin > 30) %>% pull(Bin)
dataReco1 <- dataReco1 %>%
  filter(Bin >= 1 & Bin <= 30) %>%
  rbind(dataReco1 %>% filter(Bin < 1 | Bin > 30) %>%
          mutate(Counts = sum(Counts)) %>% 
          filter(Bin == 31))

dataTruth1 <- sims_data %>%
  filter(Theory == "Theory 1" &
           Truth <= 30 & Truth >= 0) %>%
  mutate(Bin = ceiling(Truth)) %>%
  count(Bin, name = "Counts") %>%
  select(Bin, Counts)

### Data 2
dataReco2 <- sims_data %>%
  filter(Theory == "Theory 2" &
           Truth <= 30 & Truth >= 0 &
           Detected == TRUE) %>%
  mutate(Bin = ceiling(Reconstructed)) %>%
  count(Bin, name = "Counts") %>%
  select(Bin, Counts)
sidedata2 <- dataReco2 %>%
  filter(Bin < 1 | Bin > 30) %>% pull(Bin)
dataReco2 <- dataReco2 %>%
  filter(Bin >= 1 & Bin <= 30) %>%
  rbind(dataReco2 %>% filter(Bin < 1 | Bin > 30) %>%
          mutate(Counts = sum(Counts)) %>% 
          filter(Bin == 31))

dataTruth2 <- sims_data %>%
  filter(Theory == "Theory 2" &
           Truth <= 30 & Truth >= 0) %>%
  mutate(Bin = ceiling(Truth)) %>%
  count(Bin, name = "Counts") %>%
  select(Bin, Counts)

### Data 3
dataReco3 <- sims_data %>%
  filter(Theory == "Theory 3" &
           Truth <= 30 & Truth >= 0 &
           Detected == TRUE) %>%
  mutate(Bin = ceiling(Reconstructed)) %>%
  count(Bin, name = "Counts") %>%
  select(Bin, Counts)
sidedata3 <- dataReco3 %>%
  filter(Bin < 1 | Bin > 30) %>% pull(Bin)
dataReco3 <- dataReco3 %>%
  filter(Bin >= 1 & Bin <= 30) %>%
  rbind(dataReco2 %>% filter(Bin < 1 | Bin > 30) %>%
          mutate(Counts = sum(Counts)) %>% 
          filter(Bin == 31))

dataTruth3 <- sims_data %>%
  filter(Theory == "Theory 3" &
           Truth <= 30 & Truth >= 0) %>%
  mutate(Bin = ceiling(Truth)) %>%
  count(Bin, name = "Counts") %>%
  select(Bin, Counts)

### MC 1
mcReco1 <- sims_mc %>%
  filter(Theory == "Theory 1" &
           Truth <= 30 & Truth >= 0 &
           Detected == TRUE) %>%
  mutate(Bin = ceiling(Reconstructed)) %>%
  count(Bin, name = "Counts") %>%
  select(Bin, Counts)

sidemc1 <- mcReco1 %>%
  filter(Bin < 1 | Bin > 30) %>% pull(Bin)
mcmat1 <- matrix(rep(0,31*max(sidemc1-min(sidemc1)+1)),ncol=31)
mcmat1[(1:30-min(sidemc1)+1),1:30] <- diag(1,30)
mcmat1[(sidemc1-min(sidemc1)+1),31] <- 1

mcReco1 <- mcReco1 %>%
  filter(Bin >= 1 & Bin <= 30) %>%
  rbind(mcReco1 %>% filter(Bin < 1 | Bin > 30) %>%
          mutate(Counts = sum(Counts)) %>% 
          filter(Bin == 31))

mcTruth1 <- sims_mc %>%
  filter(Theory == "Theory 1" &
           Truth <= 30 & Truth >= 0) %>%
  mutate(Bin = ceiling(Truth)) %>%
  count(Bin, name = "Counts") %>%
  select(Bin, Counts)

### MC 2
mcReco2 <- sims_mc %>%
  filter(Theory == "Theory 2" &
           Truth <= 30 & Truth >= 0 &
           Detected == TRUE) %>%
  mutate(Bin = ceiling(Reconstructed)) %>%
  count(Bin, name = "Counts") %>%
  select(Bin, Counts)

sidemc2 <- mcReco2 %>%
  filter(Bin < 1 | Bin > 30) %>% pull(Bin)
mcmat2 <- matrix(rep(0,31*max(sidemc2-min(sidemc2)+1)),ncol=31)
mcmat2[(1:30-min(sidemc2)+1),1:30] <- diag(1,30)
mcmat2[(sidemc2-min(sidemc2)+1),31] <- 1

mcReco2 <- mcReco2 %>%
  filter(Bin >= 1 & Bin <= 30) %>%
  rbind(mcReco2 %>% filter(Bin < 1 | Bin > 30) %>%
          mutate(Counts = sum(Counts)) %>% 
          filter(Bin == 31))

mcTruth2 <- sims_mc %>%
  filter(Theory == "Theory 2" &
           Truth <= 30 & Truth >= 0) %>%
  mutate(Bin = ceiling(Truth)) %>%
  count(Bin, name = "Counts") %>%
  select(Bin, Counts)


```

\section{Unfolding in particle physics}

Intro to section 2

\subsection{Inverting the Response Matrix}

In the event of Equation \eqref{eq:mat} being well-posed the obvious
approach would be to construct the unique inverse of the response matrix
$\bm{R}^{-1}$ and map the reconstructed counts back to an estimate of
the true counts via \begin{align}
  \bm{\hat\mu}=\bm{R}^{-1}\bm{n}.\label{eq:naiveInv}
\end{align} A statistical justification for this comes from performing
generalized least squares \cite{Johnson2007} fit to estimate $\bm\mu$,
which relies on approximating bin count $n_i$ as normally distributed
with mean $\nu_i$ and variance $1/\nu_i$. Minimizing the sums of squares
yields \begin{align}\min_{\bm\mu}\nabla_{\bm\mu}\bm\chi^2(\bm{\mu})
  &=\nabla_{\bm\mu}(\bm{R}\bm{\mu}-\bm{n})^T\bm{\Sigma}_{\nu}^{-1}(\bm{R}\bm{\mu}-\bm{n})\nonumber\\
  &=\nabla_{\bm\mu}(\bm{\mu}^T\bm{R}^T-\bm{n}^T)\bm{\Sigma}_{\nu}^{-1}(\bm{R}\bm{\mu}-\bm{n})\nonumber\\
  &=\nabla_{\bm\mu}(\bm{\mu}^T\bm{R}^T\bm{\Sigma}_{n}^{-1}\bm{R}\bm{\mu}-\bm{\mu}^T\bm{R}^T\bm{\Sigma}_{\nu}^{-1}\bm{n}-\bm{n}^T\bm{\Sigma}_{\nu}^{-1}\bm{R}\bm{\mu}+\bm{n}^T\bm{\Sigma}_{\nu}^{-1}\bm{n})\nonumber\\
  &=\nabla_{\bm\mu}(\bm{\mu}^T\bm{R}^T\bm{\Sigma}_{n}^{-1}\bm{R}\bm{\mu}-2\bm{\mu}^T\bm{R}^T\bm{\Sigma}_{\nu}^{-1}\bm{n}+\bm{n}^T\bm{\Sigma}_{\nu}^{-1}\bm{n})\nonumber\\
  &=2\bm{R}^T\bm{\Sigma}_{\nu}^{-1}\bm{R}\bm{\mu}-2\bm{R}^T\bm{\Sigma}_{\nu}^{-1}\bm{n}\nonumber\\
  &=0\nonumber\\
  \implies\bm{R}^T\bm{\Sigma}_{\nu}^{-1}\bm{R}\bm{\mu}&=\bm{R}^T\bm{\Sigma}_{\nu}^{-1}\bm{n}\nonumber\\
  \implies\bm{\hat\mu}&=(\bm{R}^T\bm{\Sigma}_{\nu}^{-1}\bm{R}\bm{\mu})^{-1}\bm{R}^T\bm{\Sigma}_{\nu}^{-1}\bm{n}=\bm{R}^{+}\bm{n}\label{eq:naivemu}\\
  \implies\bm{R}^{+}&=(\bm{R}^T\bm{\Sigma}_{\nu}^{-1}\bm{R}\bm{\mu})^{-1}\bm{R}^T\bm{\Sigma}_{\nu}^{-1},\label{eq:penrose}
\end{align} where $\bm{R}^{+}$ is the Moore-Penrose generalized inverse
(or pseudo-inverse) \cite{Blobel2013}, and it is assumed that
$\bm{R}^T\bm{\Sigma}_{\nu}^{-1}\bm{R}\bm{\mu}$ is not singular. The
corresponding covariance matrix is more quickly calculated by way of
\begin{align}
  \bm{\Sigma}_\mu&=\text{Cov}[\bm{\hat\mu},\bm{\hat\mu}]=\text{Cov}[\bm{R}^{+}\bm{n},\bm{R}^{+}\bm{n}]=\bm{R}^{+}\text{Cov}[\bm{n},\bm{n}]\bm{R}^{+^T}=\bm{R}^{+}\bm{\Sigma}_\nu\bm{R}^{+^T}\nonumber.
\end{align} One such inverse matrix was calculated from each of the MC
Theory simulations and both were applied to every set of the
Reconstructed counts. The residuals of these unfolding calculations are
paired with a $1 \sigma$ uncertainty from the estimated Truth counts.
They are then scaled by the expected Truth for the assumed theory and
plotted on a log10 scale in Figure \ref{NaiveRes}.

```{r, echo=F, fig.height=3}
R_mc1 <- sims_mc %>%
  filter(Theory == "Theory 1" &
           Detected == TRUE &
           Truth <= 30 & Truth >= 0) %>%
  mutate(Truth = ceiling(Truth),
         Reconstructed = ceiling(Reconstructed)) %>%
  count(Truth, Reconstructed, name = "Counts") %>%
  complete(Reconstructed=max(Reconstructed):min(Reconstructed),
           Truth=1:30, fill = list(Counts=0)) %>%
  pull(Counts) %>% matrix(byrow = T, ncol = 30) %>% 
  t() %*% mcmat1 %>% t() * matrix(rep(1/mcTruth1$Counts,each=31),ncol=30)

R_mc2 <- sims_mc %>%
  filter(Theory == "Theory 2" &
           Detected == TRUE &
           Truth <= 30 & Truth >= 0) %>%
  mutate(Truth = ceiling(Truth),
         Reconstructed = ceiling(Reconstructed)) %>%
  count(Truth, Reconstructed, name = "Counts") %>%
  complete(Reconstructed=max(Reconstructed):min(Reconstructed),
           Truth=1:30, fill = list(Counts=0)) %>%
  pull(Counts) %>% matrix(byrow = T, ncol = 30) %>% 
  t() %*% mcmat2 %>% t() * matrix(rep(1/mcTruth2$Counts,each=31),ncol=30)


Vnu1 <- diag(mcReco1$Counts/dmrat)
Vnu2 <- diag(mcReco2$Counts/dmrat)

inv_R_mc1 <- solve(t(R_mc1) %*% solve(Vnu1) %*% R_mc1) %*% 
  t(R_mc1) %*% solve(Vnu1)
inv_R_mc2 <- solve(t(R_mc2) %*% solve(Vnu2) %*% R_mc2) %*% 
  t(R_mc2) %*% solve(Vnu2)

Vmu1 <- inv_R_mc1 %*% Vnu1 %*% t(inv_R_mc1)
Vmu2 <- inv_R_mc2 %*% Vnu2 %*% t(inv_R_mc2)

data1_unfolded_mc1 <- inv_R_mc1 %*% dataReco1$Counts
data1_unfolded_mc2 <- inv_R_mc2 %*% dataReco1$Counts
data2_unfolded_mc1 <- inv_R_mc1 %*% dataReco2$Counts
data2_unfolded_mc2 <- inv_R_mc2 %*% dataReco2$Counts
data3_unfolded_mc1 <- inv_R_mc1 %*% dataReco3$Counts
data3_unfolded_mc2 <- inv_R_mc2 %*% dataReco3$Counts
mc1_unfolded_mc1 <- inv_R_mc1 %*% mcReco1$Counts
mc1_unfolded_mc2 <- inv_R_mc2 %*% mcReco1$Counts
mc2_unfolded_mc1 <- inv_R_mc1 %*% mcReco2$Counts
mc2_unfolded_mc2 <- inv_R_mc2 %*% mcReco2$Counts
```


\begin{figure}[!ht]
    \centering
```{r, echo=F,fig.height=3.5, fig.align='center'}
bins_hat <- tibble(Theory = c(rep(c(rep(c("Theory 1",
                                          "Theory 2"),each=30),
                                    rep(c("Theory 1",
                                          "Theory 2"),each=30)),2),
                              c(rep(c("Theory 1",
                                      "Theory 2"),each=60),
                                  rep(c("Theory 1",
                                        "Theory 2"),each=60))),
                   `Assumed Theory` = c(rep(c(rep(c("Theory 1",
                                                    "Theory 2"),each=30),
                                    rep(c("Theory 1",
                                          "Theory 2"),each=30)),2),
                              c(rep(rep(c("Theory 1",
                                          "Theory 2"),each=30),4))),
                   LBin = rep(0:29,16),
                   Bin = rep(1:30,16),
                   LCounts = c(dataReco1$Counts[c(1,1:29)],
                               dataReco2$Counts[c(1,1:29)],
                               mcReco1$Counts[c(1,1:29)]/dmrat,
                               mcReco2$Counts[c(1,1:29)]/dmrat,
                               dataTruth1$Counts[c(1,1:29)],
                               dataTruth2$Counts[c(1,1:29)],
                               mcTruth1$Counts[c(1,1:29)]/dmrat,
                               mcTruth2$Counts[c(1,1:29)]/dmrat,
                               data1_unfolded_mc1[c(1,1:29)],
                               data1_unfolded_mc2[c(1,1:29)],
                               data2_unfolded_mc1[c(1,1:29)],
                               data2_unfolded_mc2[c(1,1:29)],
                               mc1_unfolded_mc1[c(1,1:29)]/dmrat,
                               mc1_unfolded_mc2[c(1,1:29)]/dmrat,
                               mc2_unfolded_mc1[c(1,1:29)]/dmrat,
                               mc2_unfolded_mc2[c(1,1:29)]/dmrat),
                   Counts = c(dataReco1$Counts[1:30],
                              dataReco2$Counts[1:30],
                              mcReco1$Counts[1:30]/dmrat,
                              mcReco2$Counts[1:30]/dmrat,
                              dataTruth1$Counts,
                              dataTruth2$Counts,
                              mcTruth1$Counts/dmrat,
                              mcTruth2$Counts/dmrat,
                              data1_unfolded_mc1,
                              data1_unfolded_mc2,
                              data2_unfolded_mc1,
                              data2_unfolded_mc2,
                              mc1_unfolded_mc1/dmrat,
                              mc1_unfolded_mc2/dmrat,
                              mc2_unfolded_mc1/dmrat,
                              mc2_unfolded_mc2/dmrat),
                   Treatment = c(rep(c("Reconstructed","Truth"),each=4*30),
                                 rep("TruthHat",240)),
                   Source = c(rep("Data",2*30),rep("MC",2*30),
                              rep("Data",2*30),rep("MC",2*30),
                              rep("Data",4*30),rep("MC",4*30)),
                   Name = c(rep("Reconstucted Theory 1 Data",30),
                            rep("Reconstucted Theory 2 Data",30),
                            rep("Reconstucted Theory 1  MC",30),
                            rep("Reconstucted Theory 2  MC",30),
                            rep("Truth Theory 1 Data",30),
                            rep("Truth Theory 2 Data",30),
                            rep("Truth Theory 1 MC",30),
                            rep("Truth Theory 2 MC",30),
                            rep("Unfolded Theory 1 Data",60),
                            rep("Unfolded Theory 2 Data",60),
                            rep("Unfolded Theory 1 MC",60),
                            rep("Unfolded Theory 2 MC",60)),
                   Error = c(sqrt(diag(Vnu1))[1:30],
                             sqrt(diag(Vnu2))[1:30],
                             rep(0,20),
                             sqrt(diag(Vmu1)),
                             sqrt(diag(Vmu2)),
                             rep(0,11*20),
                             sqrt(diag(Vmu1)),
                             sqrt(diag(Vmu2)),
                             sqrt(diag(Vmu1)),
                             sqrt(diag(Vmu2))))

bins_hat <- bins_hat %>%
  mutate(Step = "Counts") %>%
  rbind(bins_hat %>% 
          filter(Treatment == "TruthHat") %>%
          mutate(LCounts = rep(bins_hat %>% 
                                 filter(Source == "MC" & 
                                          Treatment == "Truth") %>% 
                                 pull(LCounts),3) - LCounts,
                 Counts = rep(bins_hat %>% 
                                filter(Source == "MC" & 
                                         Treatment == "Truth") %>% 
                                pull(Counts),3) - Counts,
                 Step = "Residuals")) %>%
  mutate(across(Name, factor, 
                levels = c("Unfolded Theory 1 Data",
                           "Unfolded Theory 2 Data",NA,
                           "Unfolded Theory 1 MC",
                           "Unfolded Theory 2 MC",NA)))


asinh_trans <- function(){
  trans_new(name = 'asinh', transform = function(x) asinh(x), 
            inverse = function(x) sinh(x))
}
fancy_scientific <- function(l) {
     # turn in to character string in scientific notation
     l <- format(l, scientific = TRUE)
     # replace 0e+00 with 0
     l <- gsub("0e\\+00","0",l)
     # remove + after exponent, if exists. E.g.: (3x10^+2 -> 3x10^2)
     l <- gsub("e\\+","e",l)
     # turn the 'e+' into plotmath format
     l <- gsub("e", "\\10^", l)
     parse(text=l)
}

ggplot() + 
  theme_bw() +
  geom_hline(yintercept = 0, linetype = "dashed", alpha = 0.5) +
  geom_rect(data = bins_hat %>%
              filter(Treatment == "TruthHat" &
                          Step == "Residuals") %>%
              mutate(Cplus = (Counts + sqrt(rep(c(diag(Vmu1),diag(Vmu2)),3)))/
                       rep(c(mcTruth1$Counts/dmrat,mcTruth2$Counts/dmrat),3),
                     Cminus = (Counts - sqrt(rep(c(diag(Vmu1),diag(Vmu2)),3)))/
                       rep(c(mcTruth1$Counts/dmrat,mcTruth2$Counts/dmrat),3)),
            mapping = aes(xmin = LBin,
                          ymin = Cminus,
                          xmax = Bin,
                          ymax = Cplus,
                          fill = `Assumed Theory`),
            alpha = 0.3) +
  geom_segment(data = bins_hat %>%
                 filter(Treatment == "TruthHat"&
                          Step == "Residuals"),
               mapping = aes(x = LBin,
                             y = Counts/rep(c(mcTruth1$Counts/dmrat,
                                              mcTruth2$Counts/dmrat),3),
                          xend = Bin,
                          yend = Counts/rep(c(mcTruth1$Counts/dmrat,
                                              mcTruth2$Counts/dmrat),3),
                         color = `Assumed Theory`),
               alpha = 0.9) +
  geom_segment(data = bins_hat %>%
                 filter(Treatment == "TruthHat"&
                          Step == "Residuals"),
               mapping = aes(x = LBin,
                             y = LCounts/rep(c(1,mcTruth1$Counts[-30]/dmrat,
                                               1,mcTruth2$Counts[-30]/dmrat),3),
                          xend = LBin,
                          yend = Counts/rep(c(mcTruth1$Counts/dmrat,
                                              mcTruth2$Counts/dmrat),3),
                         color = `Assumed Theory`),
               alpha = 0.9) +
  scale_x_continuous(breaks = seq(0,30,by=3), 
                       limits = c(0,30), 
                       expand = c(0,0)) +
  scale_y_continuous(trans="asinh",
                     breaks = c(-10^(4:1),0,10^(1:4)),
                     labels = c(-10^(4:1),0,10^(1:4))) +
  scale_color_manual(labels = c("Theory 1","Theory 2"),
                     breaks = c("Theory 1","Theory 2"),
                     values = c("#5e81b5","#e19c24")) +
  scale_fill_manual(labels = c("Theory 1","Theory 2"),
                    breaks = c("Theory 1","Theory 2"),
                    values = c(alpha("#5e81b5",0.3),
                               alpha("#e19c24",0.3))) +
  facet_wrap(~Name, drop = F, ncol = 3) +
  theme(legend.position = c(0.95, 0.1), legend.justification = c(1, 0),
        axis.text = element_text(size=8)) +
  labs(title = 
         expression(Residuals~of~estimated~true~counts~using~
                      (widehat(mu)==R^paste("+")*~n)), x = "X (Truth)", 
       y = expression(frac(mu~-~widehat(mu),mu)~~~~~~
                      paste("Residuals as % of\n Assumed Truth")))
```
\caption{\emph{\small Plots featuring the ratio of the residuals to the assumed theory's expected truth for the five sets of reconstructed counts when unfolded using response matrices derived from MC simulations for both Theory 1 and Theory 2. Note the log10 scale of the vertical axes, the large uncertainties, and negative estimated counts. The MC results are exact when unfolded under the correct theory for tautological reasons.}}
  \label{NaiveRes}
\end{figure}

This degree of failure is disastrous, but one thing to notice here is that this
problem does not satisfy Condition 3 of a well-posed problem as
described in Section \ref{intro}. That the inverse calculated here maps
$\bm{n}$ to negative counts indicates that the estimate
$\bm{\hat\mu}\notin\mathcal{U}$, the space of allowable solutions
defined at the beginning of Section \ref{discret}.

```{r echo = F}

C <- rbind(rep(0,30),cbind(diag(1,29),rep(0,29))) + 
  rbind(cbind(rep(0,29),diag(1,29)),rep(0,30)) +
  diag(c(-1,rep(-2,28),-1)) + diag(10^(-3),30)

A1 <- R_mc1 %*% diag(mcTruth1$Counts)
Ainv1 <- svd(A1)$v %*% solve(diag(svd(A1)$d)) %*% t(svd(A1)$u)

plot(1:30, svd(A1)$d, log="y")

#max(svd(A1)$d)/min(svd(A1)$d)

#svd(A1)$v %*% solve(diag(svd(A1)$d))^2 %*% t(svd(A1)$v)

y1 <- dataReco1$Counts/sqrt(mcReco1$Counts)
#t(svd(A1)$u) %*% y1


B1 <- Vnu1
Q1 <- svd(B1)$u
R1 <- diag(svd(B1)$d)
As1 <- t(t(Q1) %*% solve(R1)) %*% A1
Xinv1 <- diag(0,30)
for(j in 1:30){
  for(k in 1:30){
    Xinv1[j,k] <- (1/prod(mcTruth1$Counts[c(j,k)]))*sum(As1[,j]*As1[,k])
  }
}
bs1 <- diag(solve(R1)) * (Q1 %*% mcReco1$Counts)
U1 <- svd(As1 %*% solve(C))$u
V1 <- svd(As1 %*% solve(C))$v
d <- t(U1) %*% bs1

plot(1:30, abs(d), log="y")
abline(h = sqrt(pi)/2, lty = 2)

plot(1:30,rev(cumsum(rev(abs(d)))/(1:30)),log="y")
abline(h = sqrt(pi)/2, lty = 2)

A2 <- R_mc2 %*% diag(mcTruth2$Counts)
B2 <- Vnu2
Q2 <- svd(B2)$u
R2<- diag(svd(B2)$d)

inv_R_mc1 <- solve(t(R_mc1) %*% solve(Vnu1) %*% R_mc1) %*% t(R_mc1) %*% solve(Vnu1)
inv_R_mc2 <- solve(t(R_mc2) %*% solve(Vnu2) %*% R_mc2) %*% t(R_mc2) %*% solve(Vnu2)

Vmu1 <- inv_R_mc1 %*% Vnu1 %*% t(inv_R_mc1)
Vmu2 <- inv_R_mc2 %*% Vnu2 %*% t(inv_R_mc2)

data1_unfolded_mc1 <- inv_R_mc1 %*% dataReco1$Counts
data1_unfolded_mc2 <- inv_R_mc2 %*% dataReco1$Counts
data2_unfolded_mc1 <- inv_R_mc1 %*% dataReco2$Counts
data2_unfolded_mc2 <- inv_R_mc2 %*% dataReco2$Counts
data3_unfolded_mc1 <- inv_R_mc1 %*% dataReco3$Counts
data3_unfolded_mc2 <- inv_R_mc2 %*% dataReco3$Counts
mc1_unfolded_mc1 <- inv_R_mc1 %*% mcReco1$Counts
mc1_unfolded_mc2 <- inv_R_mc2 %*% mcReco1$Counts
mc2_unfolded_mc1 <- inv_R_mc1 %*% mcReco2$Counts
mc2_unfolded_mc2 <- inv_R_mc2 %*% mcReco2$Counts
```

\newpage

\appendix

\section{Supplementary Mathematical Definitions and Derivations}

\subsection{Hilbert Spaces}\label{appHilbert}

Hilbert spaces are a prominent feature in the field of
functional analysis. They see significant application in partial
differential equations, quantum mechanics, and signal processing, where
they are commonly implemented in the performance of Fourier analysis.
Mathematically they represent an extension beyond the real and complex
geometric-like vector spaces developed by earlier generalizations of
Euclidean spaces in the 19th century. Developments in real analysis at
the beginning of the 20th century lead the spaces of functions and
sequences to being conceptualized as linear spaces in their own right.

As extensions of previously understood spaces they necessarily exist at
the intersection of several other important spaces that aught to be
understood beforehand. With that said, the following definitions come
from Rudin in \cite{Rudin1991}. To start, a \textbf{vector space}, as
defined here, consists of a set $X$ of vectors for which addition and
scalar multiplication are defined such that for all $x,y,z\in X$ and any
complex number $\alpha\in\mathbb{C}$
\begin{enumerate}
  \item there exists a vector in $X$ such that
    \begin{enumerate}
      \item addition is commutative: $x+y=y+x$,
      \item addition is associative: $x+(y+z)=(x+y)+z$,
    \end{enumerate}
  \item $\alpha x$ exists in $X$ such that $1x=x$, $0x=0$ (the zero vector), and multiplication is distributive:
    \begin{enumerate}
      \item $\alpha(\beta x)=(\alpha\beta x)$,
      \item $\alpha(x+y)=\alpha x+\alpha y$, and
      \item $(\alpha +\beta)x=\alpha x+\beta x$.
    \end{enumerate}
\end{enumerate}
The range of $\alpha$ above describes a complex vector space. If
$\alpha$ is restricted to the reals $\mathbb{R}$, then $X$ is considered
a real vector space. Note that vector spaces include more than just
traditional coordinate-style vectors, but also include function spaces
such as the vector space of all polynomials with degree of at most $n$,
which has the basis $\{1,x,x^2,\dots,x^{n-1},x^n\}$.

Typically associated in applications, metric spaces form a another
relevant set of spaces that has some significant overlap with the vector
spaces. A space $X$ is said to be a \textbf{metric space} if for all
$x,y\in X$ there exists an operator $d(x,y)$ that maps them to a
nonnegative real number that defines their distance from each other
within $X$. The properties of this operator are
\begin{enumerate}
  \item $0\leq d(x,y)<\infty$ for all $x$ and $y\in X$,
  \item $d(x,y)=0$ iff $x=y$,
  \item $d(x,y)=d(y,x)$ for all $x$ and $y\in X$,
  \item $d(x,z)\leq d(x,y)+d(y,z)$ for all $x$, $y$, $z\in X$.
\end{enumerate}
For a metric space $X$, the distance operator $d$ is referred to as the
metric on $X$. The intersection of the vector and metric spaces form the
set of normed spaces. As an extension of the conditions thus far, a
space $X$ is a \textbf{normed space} if $\forall x\in X$ there exists a
nonnegative real number $\vert\vert x\vert\vert$, called the
\textbf{norm} of $x$ such that
\begin{enumerate}
  \item $\vert\vert x+y\vert\vert\leq\vert\vert x\vert\vert+\vert\vert y\vert\vert\;\forall x,y\in X$,
  \item $\vert\vert\alpha x\vert\vert = \vert\alpha\vert\,\vert\vert x\vert\vert$ if $x\in X$ and $\alpha$ is a scalar,
  \item $\vert\vert x\vert\vert>0$ if $x\neq 0$.
\end{enumerate}
Such a set is said to be \textbf{complete} if every
\textbf{Cauchy sequence} in $X$ converges to a point in $X$. A Cauchy
sequence in a metric space $X$ is any sequence $\{x_n\}$ that
$\forall\varepsilon>0$ there exists an integer $N$ such that
$d(x_m,x_n)<\varepsilon$ when $m>N$ and $n>N$. A quick example of this
is the sequence defined by $x_n=\sqrt{n}$. For some starting $x_m$ and
$x_n$ where $m-n=\delta$, we have \begin{align}
d(x_m,x_n)
  &=\sqrt{m}-\sqrt{n}\nonumber\\
  &=\sqrt{n+\delta}-\sqrt{n}\nonumber\\
  &=(\sqrt{n+\delta}-\sqrt{n})\frac{\sqrt{n+\delta}+\sqrt{n}}{\sqrt{n+\delta}+\sqrt{n}}\nonumber\\
  &=\frac{n+\delta-n}{\sqrt{n+\delta}+\sqrt{n}}\nonumber\\
  &=\frac{\delta}{\sqrt{n}(\sqrt{1+\delta/n}+\sqrt{1})}\nonumber\\
  &<\frac{1}{\sqrt{n}}\left(\frac{\delta}{2}\right)<\varepsilon\nonumber\\
  \implies n &> \left(\frac{\delta}{2\varepsilon}\right)^2.\nonumber
\end{align} Noting that for constant $\delta$ the limit of
$\frac{1}{\sqrt{n}}\left(\frac{\delta}{2}\right)$ as
$n\longrightarrow\infty$ is the zero vector (the point of convergence)
would also be sufficient to show that $x_n=\sqrt{n}$ is a Cauchy
sequence.

\begin{wrapfigure}{r}{0.55\textwidth}
  \centering
```{r, echo=F, fig.height=3.6, fig.width=3.6, fig.align='center'}
venndiagram <- tibble(X0 = c( 0.0,0.0,
                             -1.3,0.9),
                      Y0 = c(-0.75, 0.75,
                              0.00,-1.05),
                      R = c(4.50,4.5,
                            2.75,2.3),
                      space = c("Vector Space",
                                "Metric Space",
                                "Inner Product Space",
                                "Banach Space")) %>%
  mutate(across(space, factor, 
                levels = c("Vector Space",
                           "Metric Space",
                           "Inner Product Space",
                           "Banach Space")))
venntext <- tibble(X = c(0.0, 1.90,
                         0.0,-2.25,
                         1.7, 0.00),
                   Y = c( 4.45, 2.10,
                         -4.45, 0.75,
                         -1.70,-0.60),
                   theta = c( 0,-35,
                              0, 50,
                             55,  0),
                   text = c("Vector Space\n(vectors)",
                            "Normed Space\n(length)",
                            "Metric Space\n(distance)", 
                            "Inner Product\nSpace\n(angle/orthogonality)",
                            "Banach Space\n(completeness)",
                            "Hilbert\nSpace"))
                           

ggplot() +
  coord_fixed() + theme_void() +
  geom_circle(data = venndiagram,
              mapping = aes(x0 = X0, y0 = Y0, r = R,
                            fill = space)) +
  geom_text(data = venntext,
            mapping = aes(x = X, y = Y, angle = theta,
                          label = text), size = 3.5) +
  theme(legend.position = "none") +
  scale_fill_manual(values = c(alpha("#b91500",0.35),
                               alpha("#45a500",0.35),
                               alpha("#ef6800",0.25),
                               alpha("#0000e0",0.25)))
  
```
  \caption{\emph{A Venn diagram representing the intersection and nesting of the spaces described in Appendix \ref{appHilbert}.}}
  \label{spaceVenn}
  \vspace{-30pt}
\end{wrapfigure}

Incidentally, a normed vector space that is complete as defined here
meets the definition of a \textbf{Banach space}. An additional subset of
the normed vector spaces consists of those spaces in which for all
$x,y\in X$ there exists a real or complex number $\langle x,y\rangle$
defined by an operator called the \textbf{inner product}. For all
$x,y,z\in X$ this operation must satisfy
\begin{enumerate}
  \item $\langle x,y\rangle=\langle y,x\rangle^*$ (where the ${}^*$ represents the complex conjugate),
  \item $\langle x+y,z\rangle=\langle x,z\rangle+\langle y,z\rangle$,
  \item $\langle \alpha x,y\rangle=\alpha\langle x,y\rangle$ (for $\alpha\in\mathbb{C}$),
  \item $\langle x,x\rangle\geq0$, and
  \item $\langle x,x\rangle=0$ iff $x=0$.
\end{enumerate}
A space that satisfies these requirements forms an
\textbf{inner product space}, and the inner product defined in such a
space relates to the form of its norm, such that
$\vert\vert x\vert\vert=\langle x,x\rangle^{1/2}$. Finally, at the
intersection of Banach spaces and inner product spaces are the Hilbert
spaces. I.e. a \textbf{Hilbert space} is a complete vector space with an
inner product defined by its norm.

A commonly presented example is the $L^2$ function space, which consists
of functions that are square integrable, i.e. if
$f(x)\in L^2\implies \vert\vert f(x)\vert\vert^2=\int_\chi\vert f(x)\vert^2dx<\infty$,
where $\chi$ is the domain of $x$. The subset $L^2[-\pi,\pi]$, where
$\chi=[-\pi,\pi]$, has the well known Fourier series as a basis, which
is commonly written such that for $f(x)\in L^2[-\pi,\pi]$ \begin{align}
  f(x)=\frac{a_0}{2} + \sum_{n=1}^\infty\left[a_n\cos(nx)+b_n\sin(nx)\right],\nonumber
\end{align} where $$\begin{matrix}
  a_n=\frac{1}{\pi}\int_{-\pi}^\pi f(x)\cos(nx)dx \\ \text{and} \\ b_n=\frac{1}{\pi}\int_{-\pi}^\pi f(x)\sin(nx)dx.
\end{matrix}$$ Verification that this basis meets all the requirements
laid out so far is beyond the scope of this paper.

\section{Description of simulations} \label{simuApp}

The Cauchy processes specifically are of the form
\begin{align}
  X_{1,i}&\sim \text{Cauchy}(11,4)\nonumber\\
  X_{2,i}&\sim \text{Cauchy}(18,4)\nonumber\\
  X_{3,i}&\sim \text{Cauchy}(14,5)\nonumber
\end{align} 
The probabilities under Theory 1 (in which only the first two processes take place) is $\bm{p}=\{0.3,0.7\}$. The probabilities governing Theory 2 are generated from $\bm{p}$ by
\begin{align}
  \bm{p'}
    &=\bm{\Lambda}\bm{p}
    =\begin{pmatrix}0.75 & 0 \\ 0 & 0.75 \\ 0.6 & 0.1\end{pmatrix}\begin{pmatrix}0.3\\0.7\end{pmatrix}
    =\begin{pmatrix}0.225 \\ 0.525 \\ 0.25 \end{pmatrix}\nonumber
\end{align}
the effects of detector smearing is represented by i.i.d random variables generated by the conditional Gaussian process
$$\varepsilon_i\sim N\left(\mu(X_i),\sigma(X_i)^2\right),$$
the mean and variance of which are functions defined by 
\begin{align}
  \mu(X_i=x)&=-x^{1/4}\;\;\text{and}\nonumber\\
  \sigma(X_i=x)&=\log\left(\frac{x+10}{4}\right).\nonumber
\end{align} 
The efficiency is similarly conditional on $X_i$, and is
modeled here as a Bernoulli process with i.i.d random variables
$\epsilon_i\sim\text{Bernoulli}\big(p(X_i)\big)$, where the average
detection rate (when $\epsilon_i=1$) is a function of the form
\begin{align}
  p(X_i=x)=1-e^{-\sqrt{x}/4}.\nonumber
\end{align}

\section{Bin-by-bin}

In this approach a multiplicative \textbf{correction factor} $C_i$ is
applied to the observed number of signal events $n_i$ for each bin to
produce the estimator of $\mu_i$ \cite{Cowan1998}, 
\begin{align}
  \hat{\mu}_i &= C_in_i.\label{eq:binest}
\end{align} 
The correction factors are determined by taking the
respective ratios of a bin's MC simulated truth signal event counts
$\mu_i^{\text{MC}}$ to its MC simulated reconstructed signal event
counts $\nu_i^{\text{MC}}$,
\begin{align}
  C_i=\frac{\mu_i^{\text{MC}}}{\nu_i^{\text{MC}}}.\label{eq:cfact}
\end{align}
The covariance matrix $\bm{\Sigma}_\mu$ of this estimator derives
naturally from Equations \eqref{eq:cov} and \eqref{eq:binest}, with
components 
\begin{align}
  \Sigma^\mu_{ij}
    &=\text{Cov}[\hat\mu_i,\hat\mu_j]\nonumber\\
    &=C_iC_j\text{Cov}[n_i,n_j]\nonumber\\
    &=C_i^2\delta_{ij}\nu_i\nonumber\\
    &=\left(\frac{\mu_i^{\text{MC}}}{\nu_i^{\text{MC}}}\right)^2\delta_{ij}\nu_i.\label{eq:bincov}
\end{align} 
The expectation value of the estimate can be calculated
easily enough as well, and with it the bias 
\begin{align}
  \text{Bias}[\hat{\mu}_i]
    &=E_i[\hat{\mu}_i]-\mu_i\nonumber\\ 
    &=C_iE[n_i]-\mu_i\nonumber\\ 
    &=\frac{\mu_i^{\text{MC}}}{\nu_i^{\text{MC}}}\nu_i-\mu_i\nonumber\\
    &=\left(\frac{\mu_i^{\text{MC}}}{\nu_i^{\text{MC}}}-\frac{\mu_i}{\nu_i}\right)\nu_i.\label{eq:binbias}
\end{align}

```{r, echo = F, fig.align='center',fig.height=3}
BinByBin_bins <- tibble(Theory = c(rep(c("Theory 1","Theory 2"),each=60),
                              rep(c("Theory 1","Theory 2"),each=30)),
                   `Assumed Theory` = rep(rep(c("Theory 1","Theory 2"),
                                              each=30),3),
                   LBin = rep(0:29,6), Bin = rep(1:30,6),
                   LCounts = c(((mcTruth1$Counts/mcReco1$Counts[1:30])*
                                  dataReco1$Counts[1:30])[c(1,1:29)],
                               ((mcTruth2$Counts/mcReco2$Counts[1:30])*
                                  dataReco1$Counts[1:30])[c(1,1:29)],
                               ((mcTruth1$Counts/mcReco1$Counts[1:30])*
                                  dataReco2$Counts[1:30])[c(1,1:29)],
                               ((mcTruth2$Counts/mcReco2$Counts[1:30])*
                                  dataReco2$Counts[1:30])[c(1,1:29)],
                               mcTruth1$Counts[c(1,1:29)]/dmrat,
                               mcTruth2$Counts[c(1,1:29)]/dmrat),
                   Counts = c((mcTruth1$Counts/mcReco1$Counts[1:30])*
                                dataReco1$Counts[1:30],
                              (mcTruth2$Counts/mcReco2$Counts[1:30])*
                                dataReco1$Counts[1:30],
                              (mcTruth1$Counts/mcReco1$Counts[1:30])*
                                dataReco2$Counts[1:30],
                              (mcTruth2$Counts/mcReco2$Counts[1:30])*
                                dataReco2$Counts[1:30],
                              mcTruth1$Counts/dmrat,
                              mcTruth2$Counts/dmrat),
                   Name = c(rep(c("Theory 1 Data",
                                  "Theory 2 Data"),each=60),
                            rep("MC",60)))



BinByBin_bins_res <- BinByBin_bins %>% 
  filter(Name == "Theory 1 Data" | Name == "Theory 2 Data") %>%
  mutate(`Assumed Theory` = rep(rep(c("Theory 1 Residuals",
                                      "Theory 2 Residuals"),each=30),2),
         LCounts = (rep(c(0,mcTruth1$Counts[1:29]/dmrat,
                          0,mcTruth2$Counts[1:29]/dmrat),2) - LCounts),
         Counts = rep(c(mcTruth1$Counts/dmrat,
                        mcTruth2$Counts/dmrat),2) - Counts)


ggplot() + 
  theme_bw() + 
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_segment(data = BinByBin_bins %>% 
                 filter(Name == "MC"),
               mapping = aes(x = LBin,
                             y = Counts,
                          xend = Bin,
                          yend = Counts),
               alpha = 0.6) +
  geom_segment(data = BinByBin_bins %>% 
                 filter(Name == "MC"),
               mapping = aes(x = LBin,
                             y = LCounts,
                          xend = LBin,
                          yend = Counts),
               alpha = 0.6) +
  geom_rect(data = BinByBin_bins %>%
              mutate(ymin = Counts - 
                       rep(sqrt(c((mcTruth1$Counts^2/
                                 mcReco1$Counts[1:30]),
                              (mcTruth2$Counts^2/
                                 mcReco2$Counts[1:30]))),3),
                     ymax = Counts + 
                       rep(sqrt(c((mcTruth1$Counts^2/
                                 mcReco1$Counts[1:30]),
                              (mcTruth2$Counts^2/
                                 mcReco2$Counts[1:30]))),3),
                     xmin = LBin, xmax = Bin),
            mapping = aes(xmin=xmin,ymin=ymin,
                          xmax=xmax,ymax=ymax,
                          fill=Name),alpha=0.2) +
  geom_segment(data = BinByBin_bins %>% 
                 filter(Name == "Theory 1 Data" | Name == "Theory 2 Data"),
               mapping = aes(x = LBin,
                             y = Counts,
                          xend = Bin,
                          yend = Counts,
                         color = Name),
               alpha = 0.7) +
  geom_segment(data = BinByBin_bins %>% 
                 filter(Name == "Theory 1 Data" | Name == "Theory 2 Data"),
               mapping = aes(x = LBin,
                             y = LCounts,
                          xend = LBin,
                          yend = Counts,
                         color = Name),
               alpha = 0.7) +
  scale_x_continuous("X (Truth)",
                     breaks = seq(0,30,by=3), 
                       limits = c(0,30), 
                       expand = c(0,0)) +
  scale_color_manual(labels = c("Theory 1","Theory 2"),
                     breaks = c("Theory 1 Data","Theory 2 Data"),
                     values = c("#5e81b5","#e19c24")) +
  scale_fill_manual(labels = c("Theory 1","Theory 2"),
                     breaks = c("Theory 1 Data","Theory 2 Data"),
                     values = c(alpha("#5e81b5",0.2),
                                alpha("#e19c24",0.2),NA)) +
  guides(fill = guide_legend("Data", override.aes = list(fill=c(alpha("#5e81b5",0.2),alpha("#e19c24",0.2)))),
         color = guide_legend("Data")) +
  facet_wrap(`Assumed Theory` ~ ., ncol = 2, dir = "v")
```

```{r, echo = F, fig.align='center',fig.height=3}
ggplot() + 
  theme_bw() + 
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_rect(data = BinByBin_bins_res %>%
              mutate(ymin = Counts - 
                       rep(sqrt(c((mcTruth1$Counts^2/
                                 mcReco1$Counts[1:30]),
                              (mcTruth2$Counts^2/
                                 mcReco2$Counts[1:30]))),2)/dmrat,
                     ymax = Counts + 
                       rep(sqrt(c((mcTruth1$Counts^2/
                                 mcReco1$Counts[1:30]),
                              (mcTruth2$Counts^2/
                                 mcReco2$Counts[1:30]))),2)/dmrat,
                     xmin = LBin, xmax = Bin),
            mapping = aes(xmin=xmin,ymin=ymin,
                          xmax=xmax,ymax=ymax,
                          fill=Name),alpha=0.2) +
  geom_segment(data = BinByBin_bins_res[1:120,],
               mapping = aes(x = LBin,
                             y = Counts,
                          xend = Bin,
                          yend = Counts,
                         color = Name),
               alpha = 0.7) +
  geom_segment(data = BinByBin_bins_res[1:120,],
               mapping = aes(x = LBin,
                             y = LCounts,
                          xend = LBin,
                          yend = Counts,
                         color = Name),
               alpha = 0.7) +
  scale_x_continuous("X (Truth)",
                     breaks = seq(0,30,by=3), 
                       limits = c(0,30), 
                       expand = c(0,0)) +
  scale_color_manual(labels = c("Theory 1","Theory 2"),
                     breaks = c("Theory 1 Data","Theory 2 Data"),
                     values = c("#5e81b5","#e19c24")) +
  scale_fill_manual(labels = c("Theory 1","Theory 2"),
                     breaks = c("Theory 1 Data","Theory 2 Data"),
                     values = c(alpha("#5e81b5",0.3),
                                alpha("#e19c24",0.3))) +
  guides(fill = guide_legend("Data", override.aes = list(fill=c(alpha("#5e81b5",0.2),alpha("#e19c24",0.2)))),
         color = guide_legend("Data")) +
  facet_wrap(`Assumed Theory` ~ ., ncol = 2, dir = "v")
```

```{r, echo = F, fig.align='center',fig.height=3}

biases <- 
  c((mcTruth1$Counts/mcReco1$Counts[1:30] - 
       (bins_expected1 %>% filter(Treatment == "Truth") %>% pull(Counts))/
       (bins_expected1 %>% filter(Treatment == "Measured") %>% pull(Counts))) *
      (bins_expected1 %>% filter(Treatment == "Measured") %>% pull(Counts))/
      (bins_expected1 %>% filter(Treatment == "Truth") %>% pull(Counts)),
    (mcTruth2$Counts/mcReco2$Counts[1:30] - 
       (bins_expected1 %>% filter(Treatment == "Truth") %>% pull(Counts))/
       (bins_expected1 %>% filter(Treatment == "Measured") %>% pull(Counts))) *
      (bins_expected1 %>% filter(Treatment == "Measured") %>% pull(Counts))/
      (bins_expected1 %>% filter(Treatment == "Truth") %>% pull(Counts)),
    (mcTruth1$Counts/mcReco1$Counts[1:30] - 
       (bins_expected2 %>% filter(Treatment == "Truth") %>% pull(Counts))/
       (bins_expected2 %>% filter(Treatment == "Measured") %>% pull(Counts))) *
      (bins_expected2 %>% filter(Treatment == "Measured") %>% pull(Counts))/
      (bins_expected2 %>% filter(Treatment == "Truth") %>% pull(Counts)),
    (mcTruth2$Counts/mcReco2$Counts[1:30] - 
       (bins_expected2 %>% filter(Treatment == "Truth") %>% pull(Counts))/
       (bins_expected2 %>% filter(Treatment == "Measured") %>% pull(Counts))) *
      (bins_expected2 %>% filter(Treatment == "Measured") %>% pull(Counts))/
      (bins_expected2 %>% filter(Treatment == "Truth") %>% pull(Counts)),
    ((bins_expected1 %>% filter(Treatment == "Truth") %>% pull(Counts))/
       (bins_expected1 %>% filter(Treatment == "Measured") %>% pull(Counts))-
       (bins_expected2 %>% filter(Treatment == "Truth") %>% pull(Counts))/
       (bins_expected2 %>% filter(Treatment == "Measured") %>% pull(Counts))) *
      (bins_expected2 %>% filter(Treatment == "Measured") %>% pull(Counts))/
      (bins_expected2 %>% filter(Treatment == "Truth") %>% pull(Counts)),
    ((bins_expected2 %>% filter(Treatment == "Truth") %>% pull(Counts))/
       (bins_expected2 %>% filter(Treatment == "Measured") %>% pull(Counts))-
       (bins_expected1 %>% filter(Treatment == "Truth") %>% pull(Counts))/
       (bins_expected1 %>% filter(Treatment == "Measured") %>% pull(Counts))) *
      (bins_expected1 %>% filter(Treatment == "Measured") %>% pull(Counts))/
      (bins_expected1 %>% filter(Treatment == "Truth") %>% pull(Counts)))

biasSD <- c(sqrt(((mcTruth1$Counts/mcReco1$Counts[1:30])*
                    (bins_expected1 %>% 
                       filter(Treatment == "Measured") %>% 
                       pull(Counts))))/
                   (bins_expected1 %>% 
                      filter(Treatment == "Truth") %>% 
                      pull(Counts)),
            sqrt(((mcTruth2$Counts/mcReco2$Counts[1:30])*
                    (bins_expected1 %>% 
                       filter(Treatment == "Measured") %>% 
                       pull(Counts))))/
                   (bins_expected1 %>% 
                      filter(Treatment == "Truth") %>% 
                      pull(Counts)),
            sqrt(((mcTruth1$Counts/mcReco1$Counts[1:30])*
                    (bins_expected2 %>% 
                       filter(Treatment == "Measured") %>% 
                       pull(Counts))))/
                   (bins_expected2 %>% 
                      filter(Treatment == "Truth") %>% 
                      pull(Counts)),
            sqrt(((mcTruth2$Counts/mcReco2$Counts[1:30])*
                    (bins_expected2 %>% 
                       filter(Treatment == "Measured") %>% 
                       pull(Counts))))/
                   (bins_expected2 %>% 
                      filter(Treatment == "Truth") %>% 
                      pull(Counts)))


BinByBin_bias <- tibble(x = c(rep(c(0,rep(1:29,each=2),30,
                                     30,rep(29:1,each=2),0),4),
                               rep(c(0,rep(1:29,each=2),30),4)),
                         y = c(rep(biases[1:30]+biasSD[1:30],each=2),
                               rep(biases[30:1]-biasSD[30:1],each=2),
                               rep(biases[31:60]+biasSD[31:60],each=2),
                               rep(biases[60:31]-biasSD[60:31],each=2),
                               rep(biases[61:90]+biasSD[61:90],each=2),
                               rep(biases[90:61]-biasSD[90:61],each=2),
                               rep(biases[91:120]+biasSD[91:120],each=2),
                               rep(biases[120:91]-biasSD[120:91],each=2),
                               rep(biases[1:120],each=2)),
                         `Expectation Value` = 
                           c(rep("Theory 1\nExpectation Value",240),
                             rep("Theory 2\nExpectation Value",240),
                             rep("Theory 1\nExpectation Value",120),
                             rep("Theory 2\nExpectation Value",120)),
                         `MC Truth` = c(rep(c(rep("Theory 1",120),
                                              rep("Theory 2",120)),2),
                                        rep(c(rep("Theory 1",60),
                                              rep("Theory 2",60)),2)),
                         MCiEXPj = c(rep(c("MC1EXP1","MC2EXP1",
                                           "MC1EXP2","MC2EXP2"),each=120),
                                     rep(c("MC1EXP1","MC2EXP1",
                                           "MC1EXP2","MC2EXP2"),each=60)),
                         Signal = c(rep("Error",480),rep("CV",240)))

ggplot() + theme_bw() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_polygon(data = BinByBin_bias %>% filter(Signal == "Error"),
               mapping = aes(x=x, y=y, color=`MC Truth`, fill=`MC Truth`),
               linetype = "dotted") +
  geom_line(data = BinByBin_bias %>% filter(Signal == "CV"),
               mapping = aes(x=x, y=y, color=`MC Truth`)) +
  scale_color_manual(values = c("#5e81b5","#e19c24","#eb6235","#8fb032")) +
  scale_fill_manual(values = c(alpha("#5e81b5",0.3),alpha("#e19c24",0.3),
                               alpha("#eb6235",0.3),alpha("#8fb032",0.3))) +
  scale_x_continuous("X (Truth)",
                     breaks = seq(0,30,by=3), 
                       limits = c(0,30), 
                       expand = c(0,0)) +
  scale_y_continuous(
    name = TeX(r"($\frac{\mu_i^{{MC}}/\mu_i}{\nu_i^{{MC}}/\nu_i}-1$)"),
    breaks = seq(-0.15,0.15,0.05),
    labels = c("-0.15","-0.10","-0.05","0.00","0.05","0.10","0.15"),
    limits = c(-0.175,0.175), 
    expand = c(0,0)) +
  facet_wrap(~`Expectation Value`) +
  theme(axis.text.x = element_text(size = 8))

```

\section{R Code}
