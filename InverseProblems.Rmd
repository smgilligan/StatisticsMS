---
title: "The Structure of Ill-Posed Inverse Problems in Experimental Particle Physics"
author: "Sean Gilligan"
output: 
  pdf_document:
    citation_package: biblatex
    number_sections: TRUE
keep_tex: TRUE
bibliography: citations.bib
header-includes:
  - \usepackage{setspace}\onehalfspacing
  - \usepackage{xcolor}
  - \usepackage{bm}
  - \usepackage{amsmath}
  - \usepackage{parskip}
  - \usepackage{hyperref}
  - \usepackage{csquotes}
  - \usepackage{float}
  - \usepackage{multirow}
  - \usepackage{wrapfig}
  - \hypersetup{colorlinks=TRUE,linkcolor=red,citecolor=blue,filecolor=magenta,urlcolor=blue}
  - \newcommand{\comment}[1]{}
abstract: \singlespacing This report provides a brief look at ill-posed inverse problems within the context of measurement related data distortions in experimental particle physics. The methods they use in solving them are in general collectively referred to as unfolding. The specifics of data and data collection methods are generalized. Common features are discussed insofar as they contribute to the necessary understanding of the data and implementation of any covered methods. In order to construct a slightly more holistic picture some additional topics are briefly touched upon if they relate to other common aspects of data analysis in particle physics, but only during parts of relevant discussions where they would otherwise normally appear. One non-iterative unfolding procedure that uses extensions of common linear regression tools is discussed and applied to a simulated example.
fontsize: 12pt
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

```{r, message = F, echo=F}
library(tidyverse)
library(gridExtra)
library(grid)
library(gridtext)
library(gtable)
library(knitr)
library(RColorBrewer)
library(viridisLite)
library(viridis)
library(gtools)
library(cowplot)
library(egg)
library(scales)
library(ggforce)
library(latex2exp)
```

\section{Introduction} \label{intro}

A common problem faced in the quantitative sciences and their associated technologies is the introduction of errors during the data collection process. While the possible sources of these errors are as varied as the possible events which the data might describe, significant work has been done to develop methods the can help would-be analysts reconcile them. The requisite understanding of a scenario's underlying systematic and stochastic processes might not allow researchers to truly reverse entropy or make up for the finite resolution of a detector, but it can approximate them with a quantifiable degree of certainty. 

The applied mathematics that this involves falls within the general category of \textbf{inverse problems}, and there are a variety of labels used to refer to the procedures in its arsenal. There is the colloquially vague \textbf{unsmearing}, but there are also names that reference specific applications and methods. For the sake of simplicity, and satisfying necessary physical constraints, the manner of inverse problems addressed here will only have satisfactory solutions that involve linear operations that map from one Hilbert space\footnote{The definition of a Hilbert space is provided in Appendix \ref{appHilbert} for convenience.} to another. Symbolically this can be expressed by the equation
\begin{align}
Az=u,\nonumber
\end{align} 
where $A$ is a linear operator acting on an element $z\in Z$, the sought solution, to produce an element $u\in U$, the observed data. Within the context of the methods described here $z$
and $u$ take the form of continuous or discrete distributions that when integrated or summed over the domain of their arguments result in finite real quantities.

The difficulty of solving for $z$ can be classified into one of two camps. The easiest cases involve conditions that create a \textbf{well-posed} problem, which requires that \cite{Tikh1977}
\begin{enumerate}
  \item $\forall u\in U$ there exists a solution $z\in Z$,
  \item the solution is unique,
  \item and the problem is stable on $U$ and $Z$.
\end{enumerate}
Conditions 1 and 2 work together to imply that the inverse operator
$A^{-1}$ exists, and Condition 3 is often worded to describe the inverse
as continuous, which implies that small deviations in $u$ should
correspond to similar deviations in $z$. When one or more of
these conditions are not met, the problem is said to be
\textbf{ill-posed}, and some of the consequences of assuming otherwise
should hopefully become clear in the coming pages.

Entire books have been written on this subject that do not begin to cover the full scope of the methods developed to deal with ill-posed problems. With that in mind the hope for this short paper is for it to serve as an introduction to ill-posed problems while providing some degree of direction for those who would like to know more. The specific case of data and measurement error being uncorrelated and analyzable as continuous, parametric distributions will serve as a starting point. Being permissible under these conditions the method of deconvolution will be introduced, after which some generalization will occur and nonparametric cases with discrete representations will be become the focus.

\subsection{The Deconvolution}\label{deconvolution}

One way to characterize a basic example of a situation suitable for
being treated as a convolution would be one that should be very familiar
to anyone who has ever taken a statistics course. Assume that data
collected regarding $n$ statistical events represent the measurement of
$n$ independent and identically distributed (i.i.d.) random variables
$\bm{X}=\{X_1,X_2,\dots,X_n\}$ from a distribution of possible values
represented by the probability density function (PDF) $f_X(x)$, such
that the probability of a random variable $X_i$ having a value between
$x_a$ and $x_b$ is $$P(\:x_a<X_i<x_b\:)=\int_{x_a}^{x_b}f_X(x)\,dx$$ and
$$\int_\mathcal{X}f_X(x)\,dx=1,$$ where $\mathcal{X}$ represents the
domain of $x$. The error introduced during the measurement process is
similarly represented by a set of i.i.d. random variables
$\bm{\varepsilon}=\{\varepsilon_1,\varepsilon_2,\dots,\varepsilon_n\}$
with a PDF $f_\varepsilon(\varepsilon)$, where the sets
$\bm{\varepsilon}$ and $\bm{X}$ are assumed to be independent 
of each other. This is naive but appropriate for this simple example. 
The set of measured/reconstructed values $\bm{Y}=\{Y_1,Y_2,\dots,Y_n\}$ 
then are also i.i.d. and can be defined in terms of the preceding sets of 
variables such that for event $i\in\{1,\dots,n\}$,
\begin{align}
  Y_i
    &=g(X_i,\varepsilon_i)\nonumber\\
    &=X_i+\varepsilon_i.\label{eq:meas}
\end{align}
In light of this relationship, the corresponding PDF $f_Y(y)$ can be
found explicitly through an operation on $f_X(x)$ and
$f_\varepsilon(\varepsilon)$ using the mathematics of functional
analysis. Stated in more general terms, the empirical density function
$f_Y$ is formed from the \textbf{convolution} of the true density
function $f_X$ and the error density function $f_\varepsilon$, and is
defined by \cite{Panaretos2011}
\begin{align}
  f_Y&\equiv f_X*f_\varepsilon\label{eq:conv1}\\
  f_Y(y)&\equiv\int_\mathcal{X}f_X(x)f_\varepsilon(\varepsilon)\,dx\nonumber\\
    &=\int_\mathcal{X}f_X(x)f_\varepsilon\big(g_x^{-1}(y)\big)\left\vert J_{g_x^{-1}}(y)\right\vert dx\nonumber\\
    &=\int_\mathcal{X}f_X(x)f_\varepsilon(y-x)\,dx,\label{eq:conv2}
\end{align}
where $J$ represents the Jacobian of the transformation involved in
performing the change of basis on $f_\varepsilon$ from $\varepsilon$ to
$x$, which is necessary for the evaluation of the integral for a given
$y$. The magnitude of the Jacobian for transformation of $\varepsilon$
to $y-x$ through the manipulation of Equation \eqref{eq:meas} happens to
be $1$.

As the collection of measured values $\bm{Y}$ accumulates an estimate of
empirical density $\hat{f}_Y$ can readily be formed. However, a major
goal in an analysis of data like this is typically to develop an
accurate estimate of the true density $\hat{f}_X$. Using the information
contained in $\hat{f}_Y$ to accomplish this necessarily requires some
attempt at finding an inverse process to the convolution, i.e. the
\textbf{deconvolution}.

For cases in the form of this particular example there are a variety
approaches, but they commonly involve the Fourier transform of the
density functions $\left\{f_X,f_\varepsilon,f_Y\right\}$ into their
corresponding characteristic functions
$\left\{\phi_X,\phi_\varepsilon,\phi_Y\right\}$
\cite{Meister2009}\cite{Panaretos2011}. Minor aspects of the definition
for the Fourier transform can vary slightly between applications,
resulting primarily from the use of different scale factors and sign
conventions. Here it will be defined for some random variable
$U\in\mathbb{R}$ with density function $f_U(u)$ and random variable
$T\in\mathbb{R}$ as
\begin{align}\phi_T(t)=\int_{-\infty}^\infty f_U(u)\,e^{itu}\,du.\label{eq:ft}\end{align}
When conditions permit the inverse Fourier transform can be found via
\begin{align}f_U(u)=\int_{-\infty}^\infty \phi_T(t)\,e^{-itu}\,dt.\label{eq:ift}\end{align}
The Fourier transform is important in deconvolution methods because when
it is applied to the convolution of two density functions the link
between their respective characteristic functions becomes purely
multiplicative, i.e.
$$f_Y=f_X*f_\varepsilon\implies\phi_Y=\phi_X\phi_\varepsilon.$$ An
instructional proof of this result is provided on page 447 of
\cite{Boas2005}. The steps so far characterize a typical deconvolution
scheme, with later steps consisting of various ways to perform density
estimation and addressing issues similar to those that will be seen
ahead \cite{Meister2009}.

\subsection{Generalizing}\label{general}

The remainder of this paper is dedicated to a more generalized study of
these types of problems. With the understanding that even experts can be
fairly loose and inconsistent with their vocabulary, this paper will do
its best to provide clear definitions. To begin, while most literature
on deconvolution methods do use the word "convolution", this operation
is also referred to by the German word \textit{faltung}
\cite{Weisstein}. The latter's English translation, \textbf{folding}, is
featured prominently in the particle physics community, but refers to a
more generalized process than what is described by Equation
\eqref{eq:conv2} \cite{DAgostini1994}\cite{Adye2011}\cite{Blobel2013}.
In general, folding and \textbf{unfolding} refer to two sets of
processes within which the sets of convolution and deconvolution
processes form proper respective subsets.

One way to arrive at the desired generalization is with the help of
conditional probability. Thinking of $\{X,Y\}$ as a continuous bivariate
random vector with joint PDF $f(x,y)$ and marginal PDFs $f_X(x)$ and
$f_Y(y)$, we can define the conditional PDF of $Y$ given that $X=x$ as
function of $y$, $f(y\,\vert x)$ \cite{Casella2001}. The relationship
between these PDFs is sufficient to define any one of them in terms of
operations involving one or more of the others. As such, for $f_Y(y)$ it
can be shown
\begin{align}
  f_Y(y)
    &=\int_\mathcal{X}f(x,y)\,dx\nonumber\\
    &=\int_\mathcal{X}f(y\,\vert x)f_X(x)\,dx\nonumber\\
    &=\int_\mathcal{X}K(x,y)f_X(x)\,dx.\label{eq:fred}
\end{align}
While integrating over $x$, $f(y\,\vert x)$ is implicitly treated as a
function of both $x$ and $y$. Acknowledging this allows for
understanding Equation \eqref{eq:fred} as a Fredholm integral of the
first kind with a Kernel function $K(x,y)$ that reflects the physical
measurement process \cite{Blobel2011}. The relationship between $x$ and
$y$ in $K(x,y)$ is not defined, but when the kernel is a function of the
difference of its arguments, such that $K(x,y)=K(y-x)$, Equation
\eqref{eq:fred} becomes the convolution described in Equation
\eqref{eq:conv2}.

In particle physics experiments, analysts make use of Monte-Carlo (MC)
simulations to estimate detector response to randoms samples from some
true distribution $f_X(x)^{\text{MC}}$, which is itself estimated by way
of MC simulations using models that typically contain theory being
tested by the experiment in question. The resulting measured
distribution $f_Y(y)^{\text{MC}}$ grants implicit knowledge of $K(x,y)$
by way of Equation \eqref{eq:fred} \cite{Blobel2013}. Finding the
inverse of this Kernel is then the goal, as it should in theory allow
for the mapping of experimental observations $\bm Y$, as randomly
sampled from $f_Y(y)$, back to their true values $\bm X$.

\subsection{Discretization} \label{discret}

In practice researchers are often dealing with estimates $\hat f_X$,
$\hat f_Y$, $\hat f_X^{\text{MC}}$, and $\hat f_Y^{\text{MC}}$, and the
sets of data that contribute to these estimates are organized by bin
into histograms that form unnormalized granular approximations of their
true distributions, which to an extent is a natural result of data
digitization, and is required for most modern computational methods. 
Thinking in terms of these histograms allows for the reformulation of 
Equation \eqref{eq:fred} into the linear matrix equation: 
\begin{align}
  \bm\nu = \bm{R}\bm\mu.\label{eq:mat}
\end{align}
The vectors $\bm\nu$, $\bm\mu$ and matrix $\bm{R}$ relate to their
continuous counterparts by \cite{Blobel2013}: \begin{align}
  \text{true distribution }f_X(x)&\longrightarrow\bm\mu\,\in\,\mathcal{U}\equiv\{\mathbb{R}^M_{+}\cup\bm{0}\}\text{ the unknown true bin counts,}\nonumber\\
  \text{measured distribution }f_Y(y)&\longrightarrow\bm\nu\,\in\,\mathcal{V}\equiv\{\mathbb{R}^N_{+}\cup\bm{0}\}\text{ the measured bin counts,}\nonumber\\
  \text{Kernel }K(x,y)&\longrightarrow\bm{R}\;\;\text{the rectangular }N\text{-by-}M\text{ \bf response matrix}\text{.}\nonumber
\end{align}
The components of vectors $\bm\nu$ and $\bm\mu$ represent
the number of events that have occurred within the regions of $x$ and
$y$ that define the components' corresponding bins. For $i=1,\dots,N$
and $j=1,\dots,M$ the components of matrix $\bm R$ are defined by the
conditional probability \cite{Cowan1998} \begin{align}
  R_{ij}&=P(\text{measured value in bin }i\vert\text{true value in bin }j)\nonumber\\
        &=\frac{P(\text{measured value in bin }i\text{ and true value in bin }j)}{P(\text{true value in bin }j)}\nonumber\\
        &=\frac{\int_{\text{bin }i}\int_{\text{bin }j}K(x,y)f_X(x)dx\,dy}{\int_{\text{bin }j}dx\,f_X(x)}\nonumber\\
        &\equiv P(\nu_i\vert\mu_j).\label{eq:Rij}
\end{align} In terms of $P(\nu_i\vert\mu_j)$ the full response matrix
then has the form \begin{align}
  \bm{R}=\begin{pmatrix}
    P(\nu_1\vert\mu_1)     & P(\nu_1\vert\mu_2)     & \dots  & P(\nu_1\vert\mu_{N})   \\
    P(\nu_2\vert\mu_1)     & P(\nu_2\vert\mu_2)     & \cdots & P(\nu_2\vert\mu_{N})   \\
    \vdots                 & \vdots                 & \ddots & \vdots                 \\
    P(\nu_{M}\vert\mu_1)   & P(\nu_{M}\vert\mu_2)   & \dots  & P(\nu_{M}\vert\mu_{N})
  \end{pmatrix}.\label{eq:Rmat}
\end{align} With these definitions Equation \eqref{eq:mat} tells us that
an event produced in bin $\mu_j$ has some probability $\geq 0$ of being
measured in each of the $N$ bins of $\bm\nu$, and that each bin count
$\nu_i$ receives potential contributions from each of the $M$ bins in
$\bm\mu$, i.e.
\begin{align}
  \nu_i &= \sum_{j=1}^MR_{ij}\mu_j\;\text{     and}\label{eq:bini}\\
  \frac{\partial\nu_i}{\partial\mu_j}&=R_{ij}\label{dnudmu}.
\end{align}
The number of bins are typically set such that $M\leq N$, with the
convention $N=M+1$ being common. A higher number of bins in the measured
distribution reflects that the measuring process is expected to map some
events in $\bm X$ to values of $\bm Y$ that are outside the region of
values that define the initial $M$ bins. These one or more extra bins
are intended to account for all the possible values that a particular
event could be mapped to, such that for a given event starting in bin
$j$ one might expect the probabilities of it being measured in each of
the $N$ final bins to sum to $1$.

However, in practice there are a variety of constraints on events that
can either result in them not being included for analysis or even
prevent them from being detected at all. For example, an analyst might
cut events observed in regions of a detector that result in insufficient
data collection, or maybe some event information carriers miss the
detector entirely, resulting in such events going unseen. In either case
the effect of these missing events is described using the detector
\textbf{efficiency}, and represented mathematically by the $N$-vector
$\bm\epsilon$, where component $\epsilon_j$ is the efficiency of the
$j$th true bin
defined\footnote{In the continuous case it is typically written as $\epsilon(x)$, and understood to be the conditional probability of an event producing any measured value given it has a true value of $x$. It is typically absorbed into $K(x,y)$ where it goes on to manifest within $\bm R$ in the manner shown in Equation \eqref{eq:eff} \cite{Blobel2013}.}
by \cite{Cowan1998}:
\begin{align}\sum_{i=1}^{N}P(\nu_i\vert\mu_j)=\sum_{i=1}^{N}R_{ij}=\epsilon_j\leq 1.\label{eq:eff}\end{align}
In contrast to this are contributions to measured counts from
\textbf{background} processes. Just as events produced in a region of
interest can be smeared out of it, events produced out of it can be
smeared into it. The crossed barrier could correspond to the variable of
interest, but it can also include events excluded from analysis due to
assigned constraints on other variables that describe the event.
Background processes can be studied and dealt with prior to the
unfolding procedures described in the paper. It is briefly mentioned here to provide a slightly more holistic picture of particle physics analyses. Mathematically, background would be included by modifying
Equation \eqref{eq:bini} to read
\begin{align}\nu_i = \sum_{j=1}^MR_{ij}\mu_j+\beta_i,\end{align} where
$\beta_i$ is the $i$th component of the $N$-vector $\bm\beta$, which
represents the binned background counts. This leads to equations like
$\nu_i^{\text{sig}}=\nu_i-\beta_i$ in order to specify the expected
number of measured counts that are from the signal of interest. Going
forward background will be assumed to already have been accounted for,
and $\nu_i$ will refer to the expected signal counts of bin $i$.

As all these variables so far have been derived from the exact
continuous distributions $f_X(x)$ and $f_Y(y)$, they correspond to the
expectation values that researchers are estimating during data
collection and analysis. As this is a counting process the components of
the observed number of signal events $\bm{n}$, an $N$-vector, are often
related to the components of the expected number of observed counts
$\bm\nu$ as a collection of $N$ separate and independent Poisson
processes. That is to say the observed counts $n_i$ in bin $i$ are
treated as i.i.d. random variables with the probability mass function
\begin{align}P(n_i\vert\nu_i)=\frac{\nu_i^{n_i}e^{-\nu_i}}{n_i!}.\label{eq:pois}\end{align}
As such counts $n_i$ form the estimate $\hat\nu_i$ of
the expected counts $\nu_i$ by
\begin{align}
  \nu_i
    &=\text{E}[\hat\nu_i]=\text{E}[n_i]\nonumber\\
    &=\text{Cov}[\hat\nu_i]=\text{Cov}[n_i].\nonumber
\end{align}
Understanding the probability distribution of $\bm{n}$ allows for
unfolding methods that involve the use of maximum likelihood estimation.
Additionally, it will be convenient, and necessary for methods based on least-squares, to estimate the
covariance matrix $\bm{\hat\Sigma}_{\nu}$ (written $\hat\Sigma^{\nu}_{ij}$ when referring to components) of the observations, which for
independent Poisson processes has components of the form
\begin{align}
  \hat\Sigma^{\nu}_{ij}
      &=\text{Cov}[\hat\nu_i,\hat\nu_j]\nonumber\\
      &=\text{Cov}[n_i,n_j]\nonumber\\
      &=\delta_{ij}n_i,\label{eq:cov}\end{align}
where $\delta_{ij}$ is the Kronecker
delta\footnote{The Kronecker delta $\delta_{ij}$ is a piecewise function of variables $i$ and $j$ defined by $\delta_{ij}=\begin{cases}0\;\;\;\text{if }i\neq j\\1\;\;\;\text{if }i=j\end{cases}.$}.
The path to an estimated covariance matrix of the estimated true distribution
$\bm{\hat\mu}$, itself a function of $\bm{n}$ and $\bm{\nu}$ (or $\bm{\hat\nu}$), can be considered briefly by considering the maximum
log-likelihood, where it can be shown \begin{align}
  \log L(\bm\mu)
    &=\sum_{i=1}^N\log\left(\frac{\nu_i^{n_i}e^{-\nu_i}}{n_i!}\right)\nonumber\\
    &=\sum_{i=1}^N\left(n_i\log\nu_i-\nu_i-\log n_i!\right)\label{eq:logL}\\
  \frac{\partial\log L}{\partial\mu_k}
    &=\sum_{i=1}^N\frac{\partial\log L}{\partial\nu_i}\frac{\partial\nu_i}{\partial\mu_k}\nonumber\\
    &=\sum_{i=1}^N\left(\frac{n_i}{\nu_i}-1\right)R_{ik}=0.\label{eq:dlogL}
\end{align} Some minor algebra here thankfully reproduces the estimate
$\bm{\hat\nu}=\bm{n}$, as expected from an earlier observation about $n_i$. Continuing
with an additional derivative shows 
\begin{align}
  \frac{\partial^2\log L}{\partial\mu_k\partial\mu_l}
    &=-\sum_{i=1}^N\left(\frac{n_i}{\nu_i^2}\frac{\partial\nu_i}{\partial\mu_l}\right) R_{ik}\nonumber\\
    &=-\sum_{i=1}^N\frac{n_i R_{il}R_{ik}}{\nu_i^2},\label{eq:d2logL}
\end{align} the negative of the expectation value of which is the Fisher information matrix $\bm{\mathcal{I}}(\bm\mu)$. Since the Fisher information's relationship with the Cram$\acute{\text{e}}$r-Rao lower bound can, as its name implies, be used to determine the lower bound on the covariance matrix of an estimator of $\bm{\mu}$, one can show for one such \emph{unbiased} estimator, say $\bm{T}(\bm{n})$, that
\begin{align}
  \text{Cov}_{\!\bm{\mu}}\!\left[\bm{T}(\bm{n})\right]\geq \bm{\mathcal{I}}(\bm{\mu})^{-1}
    &=\left(-E\left[\frac{\partial^2\log L}{\partial\mu_k\partial\mu_l}\right]\right)^{-1}\nonumber\\
    &=\left(\sum_{i=1}^N\frac{E[n_i] R_{il}R_{ik}}{\nu_i^2}\right)^{-1}\nonumber\\
    &=\left(\sum_{i=1}^N\frac{R_{il}R_{ik}}{\nu_i}\right)^{-1}\nonumber\\
    &=\left(\bm{R}^T\bm{\Sigma}_\nu^{-1}\bm{R}\right)^{-1}\nonumber\\
    &=\bm{R}^{-1}\bm{\Sigma}_\nu(\bm{R}^{-1})^{T}.\label{CRlb}
\end{align}
Indeed, this matrix must then be the lower bound for the covariance matrix of any unbiased estimator of $\bm{\mu}$. This is some good insight to have before getting into the weeds of working on actual data.

\subsection{A Simulated Example}\label{simulation}

The following example is not meant to reflect any actual physics. Consider the following three sets of i.i.d. random variables from separate Cauchy distributions.
\begin{align}
  X_{1,i}&\sim \text{Cauchy}(x_0^{(1)},\gamma_1)\nonumber\\
  X_{2,i}&\sim \text{Cauchy}(x_0^{(2)},\gamma_2)\nonumber\\
  X_{3,i}&\sim \text{Cauchy}(x_0^{(3)},\gamma_3)\nonumber
\end{align} 
Current models predict that some class of physics events observed in past detectors are solely coming from the first two processes (Model 1), such that for $n$ events the number coming from the first process is an i.i.d. random variable from the binomial distribution $B(n,p)$. Meanwhile, a new model (Model 2) has been developed that suggests that the third process has been occurring this whole time but has been incorrectly categorized as one or the other of the first two. Napkin math has estimated a contribution rate that is reflective of some probability $p_3$, such that the binomial distribution is actually a multinomial distribution with probabilities $\bm p=\{p_1,p_2,p_3\}$.

A new experiment is being designed and funded to test this new model, on top of many others, and simulations are being performed to give analyzers plenty of opportunities to develop their collaboration's analysis framework, perform calibrations, and make ready a myriad of studies that hope to shed light on their unanswered questions. The corresponding MC simulations performed during this time include such simulations for the physics events of interest, but also for the detector. For the purposes of this paper I am performing a relatively simple set of simulations, resulting in 200,000 simulated events per model that are meant to represent the MC simulations, and a set of 20,000 simulated events for each model that are meant to represent either hypothetical detector data or MC test sets as need dictates. I will also be writing the code to carry out any of the discussed methods that are applied to this simulated data.

```{r, echo = F}
set.seed(1234)
# Assume 10000 events
nsim_data <- 20000
dmrat <- 10
nsim_mc <- dmrat*nsim_data

# Two possible types of processes with different means but equal variances
loctn <- c(11,18,14)
scale <- c(4,4,5)
p <- 0.25
p1 <- c(0.3,0.7)
p2 <- c((1-p)*p1[1],(1-p)*p1[2],p)

events_data1 <- c(rmultinom(1, nsim_data, p1))
events_data2 <- c(rmultinom(1, nsim_data, p2))

events_mc1 <- c(rmultinom(1, nsim_mc, p1))
events_mc2 <- c(rmultinom(1, nsim_mc, p2))

x1_data1 <- rcauchy(events_data1[1], location = loctn[1], scale = scale[1])
x2_data1 <- rcauchy(events_data1[2], location = loctn[2], scale = scale[2])
x1_data2 <- rcauchy(events_data2[1], location = loctn[1], scale = scale[1])
x2_data2 <- rcauchy(events_data2[2], location = loctn[2], scale = scale[2])
x3_data2 <- rcauchy(events_data2[3], location = loctn[3], scale = scale[3])
x1_mc1 <- rcauchy(events_mc1[1], location = loctn[1], scale = scale[1])
x2_mc1 <- rcauchy(events_mc1[2], location = loctn[2], scale = scale[2])
x1_mc2 <- rcauchy(events_mc2[1], location = loctn[1], scale = scale[1])
x2_mc2 <- rcauchy(events_mc2[2], location = loctn[2], scale = scale[2])
x3_mc2 <- rcauchy(events_mc2[3], location = loctn[3], scale = scale[3])

x_data <- c(x1_data1,x2_data1,
            x1_data2,x2_data2,x3_data2)
x_mc <- c(x1_mc1,x2_mc1,
          x1_mc2,x2_mc2,x3_mc2)

# Efficiency
xdetected_data <- rbernoulli(nsim_data,1-exp(-sqrt(abs(x_data))/4)) == 1
xdetected_mc <- rbernoulli(nsim_mc,1-exp(-sqrt(abs(x_mc))/4)) == 1

# Add Smearing
err_mu_data <- -abs(x_data)^(1/4)
err_sd_data <- log((abs(x_data)+10)/4)
y_data <- x_data + rnorm(2*nsim_data, mean = err_mu_data, sd = err_sd_data)

err_mu_mc <- -abs(x_mc)^(1/4)
err_sd_mc <- log((abs(x_mc)+10)/4)
y_mc <- x_mc + rnorm(2*nsim_mc, mean = err_mu_mc, sd = err_sd_mc)

# Binned representation
sims_data <- tibble("Model" = 
                      rep(c("Model 1","Model 2"),
                          each = nsim_data),
                    "Truth" = x_data,
                    "Reconstructed" = y_data,
                    "Detected" = xdetected_data)
sims_mc <- tibble("Model" = 
                    rep(c("Model 1","Model 2"),
                        each = nsim_mc),
                  "Truth" = x_mc,
                  "Reconstructed" = y_mc,
                  "Detected" = xdetected_mc)

bins_data <- sims_data %>%
  filter(Truth <= 30 & Truth >= 0) %>%
  pivot_longer(c(Reconstructed,Truth), 
               names_to = "Treatment", 
               values_to = "Bin") %>%
  mutate(Bin = ceiling(Bin)) %>%
  filter(Bin <= 30 & Bin >= 1) %>%
  filter((Treatment == "Reconstructed" & Detected == TRUE) | Treatment == "Truth") %>%
  count(Model, Bin, Treatment, name = "Counts") %>%
  complete(Model, Bin=1:30, Treatment, fill = list(Counts=0)) %>%
  mutate(LBin = Bin-1,
         LCounts = cbind(
           rbind(diag(rep(1,62))[c(3,4,3:58,61,62),-c(1,2)],
                 diag(0,60)),
           rbind(diag(0,60),
                 diag(rep(1,62))[c(3,4,3:58,61,62),-c(1,2)])) %*% 
           Counts) %>%
  mutate(Density = Counts/nsim_data,
         LDensity = LCounts/nsim_data) %>%
  select(Model, Treatment,LBin,Bin,LCounts,Counts,LDensity,Density)

bins_mc <- sims_mc %>%
  filter(Truth <= 30 & Truth >= 0) %>%
  pivot_longer(c(Reconstructed,Truth), 
               names_to = "Treatment", 
               values_to = "Bin") %>%
  mutate(Bin = ceiling(Bin)) %>%
  filter(Bin <= 30 & Bin >= 1) %>%
  filter((Treatment == "Reconstructed" & Detected == TRUE) | Treatment == "Truth") %>%
  count(Model, Bin, Treatment, name = "Counts") %>%
  complete(Model, Bin=1:30,  Treatment, fill = list(Counts=0)) %>%
  mutate(LBin = Bin-1,
         LCounts = cbind(
           rbind(diag(rep(1,62))[c(3,4,3:58,61,62),-c(1,2)],diag(0,60)),
           rbind(diag(0,60),diag(rep(1,62))[c(3,4,3:60),-c(1,2)])) %*% 
           Counts) %>%
  mutate(Density = Counts/nsim_data,
         LDensity = LCounts/nsim_data) %>%
  select(Model,Treatment,LBin,Bin,LCounts,Counts,LDensity,Density)

bins_all <- rbind(bins_data,bins_mc) %>%
  mutate(Source = c(rep("Data",120),rep("MC",120)),
         LCounts = LCounts/c(rep(1,120),rep(dmrat,120)),
         Counts = Counts/c(rep(1,120),rep(dmrat,120))) %>%
  mutate(Name = paste(Treatment,Source)) %>%
  filter(Source == "MC" | Treatment == "Reconstructed")

bins_all$Name[which(bins_all$Source == "Data")] <- "Reconstructed Data"

#max(sims_mc$Truth,sims_mc$Reconstructed,sims_data$Truth,sims_data$Reconstructed)
```

```{r, echo = F}
# Continuous representation
X <- seq(0,30,0.01)
nx <- length(X)

fp1x <- dcauchy(X, location = loctn[1], scale = scale[1]) 
fp2x <- dcauchy(X, location = loctn[2], scale = scale[2])
fp3x <- dcauchy(X, location = loctn[3], scale = scale[3])
#fx <- p*dlnorm(X, meanlog = 2.25, sdlog = 0.25) +
#  (1-p)*dlnorm(X, meanlog = 2.8, sdlog = 0.15)

fpx_truth1 <- tibble(X = rep(X,2), Density = c(p1[1]*fp1x,p1[2]*fp2x), 
                     Process = rep(c("Process 1","Process 2"), each = nx),
                     Model = rep("Model 1",2*nx))
fpx_truth2 <- tibble(X = rep(X,3), Density = c(p2[1]*fp1x,p2[2]*fp2x,p2[3]*fp3x), 
                     Process = rep(c("Process 1","Process 2","Process 3"), each = nx),
                     Model = rep("Model 2",3*nx))
fx_truth1 <- tibble(X = X, Density = p1[1]*fp1x + p1[2]*fp2x, 
                    Model = rep("Model 1",nx),Treatment = rep("Truth",nx))
fx_truth2 <- tibble(X = X, Density = p2[1]*fp1x + p2[2]*fp2x + p2[3]*fp3x,
                    Model = rep("Model 2",nx),Treatment = rep("Truth",nx))
fy_truth1 <- read.csv("f1yEstimate.csv") %>% 
  mutate(Model = "Model 1", Treatment = "Reconstructed")
fy_truth2 <- read.csv("f2yEstimate.csv") %>% 
  mutate(Model = "Model 2", Treatment = "Reconstructed")
names(fy_truth1)[1] <- names(fx_truth1)[1] <- 
  names(fy_truth2)[1] <- names(fx_truth2)[1] <- "XY"

# Binned expected counts
bins_expected1 <- read.csv("hist1Expected.csv") %>%
  mutate(LDensity = LCounts,
         Density = Counts,
         LCounts = LCounts*nsim_data,
         Counts = Counts*nsim_data)
bins_expected2 <- read.csv("hist2Expected.csv") %>%
  mutate(LDensity = LCounts,
         Density = Counts,
         LCounts = LCounts*nsim_data,
         Counts = Counts*nsim_data)

# Counts max
fmax_density <- 1.05*max(c(bins_data$Density,bins_mc$Density,
                           fx_truth1$Density,fx_truth2$Density,
                           fy_truth1$Density,fy_truth2$Density,
                           bins_expected1$Density,bins_expected2$Density))
fmax_count <- 1.05*max(c(bins_data$Counts,bins_mc$Counts/dmrat,
                         fx_truth1$Density*nsim_data,fx_truth2$Density*nsim_data,
                         fy_truth1$Density*nsim_data,fy_truth2$Density*nsim_data,
                         bins_expected1$Counts,bins_expected2$Counts))
```

```{r, fig.height=7, fig.width=7.5, fig.align='center', echo = F}
continuous2 <- ggplot() + 
  theme_bw() +
  geom_line(data = rbind(fx_truth1,fx_truth2,
                         fy_truth1,fy_truth2) %>%
              filter(Model == "Model 1"),
            mapping = aes(x = XY, 
                          y = nsim_data*Density,
                      color = Treatment),
            alpha = 0.7) +
  geom_line(data = rbind(fx_truth1,fx_truth2,
                         fy_truth1,fy_truth2) %>%
              filter(Model == "Model 2"),
            mapping = aes(x = XY, 
                          y = nsim_data*Density,
                   linetype = Treatment),
            color = rep(c("#8fb032","#eb6235"), each=nx),
            alpha = 0.7) +
  scale_color_manual(name = "Model 2 MC",
                     labels = c("Truth",
                                "Reconstructed"),
                     breaks = c("Truth",
                                "Reconstructed"),
                     values = c("Truth" = "#5e81b5",
                                "Reconstructed" = "#e19c24")) + 
  scale_linetype_manual(name = NA,
                        labels = c("Truth",
                                   "Reconstructed"),
                        breaks = c("Truth",
                                   "Reconstructed"),
                        values = c("Truth" = "solid",
                                   "Reconstructed" = "solid")) +
  guides(color = guide_legend(title = "Model 1 MC", order = 1, 
                              override.aes = list(shape = NA,
                                                  alpha = 0.7),
                              title.vjust = unit(-0.2, "pt")),
         linetype = guide_legend(title = "Model 2", order = 2, 
                                 override.aes = list(color = c("#8fb032","#eb6235"),
                                                     alpha = c(0.8,0.8)),
                                 title.vjust = unit(-0.2, "pt"))) +
  scale_x_continuous("X (Truth), Y (Reconstructed)",
                     breaks = seq(0,30,3), 
                     limits = c(0,30), 
                     expand = c(0,0)) +
  scale_y_continuous("Counts",
                     limits = c(0,fmax_count), expand = c(0,0),
                     breaks = seq(0,ceiling(fmax_count),100)) +
  labs(title = "Scaled Exact Distributions") +
  theme(legend.text = element_text(size = 8),
        legend.title = element_text(size = 9),
        legend.justification = c("left", "top"),
        legend.box.just = "left",
        legend.spacing.y = unit(1, "pt"),
        legend.key.height = unit(10,"points"),
        legend.margin = margin(1,5,5,5,"pt"),
        aspect.ratio = 1,
        axis.title.y = element_blank())

```

```{r, fig.height=4, fig.width=7.5, fig.align='center', echo = F}
################### MC and Data #################################

discrete_DataMC <- ggplot() + 
  theme_bw() +
  geom_point(data = bins_all %>% 
               filter(LBin >= 0 & Bin <= 30 &
                        Source == "Data" & Model == "Model 2"),
             mapping = aes(x = LBin+0.5,
                           y = Counts,
                           fill = "Reconstructed"),
             shape = 20, color = "black") +
  geom_segment(data = filter(bins_all,
                               LBin >= 0 &
                               Bin <= 30 &
                               Source == "MC" &
                               Model == "Model 1"),
               mapping = aes(x = LBin,
                             y = Counts,
                          xend = Bin,
                          yend = Counts,
                         color = Name),
               alpha = 0.7) +
  geom_segment(data = filter(bins_all,
                               LBin >= 0 &
                               Bin <= 30 &
                               Source == "MC" &
                               Model == "Model 1"),
               mapping = aes(x = LBin,
                             y = LCounts,
                          xend = LBin,
                          yend = Counts,
                         color = Name),
               alpha = 0.7) +
  geom_segment(data = bins_all %>% 
                 filter(LBin >= 0 &
                          Bin <= 30 &
                          Source == "MC" &
                          Model == "Model 2") %>%
                 arrange(desc(Treatment)),
               mapping = aes(x = LBin,
                             y = Counts,
                          xend = Bin,
                          yend = Counts,
                      linetype = Name),
                         color = rep(c("#eb6235","#8fb032"),each=30),
               alpha = 0.7) +
  geom_segment(data = bins_all %>% 
                 filter(LBin >= 0 &
                          Bin <= 30 &
                          Source == "MC" &
                          Model == "Model 2") %>% 
                 arrange(desc(Treatment)),
               mapping = aes(x = LBin,
                             y = LCounts,
                          xend = LBin,
                          yend = Counts,
                      linetype = Name),
                         color = rep(c("#eb6235","#8fb032"),each=30),
               alpha = 0.7) +
  scale_x_continuous("X (Truth), Y (Reconstructed)",
                     breaks = seq(0,30,3), 
                     limits = c(0,30), 
                     expand = c(0,0)) +
  scale_y_continuous("Counts",
                     limits = c(0,fmax_count), expand = c(0,0),
                     breaks = seq(0,ceiling(fmax_count),100)) +
  scale_color_manual(name = NA,
                     labels = c("Truth",
                                "Reconstructed"),
                     breaks = c("Truth MC",
                                "Reconstructed MC"),
                     values = c("Truth MC" = "#5e81b5",
                                "Reconstructed MC" = "#e19c24")) + #"#eb6235","#8fb032"
  scale_linetype_manual(name = NA,
                        labels = c("Truth",
                                   "Reconstructed"),
                        breaks = c("Truth MC",
                                   "Reconstructed MC"),
                        values = c("Truth MC" = "solid",
                                   "Reconstructed MC" = "solid")) +
  guides(color = guide_legend(title = "Model 1 MC", order = 1, 
                              override.aes = list(shape = NA,
                                                  alpha = 0.7),
                              title.vjust = unit(-0.5, "pt")),
         linetype = guide_legend(title = "Model 2 MC", order = 2, 
                                 override.aes = 
                                   list(color = c("#eb6235","#8fb032"),
                                        alpha = 0.7),
                                 title.vjust = unit(-0.5, "pt")),
         fill = guide_legend(title = "Data", order = 3, 
                             title.vjust = unit(-0.5, "pt"))) +
  labs(title = "MC Simulations and ''Data''") +
  theme(legend.title = element_text(size = 9),
        legend.text = element_text(size = 8),
        legend.box.just = "left",
        legend.spacing.y = unit(-2.5, "pt"),
        legend.key.height = unit(10,"points"),
        aspect.ratio = 1,
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.title.y = element_blank())
```

\begin{figure}[!ht]
    \centering
```{r, fig.height=3, fig.width=8, fig.align='center', echo = F}
plot1 <- ggplotGrob(continuous2 + theme(legend.position = "none",
                                        axis.title.x = element_blank()))
plot2 <- ggplotGrob(discrete_DataMC + theme(legend.position = "none",
                                        axis.title.x = element_blank()))


egg::ggarrange(continuous2 + 
                 theme(legend.position = "none", 
                       axis.title.x = element_blank()), 
               discrete_DataMC + 
                 theme(axis.title.x = element_blank()), nrow = 1,
               left = textGrob("Counts", rot = 90, vjust = 0.25),
               bottom = textGrob("X (Truth), Y (Reconstructed)", hjust = 0.8))
```
  \caption{\emph{\small The distinguishing characteristics between Model 1 and Model 2 become greatly diminished as their respective distributions are smeared and diffused by mechanisms common to both. Ignoring the knowledge granted in this figure's legend, one would be hardpressed to predict which Truth distribution a particular Reconstructed distribution came from.}}
  \label{plots1}
\end{figure}

Please see Appendix \ref{simuApp} for more specific information about the performed simulations. Referring to Figure \ref{plots1}, the blue and red colored plots correspond to the true distributions of Model 1 and Model 2 respectively. The colored lines represent the MC simulations. They have been rescaled to represent the same number of possible events as the experimental data. The impact of the third contributing Cauchy process in Model 2 can be seen by way of the notably diminished peak and the partial filling of the dividing indent between the two processes of Model 1. While it is clear for both the continuous and discrete representations that the two larger distributions are distinct, less can be said about them once finite detector resolutions and other inefficiencies have had their effects, as one can see in the similarities between their yellow and green reconstructed distributions.

\begin{figure}[!ht]
    \centering
```{r, fig.align='center', echo = F, fig.height=3, fig.align='center'}
########## MIGRATION ###########################################################

migration <- sims_mc %>%
  filter(Detected == TRUE &
           Truth <= 30 & Truth >= 0) %>%
  ggplot() +
    scale_y_continuous(breaks = seq(0,30,by=3),
                       limits = c(0,30), 
                       expand = c(0,0)) +
    scale_x_continuous(breaks = seq(-3,36,by=3), 
                       limits = c(-5,37), 
                       expand = c(0,0)) +
    theme_bw() +
    stat_bin2d(geom="raster",
               mapping = aes(x = Reconstructed, 
                             y = Truth), 
               breaks = list(x = seq(-4,36,1),
                             y = seq(0,30,1))) +
    labs(x = "Y (Reconstructed)", y = "X (Truth)",
         title = "MC Event Migration") +
    geom_abline(intercept=0,slope=1,lty="dashed",col="red") +
    facet_grid(~Model) + coord_equal(ratio = 1)

gradient_max <- max(ggplot_build(migration)$data[[1]]$count)
gmins <- abs(floor(gradient_max/10^(floor(log10(gradient_max))-1))/
               c(5,10,20,25,50)-1)
gradient_step <- c(5,10,20,25,50)[which(gmins == min(gmins))]*
  10^(floor(log10(gradient_max))-2)

migration <- migration +
  scale_fill_gradientn(colours = viridis(11),
                       breaks = seq(0,gradient_max,gradient_step),
                       guide = guide_colourbar(barwidth = 1, 
                                               barheight = 8.5,
                                               frame.colour = "black",
                                               ticks.colour = "black",
                                               title = "Counts"))

migration
```
  \caption{\emph{\small The added dimensional perspective offered by this migration heat map reveals additional structure and dependencies concerning the smearing process.}}
  \label{plots2}
\end{figure}

Figure \ref{plots2} provides an alternative perspective to Figure \ref{plots1} by providing a visual representation of the two models' migration matrices. The red dashed lines indicate where events would live if there were no skewing or smearing processes. Note that the Reconstructed axis extends past points covered by the Truth axis, providing insight into the behavior with which events produced in the region of interest ($X\in(0,30)$ here) end up measured or reconstructed outside of it, but still in a detector's \textbf{fiducial volume}, which represents the reliable, central region of the detector or other relevant region of the events' \textbf{phase space}, which is the space of all possible event states.


```{r, echo=F}
### Data 1
dataReco1 <- sims_data %>%
  filter(Model == "Model 1" &
           Truth <= 30 & Truth >= 0 &
           Detected == TRUE) %>%
  mutate(Bin = ceiling(Reconstructed)) %>%
  count(Bin, name = "Counts") %>%
  select(Bin, Counts)
sidedata1 <- dataReco1 %>%
  filter(Bin < 1 | Bin > 30) %>% pull(Bin)
dataReco1 <- dataReco1 %>%
  filter(Bin >= 1 & Bin <= 30) %>%
  rbind(dataReco1 %>% filter(Bin < 1 | Bin > 30) %>%
          mutate(Counts = sum(Counts)) %>% 
          filter(Bin == 31))

dataTruth1 <- sims_data %>%
  filter(Model == "Model 1" &
           Truth <= 30 & Truth >= 0) %>%
  mutate(Bin = ceiling(Truth)) %>%
  count(Bin, name = "Counts") %>%
  select(Bin, Counts)

### Data 2
dataReco2 <- sims_data %>%
  filter(Model == "Model 2" &
           Truth <= 30 & Truth >= 0 &
           Detected == TRUE) %>%
  mutate(Bin = ceiling(Reconstructed)) %>%
  count(Bin, name = "Counts") %>%
  select(Bin, Counts)
sidedata2 <- dataReco2 %>%
  filter(Bin < 1 | Bin > 30) %>% pull(Bin)
dataReco2 <- dataReco2 %>%
  filter(Bin >= 1 & Bin <= 30) %>%
  rbind(dataReco2 %>% filter(Bin < 1 | Bin > 30) %>%
          mutate(Counts = sum(Counts)) %>% 
          filter(Bin == 31))

dataTruth2 <- sims_data %>%
  filter(Model == "Model 2" &
           Truth <= 30 & Truth >= 0) %>%
  mutate(Bin = ceiling(Truth)) %>%
  count(Bin, name = "Counts") %>%
  select(Bin, Counts)

### MC 1
mcReco1 <- sims_mc %>%
  filter(Model == "Model 1" &
           Truth <= 30 & Truth >= 0 &
           Detected == TRUE) %>%
  mutate(Bin = ceiling(Reconstructed)) %>%
  count(Bin, name = "Counts") %>%
  select(Bin, Counts)

sidemc1 <- mcReco1 %>%
  filter(Bin < 1 | Bin > 30) %>% pull(Bin)
mcmat1 <- matrix(rep(0,31*max(sidemc1-min(sidemc1)+1)),ncol=31)
mcmat1[(1:30-min(sidemc1)+1),1:30] <- diag(1,30)
mcmat1[(sidemc1-min(sidemc1)+1),31] <- 1

mcReco1 <- mcReco1 %>%
  filter(Bin >= 1 & Bin <= 30) %>%
  rbind(mcReco1 %>% filter(Bin < 1 | Bin > 30) %>%
          mutate(Counts = sum(Counts)) %>% 
          filter(Bin == 31))

mcTruth1 <- sims_mc %>%
  filter(Model == "Model 1" &
           Truth <= 30 & Truth >= 0) %>%
  mutate(Bin = ceiling(Truth)) %>%
  count(Bin, name = "Counts") %>%
  select(Bin, Counts)

### MC 2
mcReco2 <- sims_mc %>%
  filter(Model == "Model 2" &
           Truth <= 30 & Truth >= 0 &
           Detected == TRUE) %>%
  mutate(Bin = ceiling(Reconstructed)) %>%
  count(Bin, name = "Counts") %>%
  select(Bin, Counts)

sidemc2 <- mcReco2 %>%
  filter(Bin < 1 | Bin > 30) %>% pull(Bin)
mcmat2 <- matrix(rep(0,31*max(sidemc2-min(sidemc2)+1)),ncol=31)
mcmat2[(1:30-min(sidemc2)+1),1:30] <- diag(1,30)
mcmat2[(sidemc2-min(sidemc2)+1),31] <- 1

mcReco2 <- mcReco2 %>%
  filter(Bin >= 1 & Bin <= 30) %>%
  rbind(mcReco2 %>% filter(Bin < 1 | Bin > 30) %>%
          mutate(Counts = sum(Counts)) %>% 
          filter(Bin == 31))

mcTruth2 <- sims_mc %>%
  filter(Model == "Model 2" &
           Truth <= 30 & Truth >= 0) %>%
  mutate(Bin = ceiling(Truth)) %>%
  count(Bin, name = "Counts") %>%
  select(Bin, Counts)


```

\section{Unfolding in particle physics}

The international particle physics community is highly collaborative. Most of this is likely due to necessity, as advances in the field are requiring larger and more expensive experiments that rely on pooled resources of talent, labor, and capital. This extends to digital resources as well. ROOT \cite{Brun1997} is an open-source data analysis framework developed and used primarily by people doing particle physics. Analyses incorporating its resources are primarily written in some combination of Python and C++. While it does contain a strong base level of classes, objects, and methods within its default toolkit, including some for unfolding, extended frameworks are typically built on top of it to meet the needs of specific experiments or to facilitate expanded capabilities that would effectively serve as bloatware for most users. The RooUnfold framework \cite{Adye2011} is one such example. Two methods facilitated in whole or in part by the RooUnfold framework will be covered in this section. One is considered naive, as will be shown. The other finds ways past the issues brought up in the first.

\subsection{Inverting the Response Matrix}

In the event of Equation \eqref{eq:mat} being well-posed the obvious
approach would be to construct the unique inverse of the response matrix
$\bm{R}^{-1}$, as estimated from MC simulations, and map the reconstructed counts back to an estimate of
the true counts via \begin{align}
  \bm{\hat\mu}=\bm{R}^{-1}\bm{n}.\label{eq:naiveInv}
\end{align} A statistical justification for this comes from performing
generalized least-squares \cite{Johnson2007} fit to estimate $\bm\mu$,
which relies on approximating bin count $n_i$ as normally distributed
with mean $\nu_i$ and variance $1/\nu_i$. Minimizing the sums of squares
yields \begin{align}\min_{\bm\mu}\nabla_{\!\bm\mu}\bm\chi^2(\bm{\mu})
  &=\nabla_{\bm\mu}(\bm{R}\bm{\mu}-\bm{n})^T\bm{\Sigma}_{\nu}^{-1}(\bm{R}\bm{\mu}-\bm{n})\label{eq:leastsq}\\
  &=\nabla_{\bm\mu}(\bm{\mu}^T\bm{R}^T-\bm{n}^T)\bm{\Sigma}_{\nu}^{-1}(\bm{R}\bm{\mu}-\bm{n})\nonumber\\
  &=\nabla_{\bm\mu}(\bm{\mu}^T\bm{R}^T\bm{\Sigma}_{n}^{-1}\bm{R}\bm{\mu}-\bm{\mu}^T\bm{R}^T\bm{\Sigma}_{\nu}^{-1}\bm{n}-\bm{n}^T\bm{\Sigma}_{\nu}^{-1}\bm{R}\bm{\mu}+\bm{n}^T\bm{\Sigma}_{\nu}^{-1}\bm{n})\nonumber\\
  &=\nabla_{\bm\mu}(\bm{\mu}^T\bm{R}^T\bm{\Sigma}_{n}^{-1}\bm{R}\bm{\mu}-2\bm{\mu}^T\bm{R}^T\bm{\Sigma}_{\nu}^{-1}\bm{n}+\bm{n}^T\bm{\Sigma}_{\nu}^{-1}\bm{n})\nonumber\\
  &=2\bm{R}^T\bm{\Sigma}_{\nu}^{-1}\bm{R}\bm{\mu}-2\bm{R}^T\bm{\Sigma}_{\nu}^{-1}\bm{n}\nonumber\\
  &=0\nonumber\\
  \implies\bm{R}^T\bm{\Sigma}_{\nu}^{-1}\bm{R}\bm{\mu}&=\bm{R}^T\bm{\Sigma}_{\nu}^{-1}\bm{n}\nonumber\\
  \implies\bm{\hat\mu}&=(\bm{R}^T\bm{\Sigma}_{\nu}^{-1}\bm{R})^{-1}\bm{R}^T\bm{\Sigma}_{\nu}^{-1}\bm{n}\\
  &=\bm{R}^{+}\bm{n},\label{eq:naivemu}
\end{align} where $\bm{R}^{+}=(\bm{R}^T\bm{\Sigma}_{\nu}^{-1}\bm{R})^{-1}\bm{R}^T\bm{\Sigma}_{\nu}^{-1}$ is the Moore-Penrose generalized inverse
(or pseudo-inverse) \cite{Blobel2013} of $\bm{R}$. Note that multiplying $\bm{R}$ to the left side of $\bm{R}^{+}$ as calculated here fits the form of the hat matrix $\bm{H}$ of ordinary regression, such that $\bm{\hat\nu}=\bm{H}\bm{\nu}$. This has all so far just been basic regression using generalized least-squares. As such the inverse of $\bm{R}^T\bm{\Sigma}_{\nu}^{-1}\bm{R}$, using the Moore-Penrose generalized inverse, corresponds to the estimated true counts' covariance matrix:
\begin{align}
  (\bm{R}^T\bm{\Sigma}_{\nu}^{-1}\bm{R})^{-1}=\bm{R}^{+}\bm{\Sigma}_\nu\bm{R}^{+^T}=\bm{R}^{+}\text{Cov}[\bm{n}]\bm{R}^{+^T}=\text{Cov}[\bm{R}^{+}\bm{n}]=\text{Cov}[\bm{\hat\mu}]=\bm{\hat\Sigma}_\mu\nonumber.
\end{align} 
This is the same lower bound covariance matrix calculated in Equation \eqref{CRlb}, which indicates that $\bm{R}^{+}$ is indeed an unbiased estimator with the lowest possible variances among unbiased estimators. For the simulations one such inverse matrix was calculated from the response matrix of each MC Model simulation, and were used to form estimates of their respective $\bm{\Sigma}_\mu$s for each MC-Data combination using each Data Model's estimated covariance matrix.

The resulting true counts estimates, with their $\pm1$ estimated standard deviations, are plotted below in  \ref{NaiveRes}. The rescaled true distribution for each MC simulation was provided for comparison. They are in separate plots as their structure is not visible at the scales necessary to show the unfolded Data. The color of the lines indicate the true model behind the simulation and the shaded regions, which represent the calculated uncertainty, are colored to indicate the MC model of the response matrix.

```{r, echo=F, fig.height=3}
R_mc1 <- sims_mc %>%
  filter(Model == "Model 1" &
           Detected == TRUE &
           Truth <= 30 & Truth >= 0) %>%
  mutate(Truth = ceiling(Truth),
         Reconstructed = ceiling(Reconstructed)) %>%
  count(Truth, Reconstructed, name = "Counts") %>%
  complete(Reconstructed=max(Reconstructed):min(Reconstructed),
           Truth=1:30, fill = list(Counts=0)) %>%
  pull(Counts) %>% matrix(byrow = T, ncol = 30) %>% 
  t() %*% mcmat1 %>% t() * matrix(rep(1/mcTruth1$Counts,each=31),ncol=30)

R_mc2 <- sims_mc %>%
  filter(Model == "Model 2" &
           Detected == TRUE &
           Truth <= 30 & Truth >= 0) %>%
  mutate(Truth = ceiling(Truth),
         Reconstructed = ceiling(Reconstructed)) %>%
  count(Truth, Reconstructed, name = "Counts") %>%
  complete(Reconstructed=max(Reconstructed):min(Reconstructed),
           Truth=1:30, fill = list(Counts=0)) %>%
  pull(Counts) %>% matrix(byrow = T, ncol = 30) %>% 
  t() %*% mcmat2 %>% t() * matrix(rep(1/mcTruth2$Counts,each=31),ncol=30)


#Vnu1 <- diag(mcReco1$Counts/dmrat)
#Vnu2 <- diag(mcReco2$Counts/dmrat)
Vnu1 <- diag(dataReco1$Counts)
Vnu2 <- diag(dataReco2$Counts)

inv_R_mc1data1 <- solve(t(R_mc1) %*% solve(Vnu1) %*% R_mc1) %*% 
  t(R_mc1) %*% solve(Vnu1)
inv_R_mc1data2 <- solve(t(R_mc1) %*% solve(Vnu2) %*% R_mc1) %*% 
  t(R_mc1) %*% solve(Vnu2)
inv_R_mc2data1 <- solve(t(R_mc2) %*% solve(Vnu1) %*% R_mc2) %*% 
  t(R_mc2) %*% solve(Vnu1)
inv_R_mc2data2 <- solve(t(R_mc2) %*% solve(Vnu2) %*% R_mc2) %*% 
  t(R_mc2) %*% solve(Vnu2)

Vmu1data1 <- inv_R_mc1data1 %*% Vnu1 %*% t(inv_R_mc1data1)
Vmu1data2 <- inv_R_mc1data2 %*% Vnu2 %*% t(inv_R_mc1data2)
Vmu2data1 <- inv_R_mc2data1 %*% Vnu1 %*% t(inv_R_mc2data1)
Vmu2data2 <- inv_R_mc2data2 %*% Vnu2 %*% t(inv_R_mc2data2)

data1_unfolded_mc1 <- inv_R_mc1data1 %*% dataReco1$Counts
data1_unfolded_mc2 <- inv_R_mc2data1 %*% dataReco1$Counts
data2_unfolded_mc1 <- inv_R_mc1data2 %*% dataReco2$Counts
data2_unfolded_mc2 <- inv_R_mc2data2 %*% dataReco2$Counts

#min(diag(R_mc1 %*%inv_R_mc1data1))/max(diag(R_mc1 %*%inv_R_mc1data1))
#dRm1d1 <- as.data.frame(cbind(dataReco1$Counts,R_mc1))
#summary(lm(V1 ~ . - 1, data = dRm1d1))
#cbind(sqrt(diag(Vmu1data1)),summary(lm(V1 ~ . - 1, data = dRm1d1))$coef[,2])
#cbind(as.vector(lm(V1 ~ .-1, data = dRm1d1)$coef),data1_unfolded_mc1)
```

\begin{figure}[!ht]
    \centering
```{r, echo=F,fig.height=3.5, fig.align='center'}
bins_hat <- tibble(Model = c(rep(c(rep(c("Model 1",
                                         "Model 2"),each=30),
                                   rep(c("Model 1",
                                         "Model 2"),each=30)),2),
                             rep(c("Model 1",
                                   "Model 2"),each=60)),
                   `Assumed Model` = c(rep(c(rep(c("Model 1",
                                                   "Model 2"),each=30),
                                             rep(c("Model 1",
                                                   "Model 2"),each=30)),2),
                                       rep(rep(c("Model 1",
                                                 "Model 2"),each=30),2)),
                   LBin = rep(0:29,12),
                   Bin = rep(1:30,12),
                   LCounts = c(dataReco1$Counts[c(1,1:29)],
                               dataReco2$Counts[c(1,1:29)],
                               mcReco1$Counts[c(1,1:29)]/dmrat,
                               mcReco2$Counts[c(1,1:29)]/dmrat,
                               dataTruth1$Counts[c(1,1:29)],
                               dataTruth2$Counts[c(1,1:29)],
                               mcTruth1$Counts[c(1,1:29)]/dmrat,
                               mcTruth2$Counts[c(1,1:29)]/dmrat,
                               data1_unfolded_mc1[c(1,1:29)],
                               data1_unfolded_mc2[c(1,1:29)],
                               data2_unfolded_mc1[c(1,1:29)],
                               data2_unfolded_mc2[c(1,1:29)]),
                   Counts = c(dataReco1$Counts[1:30],
                              dataReco2$Counts[1:30],
                              mcReco1$Counts[1:30]/dmrat,
                              mcReco2$Counts[1:30]/dmrat,
                              dataTruth1$Counts,
                              dataTruth2$Counts,
                              mcTruth1$Counts/dmrat,
                              mcTruth2$Counts/dmrat,
                              data1_unfolded_mc1,
                              data1_unfolded_mc2,
                              data2_unfolded_mc1,
                              data2_unfolded_mc2),
                   Treatment = c(rep(c("Reconstructed","Truth"),each=4*30),
                                 rep("TruthHat",120)),
                   Source = c(rep("Data",2*30),rep("MC",2*30),
                              rep("Data",2*30),rep("MC",2*30),
                              rep("Data",4*30)),
                   Name = c(rep("Reconstucted Model 1 Data",30),
                            rep("Reconstucted Model 2 Data",30),
                            rep("Reconstucted Model 1 MC",30),
                            rep("Reconstucted Model 2 MC",30),
                            rep("Truth Model 1 Data",30),
                            rep("Truth Model 2 Data",30),
                            rep("MC 1",30),
                            rep("MC 2",30),
                            rep("Data 1 unfolded with MC 1",30),
                            rep("Data 2 unfolded with MC 1",30),
                            rep("Data 1 unfolded with MC 2",30),
                            rep("Data 2 unfolded with MC 2",30)),
                   Error = c(sqrt(diag(Vnu1))[1:30],
                             sqrt(diag(Vnu2))[1:30],
                             sqrt(mcReco1$Counts[1:30]/dmrat),
                             sqrt(mcReco2$Counts[1:30]/dmrat),
                             rep(0,30*4),
                             sqrt(diag(Vmu1data1)),
                             sqrt(diag(Vmu1data2)),
                             sqrt(diag(Vmu2data1)),
                             sqrt(diag(Vmu2data2))))


asinh_trans <- function(){
  trans_new(name = 'asinh', transform = function(x) asinh(x), 
            inverse = function(x) sinh(x))
}
fancy_scientific <- function(l) {
     # turn in to character string in scientific notation
     l <- format(l, scientific = TRUE)
     # replace 0e+00 with 0
     l <- gsub("0e\\+00","0",l)
     # remove + after exponent, if exists. E.g.: (3x10^+2 -> 3x10^2)
     l <- gsub("e\\+","e",l)
     # turn the 'e+' into plotmath format
     l <- gsub("e", "\\10^", l)
     parse(text=l)
}

ggplot() + 
  theme_bw() +
  geom_hline(yintercept = 0, linetype = "dashed", alpha = 0.5) +
  geom_rect(data = bins_hat %>%
              filter(Treatment == "TruthHat"),
            mapping = aes(xmin = LBin,
                          ymin = (Counts-Error),
                          xmax = Bin,
                          ymax = (Counts+Error),
                          fill = `Assumed Model`),
            alpha = 0.3) +
  geom_segment(data = bins_hat %>%
                 filter(Treatment == "TruthHat" |
                       (Treatment == "Truth" & Source == "MC")),
               mapping = aes(x = LBin,
                             y = Counts,
                          xend = Bin,
                          yend = Counts,
                         color = Model),
               alpha = 0.9) +
  geom_segment(data = bins_hat %>%
                 filter(Treatment == "TruthHat" |
                       (Treatment == "Truth" & Source == "MC")),
               mapping = aes(x = LBin,
                             y = LCounts,
                          xend = LBin,
                          yend = Counts,
                         color = `Model`),
               alpha = 0.9) +
  scale_x_continuous(breaks = seq(0,30,by=3), 
                     limits = c(0,30), 
                     expand = c(0,0)) +
  scale_color_manual(labels = c("Model 1","Model 2"),
                     breaks = c("Model 1","Model 2"),
                     values = c("#5e81b5","#e19c24")) +
  scale_fill_manual(labels = c("Model 1","Model 2"),
                    breaks = c("Model 1","Model 2"),
                    values = c(alpha("#5e81b5",0.3),
                               alpha("#e19c24",0.3))) +
  facet_wrap(~Name, ncol = 3, scales = "free", dir="v") +
  theme(axis.text = element_text(size=8),
        legend.position = "none") +
  labs(title = "Unfolded Data and MC Truth: Response Matrix Inverse", 
       x = "X (Truth)", 
       y = "Counts")
```
\caption{\emph{\small Plots comparing the unfolded reconstructed data under both model assumptions to the true MC distributions. Note the large uncertainties and the rapidly oscillating positive and negative estimated counts.}}
  \label{NaiveRes}
\end{figure}

The massive, rapid oscillations and large uncertainties indicate that these are not very good results, regardless of whether or not the Data and the MC came from the same Model. The unfolding process used here cannot account for even small random variations in Reconstructed space because it has been over-fitted by the MC results. In search of an unbiased estimator we have unwittingly created a minimum variance that is simply too large to be in any way useful. Some degree of bias in estimating $\bm{\hat\mu}$ will be needed to probe the neighborhood around the unbiased estimate resulting from minimizing the least-squares of Equation \eqref{eq:leastsq} (or equivalent maximum log-likelihood) to find an alternative solution that can actually be used elsewhere. This is done by including an extra term in these equations through a process called \textbf{Regularization}.

\subsection{Regularization}\label{sub:reg}

This section reviews a method of unfolding that utilizes a form of \textbf{Tikhonov regularization}. Tikhonov regularization as discussed here can be described as a form of \textbf{regularized ridge regression} with weighted least-squares, and using a ridge penalty known by some as the \textbf{ridge fused penalty} \cite{vanwieringen2021}. This method is referred to in particle physics as \textbf{Singular Value Decomposition} (SVD) \cite{Adye2011}\cite{Blobel2013}\cite{Cowan1998}, which unfortunately hides much of the underlying concepts. In general, SVD is a matrix factorization method in linear algebra which, to be fair, is featured prominently in this unfolding method. Please refer to Appendix \ref{app:svd} for a brief description of SVD.

As directed in H$\ddot{\text{o}}$cker and Kartvelishvili's seminal paper on this method \cite{Hocker1995} one should shift the location of event count information on the right-hand side of equation \eqref{eq:mat} from the $\bm{\mu}$ to $\bm{R}$, such that $\bm{R}$'s elements are the actual number of corresponding events, much like Figure \ref{plots2}. This new matrix will be referred to by $\bm{X}$. Next, redefine $\bm{\mu}$ by its ratio to the MC true counts $\bm{\mu}^{\text{MC}}$ to produce instead $\bm{\beta}$ such that
\begin{align}
  \beta_i=\mu_i/\mu_i^{\text{MC}}.\nonumber
\end{align}
This naturally requires an inverse rescaling of any estimated $\bm{\hat\beta}$ to get $\bm{\hat\mu}$. The substitution $\bm{Y}=\bm{n}$ will also be used to facilitate broader notation conventions used in discussions of this subject. The weighted sum of squares with these changes comes out to be
\begin{align}
  \chi^2(\bm{\beta})=\left(\bm{Y}-\bm{X}\bm{\beta}\right)^T\bm{\hat\Sigma}_\nu^{-1}\left(\bm{Y}-\bm{X}\bm{\beta}\right)\label{regSq}
\end{align}
After rescaling, singular value decomposition is applied to the reconstructed covariance matrix $\bm{\hat\Sigma}_\nu$ which, being symmetric and containing only positive elements, will result in something of the form
\begin{align}
  \bm{\hat\Sigma}_\nu=\bm{QTQ}^T,\;\;\;\;\;\text{ where }\;\;\;\;\;\bm{\hat\Sigma}_\nu^{-1}=\bm{Q}\bm{T}^{-1}\bm{Q}^T\text.
\end{align}
The elements of the diagonal matrix $\bm{T}$ are to be written $T_{ij}=t_i^2\delta_{ij}$. Substituting the inverse of this SVD of $\bm{\hat\Sigma}_\nu$ into Equation \eqref{regSq} the weighted sum of squares becomes an unweighted sum of squares, such that
\begin{align}
  \chi^2(\bm{\beta})
    &=\left(\bm{Y}-\bm{X}\bm{\beta}\right)^T\bm{\hat\Sigma}_\nu^{-1}\left(\bm{Y}-\bm{X}\bm{\beta}\right)\nonumber\\
    &=\left(\bm{Y}-\bm{X}\bm{\beta}\right)^T\bm{Q}\bm{T}^{-1}\bm{Q}^T\left(\bm{Y}-\bm{X}\bm{\beta}\right)\nonumber\\
    &=\left(\bm{Y}-\bm{X}\bm{\beta}\right)^T\bm{Q}\bm{T}^{-1/2}\bm{T}^{-1/2}\bm{Q}^T\left(\bm{Y}-\bm{X}\bm{\beta}\right)\nonumber\\
    &=\left(\bm{T}^{-1/2}\bm{Q}^T\bm{Y}-\bm{T}^{-1/2}\bm{Q}^T\bm{X}\bm{\beta}\right)^T\left(\bm{T}^{-1/2}\bm{Q}^T\bm{Y}-\bm{T}^{-1/2}\bm{Q}^T\bm{X}\bm{\beta}\right)\nonumber\\
    &=\left(\bm{\tilde{Y}}-\bm{\tilde{X}}\bm{\beta}\right)^T\left(\bm{\tilde{Y}}-\bm{\tilde{X}}\bm{\beta}\right),
\end{align}
where
\begin{align}
  \bm{\tilde{X}}=\bm{T}^{-1/2}\bm{Q}^T\bm{X}\;\;\;\;\;\;\;\text{and}\;\;\;\;\;\;\bm{\tilde{Y}}=\bm{T}^{-1/2}\bm{Q}^T\bm{Y}.
\end{align}
The bias introduced by regularizing the sum of squares exists by way of the addition of a weighted functional $\tau\,\bm{\Omega}(\bm{\beta})$. As an operator it acts on $\bm{\beta}$ in a fixed way as determined by the needs of the analyst, and $\tau$ (sometimes $\alpha$ or $\lambda$) is varied to shift the location of the minimum least-squares \cite{Tikh1977}. In this circumstance we are most concerned with reducing the oscillations in our final estimate by encouraging smoothness in the estimated distribution. The general practice is to use an estimate of the second derivative between the ordered elements of $\bm{\beta}$ by way of a matrix operator $\bm{C}$, with the overall goal of minimizing
\begin{align}
  \chi^2(\bm{\beta})
    &=\left(\bm{\tilde{Y}}-\bm{\tilde{X}}\bm{\beta}\right)^T\left(\bm{\tilde{Y}}-\bm{\tilde{X}}\bm{\beta}\right)\nonumber\\
    &=\left(\bm{\tilde{Y}}-\bm{\tilde{X}}\bm{\beta}\right)^T\left(\bm{\tilde{Y}}-\bm{\tilde{X}}\bm{\beta}\right)+\tau (\bm{C}\bm{\beta})^T\bm{C}\bm{\beta}\nonumber\\
    &=\left(\bm{\tilde{Y}}-\bm{\tilde{X}}\bm{\beta}\right)^T\left(\bm{\tilde{Y}}-\bm{\tilde{X}}\bm{\beta}\right)+\tau\bm{\beta}^T\bm{C}^T\bm{C}\bm{\beta}.\label{eq:regular}
\end{align}
The matrix $\bm{C}$ by default assumes a uniform bin width, but if that is not the case the bins can be weighted accordingly by a diagonal matrix $\bm{B}$ with positive elements $B_{ii}=\Delta x_i$, the width of bin $i$. Plugged in, this changes the regularizing expression in Equation \eqref{eq:regular} to
\begin{align}
  \tau\left(\bm{B}^{1/2}\bm{C}\bm{\beta}\right)^T\bm{B}^{1/2}\bm{C}\bm{\beta}\;\;=\;\;\tau\bm{\beta}^T\bm{C}^T\bm{B}\bm{C}\bm{\beta},\nonumber
\end{align}
where \cite{Cowan1998},
\begin{align}
  \begin{matrix}
    \bm{C}=\begin{pmatrix}
      -1&1&0&0&\dots&\\
      1&-2&1&0&\dots&&\\
      0&1&-2&1&\ddots&\\
      \vdots&\vdots&\vdots&\vdots&\ddots&\vdots&\vdots\\
      &&&&\dots&-2&1\\
      &&&&\dots&1&-1
    \end{pmatrix}&
    \text{and}&
    \bm{B}=\begin{pmatrix}
      \Delta x_1&0&\dots&&\\
      0&\Delta x_2 &\dots&&\\
      \vdots&\vdots&\ddots&\vdots&\vdots\\
      &&\dots&\Delta x_{M-1}&0\\
      &&\dots&0&\Delta x_M
    \end{pmatrix}.
  \end{matrix}\nonumber
\end{align}
Uniform bin widths where $\Delta x_i\neq1$ are equivalent to when $\Delta x_i=1$, with $\tau$ absorbing the appropriate scale factor. Continuing with uniform bin width, H$\ddot{\text{o}}$cker and Kartvelishvili conceptualize an equivalent to Equation \eqref{eq:mat} that involves stacking $N\times M$ matrix $\sqrt{\tau}\,\bm{C}$ below the response matrix and lengthening the Reconstructed vector by $N$ zeros, such that
\begin{align}
  \begin{bmatrix}
    \bm{\tilde{X}}\\\sqrt{\tau}\,\bm{C}
  \end{bmatrix}\bm{\beta} = 
  \begin{bmatrix}
    \bm{\tilde{Y}}\\
    \bm{0}_N
  \end{bmatrix},\nonumber
\end{align}
At this point you could perform SVD to calculate a pseudoinverse and be done. However, they instead implement the method of \textbf{damped least squares} to express the solution for $\tau>0$ in terms of the solution for $\tau=0$. They start with absorbing the matrix $\bm{C}$ into $\bm{\beta}$ to get $\begin{bmatrix}\bm{\tilde{X}}\bm{C}^{-1}\\\sqrt{\tau}\,\bm{I}_{N\times M}\end{bmatrix}\bm{C}\bm{\beta}$. The matrix $\bm{C}$ is notably singular, so $\bm{C}^{-1}$ does not exist. The writers work around this by adding a negligible value to its diagonal elements and creating the invertable matrix $\bm{\tilde{C}}=\bm{C}+\xi\bm{I}$, where typically $\xi=10^{-3}$ or $10^{-4}$.

Setting $\tau=0$ and applying singular value decomposition to the remaining matrix on the left-hand side, such that $\bm{\tilde{X}\tilde{C}}^{-1}=\bm{USV}^T$, will modify the least-squares equation to 
\begin{align}\chi^2
  &=\left(\bm{\tilde{Y}}-\bm{\tilde{X}}\bm{\beta}\right)^T\left(\bm{\tilde{Y}}-\bm{\tilde{X}}\bm{\beta}\right)\nonumber\\
  &=\left(\bm{\tilde{Y}}-\bm{\tilde{X}}\bm{\tilde{C}}^{-1}\bm{\tilde{C}}\bm{\beta}\right)^T\left(\bm{\tilde{Y}}-\bm{\tilde{X}}\bm{\tilde{C}}^{-1}\bm{\tilde{C}}\bm{\beta}\right)\nonumber\\
  &=\left(\bm{\tilde{Y}}-\bm{USV}^T\bm{\tilde{C}\beta}\right)^T\left(\bm{\tilde{Y}}-\bm{USV}^T\bm{\tilde{C}\beta}\right)\nonumber\\
  &=\left(\bm{\tilde{Y}}-\bm{USb}\right)^T\left(\bm{\tilde{Y}}-\bm{USb}\right)\nonumber\tag{$\bm{b}=\bm{V}^T\bm{\tilde{C}\beta}$}\\
  &=\left(\bm{\tilde{Y}}^T-\bm{b}^T\bm{S}^T\bm{U}^T\right)\left(\bm{\tilde{Y}}-\bm{USb}\right)\nonumber\\
  &=\bm{\tilde{Y}}^T\bm{UU}^T\bm{\tilde{Y}}-\bm{b}^T\bm{S}^T\bm{U}^T\bm{\tilde{Y}}-\bm{\tilde{Y}}^T\bm{USb}+\bm{b}^T\bm{S}^T\bm{U}^T\bm{USb}\tag{$\bm{UU}^T=\bm{U}^T\bm{U}=\bm{I}_{N\times N}$}\nonumber\\
  &=\bm{y}^T\bm{y}-\bm{b}^T\bm{S}^T\bm{y}-\bm{y}^T\bm{Sb}+\bm{b}^T\bm{S}^T\bm{S}\bm{b}\tag{$\bm{y}=\bm{U}^T\bm{\tilde{Y}}$}\nonumber\\
  &=(\bm{y}-\bm{Sb})^T(\bm{y}-\bm{Sb}).\nonumber
\end{align}
Since $\bm{S}$ is a diagonal matrix with elements $S_{ii}=s_i>0$ the solution can be found easily enough to be 
\begin{align}
  \hat{b}_i^{(0)}=\frac{y_i}{s_i}\;\;\;\;\text{followed by}\;\;\;\;\bm{\hat\beta}^{(0)}=\bm{\tilde{C}}^{-1}\bm{V\hat{b}}^{(0)}.
\end{align}
The estimated Truth counts would then come from reweighting $\bm{\hat\beta}$ such that $\hat\mu_i=\hat\beta_i\mu_i^{\text{MC}}$. Now since $\tau=0$ in this case there is yet no regularization. H$\ddot{\text{o}}$cker and Kartvelishvili provide a source regarding this part of damped least-squares that explains that introducing $\tau>0$ is accomplished just by modifying the non-regularized result to
\begin{align}
  y_i^{(\tau)}&=y_i^{(0)}\frac{s_i^2}{s_i^2+\tau}\nonumber\\
  \implies \hat{b}_i^{(\tau)}&=\frac{y_i^{(0)}s_i}{s_i^2+\tau},\nonumber
\end{align}
and so forth for $\bm{\hat\mu}$. The description of this last equation as a low-pass filter is fairly apt, as the extreme influence of small singular values, represented by diagonals $\{s_i\}$ in the diagonal matrix from the SVD of $\bm{\tilde{X}\tilde{C}}^{-1}$, are muffled by the presence of a nonzero $\tau$.

The analysts work out the resulting covariance matrices for these estimates, which are dependent on the value of $\tau$ such that
\begin{align}
  \hat\Sigma_{ij}^{b}&=\frac{s_i^2}{(s_i^2+\tau)^2}\delta_{ij},\nonumber\\
  \bm{\hat\Sigma}_{\beta}&=\bm{\tilde{C}}^{-1}\bm{V}\bm{\hat\Sigma}_b\bm{V}^T(\bm{\tilde{C}}^{-1})^T,\;\;\text{ and}\nonumber\\
  \hat\Sigma_{ij}^{\mu}&=\mu_i^{\text{MC}}\hat\Sigma_{ij}^\beta\mu_j^{\text{MC}}.\nonumber
\end{align}

```{r, echo = F}
C <- rbind(rep(0,30),cbind(diag(1,29),rep(0,29))) + 
  rbind(cbind(rep(0,29),diag(1,29)),rep(0,30)) +
  diag(c(-1,rep(-2,28),-1))

tC <- C + diag(10^(-3),30)

inv_tC <- svd(tC)$v %*% solve(diag(svd(tC)$d)) %*% t(svd(tC)$u)

X1 <- R_mc1 %*% diag(mcTruth1$Counts/dmrat)
X2 <- R_mc2 %*% diag(mcTruth2$Counts/dmrat)

SigNu1 <- diag(dataReco1$Counts)
SigNu2 <- diag(dataReco2$Counts)

Y1 <- dataReco1$Counts
Y2 <- dataReco2$Counts

# Decompose the covariance matrix
Q1 <- svd(SigNu1)$u
Q2 <- svd(SigNu2)$u
T1 <- diag(svd(SigNu1)$d)
T2 <- diag(svd(SigNu1)$d)

tXm1d1 <- sqrt(solve(T1)) %*% t(Q1) %*% X1
tXm1d2 <- sqrt(solve(T2)) %*% t(Q2) %*% X1
tXm2d1 <- sqrt(solve(T1)) %*% t(Q1) %*% X2
tXm2d2 <- sqrt(solve(T2)) %*% t(Q2) %*% X2
tY1 <- sqrt(solve(T1)) %*% t(Q1) %*% Y1 
tY2 <- sqrt(solve(T2)) %*% t(Q2) %*% Y2 

Um1d1 <- svd(tXm1d1 %*% inv_tC)$u
Um1d2 <- svd(tXm1d2 %*% inv_tC)$u
Um2d1 <- svd(tXm2d1 %*% inv_tC)$u
Um2d2 <- svd(tXm2d2 %*% inv_tC)$u
Vm1d1 <- svd(tXm1d1 %*% inv_tC)$v
Vm1d2 <- svd(tXm1d2 %*% inv_tC)$v
Vm2d1 <- svd(tXm2d1 %*% inv_tC)$v
Vm2d2 <- svd(tXm2d2 %*% inv_tC)$v
Sm1d1 <- diag(svd(tXm1d1 %*% inv_tC)$d)
Sm1d2 <- diag(svd(tXm1d2 %*% inv_tC)$d)
Sm2d1 <- diag(svd(tXm2d1 %*% inv_tC)$d)
Sm2d2 <- diag(svd(tXm2d2 %*% inv_tC)$d)

s11 <- diag(Sm1d1)
s12 <- diag(Sm1d2)
s21 <- diag(Sm2d1)
s22 <- diag(Sm2d2)

y11 <- t(Um1d1) %*% tY1
y12 <- t(Um1d2) %*% tY2
y21 <- t(Um2d1) %*% tY1
y22 <- t(Um2d2) %*% tY2
```

\begin{figure}[!ht]
    \centering
```{r, echo = FALSE, fig.height=3.5, fig.align='center'}
findTau <- tibble(x = rep(c(0,rep(1:29,each=2),30),4),
                  mag_y = abs(rep(c(y11,y12,y21,y22),each=2)),
                  label = c(rep("Data 1 unfolded with MC 1",60),
                            rep("Data 2 unfolded with MC 1",60),
                            rep("Data 1 unfolded with MC 2",60),
                            rep("Data 2 unfolded with MC 2",60)))

sTau <- rep(c(s11,s12,s21,s22)^2,each=2)/
  (rep(c(s11,s12,s21,s22)^2,each=2) + 
     rep(c(s11[4],s12[4],s21[4],s22[4])^2,each=60))

findTau <- rbind(findTau,
                 findTau %>% mutate(mag_y = mag_y*sTau)) %>%
  mutate(dcat = rep(c("yi0","yiTau10"),each=240))

tauArrow <- tibble(x=c(4.5,4.5,4.5,4.5),y=5e-04,
                   xend=c(4.5,4.5,4.5,4.5),
                   yend=c(0.5,0.25,0.5,0.25),
                   label = c("Data 1 unfolded with MC 1",
                             "Data 2 unfolded with MC 1",
                             "Data 1 unfolded with MC 2",
                             "Data 2 unfolded with MC 2"))
ggplot() + theme_bw() +
  geom_hline(yintercept = 1, lty = "dotted") +
  geom_line(data = findTau,
            mapping = aes(x=x,
                          y=mag_y,
                          color=label,
                          linetype=dcat)) +
  scale_color_manual(values = c("#5e81b5","#e19c24","#eb6235","#8fb032"),
                     guide = "none") +
  scale_fill_manual(values = c("#5e81b5","#e19c24","#eb6235","#8fb032"),
                    guide = "none") +
  scale_y_log10(name = expression(paste("decomposition coefficient weight |",y[i],"|"))) +
  scale_x_continuous(name = "decomposition coefficient i",
                     limits = c(0,31),
                     breaks = seq(0,30,5),
                     expand = c(0,0)) +
  scale_linetype_manual(name = element_blank(),
                        breaks = c("yi0","yiTau10"),
                        labels = c(expression(paste("|",y[i]^(0),"|")),
                                   expression(paste("|",y[i]^(tau),"|"))),
                        values = c("yi0"="solid","yiTau10"="longdash")) +
  geom_segment(data = tauArrow,
               mapping = aes(x=x,y=y,xend=xend,yend=yend),
               arrow = arrow(length = unit(4, "pt"))) +
  geom_text(data = tibble(x=c(5,5,5,5),
                          y=c(1e-05),
                          lab=c("k==5","k==5","k==5","k==5"),
                          label = c("Data 1 unfolded with MC 1",
                             "Data 1 unfolded with MC 2",
                             "Data 2 unfolded with MC 1",
                             "Data 2 unfolded with MC 2")),
            mapping = aes(x=x,y=y,label=lab), size = 3,
            parse = TRUE) +
  theme(axis.title = element_text(size = 9)) +
    facet_wrap(~label, ncol=2)

```
\caption{\emph{\small For each of the four cases the point at which $\vert y_i\vert$ drops consistently below 1 is approximately identified. This approach helps determine effective rank. This plot also shows how adding $\tau$ to get $\vert y_i^{(\tau)}\vert$ suppresses spurious influences due to statistical fluctuation.}}
  \label{TauSelect}
\end{figure}

```{r, echo = FALSE}

nMC <- 2
nData <- 2

tvec1 <- c(2^{-1:19}) # vector of trial taus
#tvec2 <- seq(4386.1,4386.35,0.01)
tvec2 <- seq(1527,1527.3,0.01)
tvecS <- c(s11,s12,s21,s22)^2

nt1 <- length(tvec1)
nt2 <- length(tvec2)
ntS <- length(tvecS)

taus <- array(0, dim=c(nMC,nData,nt1+nt2+ntS))
for(i in 1:nMC) for(j in 1:nData) taus[i,j,] <- c(tvec1,tvec2,tvecS)

chis11d <- chis12d <- chis21d <- chis22d <- 
  chis11m <- chis12m <- chis21m <- chis22m <- 
  chis12m2 <- chis21m1 <- rep(0,nt1+nt2+ntS)

mu1 <- mcTruth1$Counts
mu2 <- mcTruth2$Counts


hats <- tibble(X = rep(seq(0.5,29.5,1),4*length(taus[1,1,])),
               CVCounts = rep(0,30*4*length(taus[1,1,])),
               label = rep(rep(c("Data 1 unfolded with MC 1",
                                 "Data 2 unfolded with MC 1",
                                 "Data 1 unfolded with MC 2",
                                 "Data 2 unfolded with MC 2"),each=30),
                           length(taus[1,1,])),
               SD = rep(0,30*4*length(taus[1,1,])),
               MC = rep(rep(c("MC 1","MC 2"),each=2*30),
                        length(taus[1,1,])),
               Data = rep(rep(c("Data 1","Data 2",
                            "Data 1","Data 2"),each=30),
                          length(taus[1,1,])),
               Taus = c(rep(c(t(cbind(taus[1,1,],taus[1,2,],
                                      taus[2,1,],taus[2,2,]))),each=30)))

truth <- tibble(X = rep(c(0,rep(1:29,each=2),30),8),
                TruthCounts = c(rep(rep(mcTruth1$Counts/dmrat,each=2),2),
                                rep(rep(mcTruth2$Counts/dmrat,each=2),2),
                                rep(rep(dataTruth1$Counts,each=2),2),
                                rep(rep(dataTruth2$Counts,each=2),2)),
                MC = rep(c("MC 1","MC 2","Data 1","Data 2"),each=120),
                Data = c(rep(rep(c("Data 1","Data 2"),each=60),2),
                         rep(rep(c("MC 1","MC 2"),each=60),2)))



inv_sigMuHat11 <- inv_sigMuHat12 <- inv_sigMuHat21 <- 
  inv_sigMuHat22 <- matrix(0, nrow = 30, ncol = 30)
for(i in 1:30){
  for(j in 1:30){
    inv_sigMuHat11[i,j] <- 
      sum(tXm1d1[,i]*tXm1d1[,j])/((mu1[i]*mu1[j])/dmrat^2)
    inv_sigMuHat12[i,j] <- 
      sum(tXm1d2[,i]*tXm1d2[,j])/((mu1[i]*mu1[j])/dmrat^2)
    inv_sigMuHat21[i,j] <- 
      sum(tXm2d1[,i]*tXm2d1[,j])/((mu2[i]*mu2[j])/dmrat^2)
    inv_sigMuHat22[i,j] <- 
      sum(tXm2d2[,i]*tXm2d2[,j])/((mu2[i]*mu2[j])/dmrat^2)
  }
}

for(t in 1:(nt1+nt2+ntS)){
  
  bTauHat11 <- bTauHat12 <- 
    bTauHat21 <- bTauHat22 <- rep(0,30)
  for(i in 1:30){
    bTauHat11[i] <- (y11[i]*s11[i])/(s11[i]^2 + taus[1,1,t])
    bTauHat12[i] <- (y12[i]*s12[i])/(s12[i]^2 + taus[1,2,t])
    bTauHat21[i] <- (y21[i]*s21[i])/(s21[i]^2 + taus[2,1,t])
    bTauHat22[i] <- (y22[i]*s22[i])/(s22[i]^2 + taus[2,2,t])
  }
  
  betaTauHat11 <- inv_tC %*% Vm1d1 %*% bTauHat11
  betaTauHat12 <- inv_tC %*% Vm1d2 %*% bTauHat12
  betaTauHat21 <- inv_tC %*% Vm2d1 %*% bTauHat21
  betaTauHat22 <- inv_tC %*% Vm2d2 %*% bTauHat22
  
  muTauHat11 <- betaTauHat11*(mcTruth1$Counts/dmrat)
  muTauHat12 <- betaTauHat12*(mcTruth1$Counts/dmrat)
  muTauHat21 <- betaTauHat21*(mcTruth2$Counts/dmrat)
  muTauHat22 <- betaTauHat22*(mcTruth2$Counts/dmrat)
  
  hats$CVCounts[((t-1)*120 + 1):(t*120)] <-
    c(muTauHat11,muTauHat12,muTauHat21,muTauHat22)
  
  sig_b_hat11 <- sig_b_hat12 <- sig_b_hat21 <- 
    sig_b_hat22 <- matrix(0, nrow = 30, ncol = 30)
  for(i in 1:30){
    sig_b_hat11[i,i] <- s11[i]^2/(s11[i]^2 + taus[1,1,t])^2
    sig_b_hat12[i,i] <- s12[i]^2/(s12[i]^2 + taus[1,2,t])^2
    sig_b_hat21[i,i] <- s21[i]^2/(s21[i]^2 + taus[2,1,t])^2
    sig_b_hat22[i,i] <- s22[i]^2/(s22[i]^2 + taus[2,2,t])^2
  }
  
  sig_beta_hat11 <- inv_tC %*% Vm1d1 %*% 
    sig_b_hat11 %*% t(Vm1d1) %*% t(inv_tC)
  sig_beta_hat12 <- inv_tC %*% Vm1d2 %*% 
    sig_b_hat12 %*% t(Vm1d2) %*% t(inv_tC)
  sig_beta_hat21 <- inv_tC %*% Vm2d1 %*% 
    sig_b_hat21 %*% t(Vm2d1) %*% t(inv_tC)
  sig_beta_hat22 <- inv_tC %*% Vm2d2 %*% 
    sig_b_hat22 %*% t(Vm2d2) %*% t(inv_tC)
  
  sigMuHat11 <- sigMuHat12 <- sigMuHat21 <- 
    sigMuHat22 <- matrix(0, nrow = 30, ncol = 30)
  for(i in 1:30){
    for(j in 1:30){
      sigMuHat11[i,j] <- 
        (mu1[i]/dmrat)*sig_beta_hat11[i,j]*(mu1[j]/dmrat)
      sigMuHat12[i,j] <- 
        (mu1[i]/dmrat)*sig_beta_hat12[i,j]*(mu1[j]/dmrat)
      sigMuHat21[i,j] <- 
        (mu2[i]/dmrat)*sig_beta_hat21[i,j]*(mu2[j]/dmrat)
      sigMuHat22[i,j] <- 
        (mu2[i]/dmrat)*sig_beta_hat22[i,j]*(mu2[j]/dmrat)
    }
  }
  hats$SD[((t-1)*120 + 1):(t*120)] <- 
    sqrt(c(diag(sigMuHat11),diag(sigMuHat12),
           diag(sigMuHat21),diag(sigMuHat22)))
  
  chis11d[t] <- t(muTauHat11 - dataTruth1$Counts) %*% 
    inv_sigMuHat11 %*% (muTauHat11 - dataTruth1$Counts)
  chis12d[t] <- t(muTauHat12 - dataTruth2$Counts) %*% 
    inv_sigMuHat12 %*% (muTauHat12 - dataTruth2$Counts)
  chis21d[t] <- t(muTauHat21 - dataTruth1$Counts) %*% 
    inv_sigMuHat21 %*% (muTauHat21 - dataTruth1$Counts)
  chis22d[t] <- t(muTauHat22 - dataTruth2$Counts) %*% 
    inv_sigMuHat22 %*% (muTauHat22 - dataTruth2$Counts)
  chis11m[t] <- t(muTauHat11 - mcTruth1$Counts/dmrat) %*% 
    inv_sigMuHat11 %*% (muTauHat11 - mcTruth1$Counts/dmrat)
  chis12m[t] <- t(muTauHat12 - mcTruth1$Counts/dmrat) %*% 
    inv_sigMuHat12 %*% (muTauHat12 - mcTruth1$Counts/dmrat)
  chis21m[t] <- t(muTauHat21 - mcTruth2$Counts/dmrat) %*% 
    inv_sigMuHat21 %*% (muTauHat21 - mcTruth2$Counts/dmrat)
  chis22m[t] <- t(muTauHat22 - mcTruth2$Counts/dmrat) %*% 
    inv_sigMuHat22 %*% (muTauHat22 - mcTruth2$Counts/dmrat)
  chis12m2[t] <- t(muTauHat12 - mcTruth2$Counts/dmrat) %*% 
    inv_sigMuHat12 %*% (muTauHat12 - mcTruth2$Counts/dmrat)
  chis21m1[t] <- t(muTauHat21 - mcTruth1$Counts/dmrat) %*% 
    inv_sigMuHat21 %*% (muTauHat21 - mcTruth1$Counts/dmrat)
  
}

compareChis1 <- 
  tibble(tau = c(taus[1,1,1:nt1],taus[1,2,1:nt1],taus[2,1,1:nt1],
                 taus[2,2,1:nt1],taus[1,1,1:nt1]), 
         Chi2 = c(chis11d[1:nt1],chis12d[1:nt1],chis21d[1:nt1],
                  chis22d[1:nt1],(chis11d+chis12d+chis21d+chis22d)[1:nt1]),
         col=c(rep("Data 1 unfolded with MC 1",
                   length(taus[1,1,1:nt1])),
               rep("Data 1 unfolded with MC 2",
                   length(taus[1,2,1:nt1])),
               rep("Data 2 unfolded with MC 1",
                   length(taus[2,1,1:nt1])),
               rep("Data 2 unfolded with MC 2",
                   length(taus[2,2,1:nt1])),
               rep("Sum(Data i unf. with MC j)",
                   length(taus[2,2,1:nt1]))))

compareChis2 <- 
  tibble(tau = c(taus[1,1,1:nt1],taus[1,2,1:nt1],taus[2,1,1:nt1],
                 taus[2,2,1:nt1],taus[1,1,1:nt1]), 
         Chi2 = c(chis11m[1:nt1],chis12m[1:nt1],chis21m[1:nt1],
                  chis22m[1:nt1],(chis11m+chis12m+chis21m+chis22m)[1:nt1]),
         col=c(rep("Data 1 unfolded with MC 1",
                   length(taus[1,1,1:nt1])),
               rep("Data 1 unfolded with MC 2",
                   length(taus[1,2,1:nt1])),
               rep("Data 2 unfolded with MC 1",
                   length(taus[2,1,1:nt1])),
               rep("Data 2 unfolded with MC 2",
                   length(taus[2,2,1:nt1])),
               rep("Sum(Data i unf. with MC j)",
                   length(taus[2,2,1:nt1]))))

sumChi2 <- chis11d+chis12d+chis21d+chis22d
sumChi2m <- chis11m+chis12m+chis21m+chis22m
minChi2_index <- nt1+which.min(sumChi2[(nt1+1):(nt1+nt2)])

Chi_plot <- ggplot() + theme_bw() +
  geom_hline(yintercept = 0, lty = 2) +
  geom_vline(xintercept = taus[1,1,minChi2_index], 
             linetype = 3, color = "#FF1493") +
  geom_line(data = compareChis1, 
            mapping = aes(x=tau, y=Chi2, color=col)) +
  scale_y_continuous(name = expression(chi^2)) +
  scale_x_log10(name = expression(tau)) + 
  theme(axis.text.x = element_text(size = 8, angle=-30)) +
  geom_point(data = compareChis1,
             mapping = aes(x=tau, y=Chi2, fill=col),
             shape = 21, size = 1) +
  scale_color_manual(name = "Pairings",
                     values = c("#5e81b5","#e19c24",
                                "#eb6235","#8fb032",
                                "#333333")) +
  scale_fill_manual(name = "Pairings",
                    values = c("#5e81b5","#e19c24",
                               "#eb6235","#8fb032",
                               "#888888")) +
  labs(title = "Unfolded vs Data Truth")

Chi_plot2 <- ggplot() + theme_bw() +
  geom_hline(yintercept = 0, lty = 2) +
  geom_vline(xintercept = taus[1,1,minChi2_index], 
             linetype = 3, color = "#FF1493") +
  geom_line(data = compareChis2, 
            mapping = aes(x=tau, y=Chi2, color=col)) +
  scale_y_continuous(name = expression(Chi^2)) +
  scale_x_log10(name = expression(tau)) + 
  theme(axis.text.x = element_text(size = 8,angle=-30)) +
  geom_point(data = compareChis2,
             mapping = aes(x=tau, y=Chi2, fill=col),
             shape = 21, size = 1) +
  scale_color_manual(name = "Pairings",
                     values = c("#5e81b5","#e19c24",
                                "#eb6235","#8fb032",
                                "#333333")) +
  scale_fill_manual(name = "Pairings",
                    values = c("#5e81b5","#e19c24",
                               "#eb6235","#8fb032",
                               "#888888")) +
  labs(title = "Unfolded vs MC Truth")

D1 <- solve(lower.tri(matrix(1,nt2,nt2),diag=T))
compareDeltaChis2 <- 
  tibble(tau = c(taus[1,1,(nt1+1):(nt1+nt2)][-1],
                 taus[1,2,(nt1+1):(nt1+nt2)][-1],
                 taus[2,1,(nt1+1):(nt1+nt2)][-1],
                 taus[2,2,(nt1+1):(nt1+nt2)][-1],
                 taus[1,1,(nt1+1):(nt1+nt2)][-1]), 
         dChi2 = c(c(D1%*%chis11d[(nt1+1):(nt1+nt2)])[-1],
                   (D1%*%chis12d[(nt1+1):(nt1+nt2)])[-1],
                   (D1%*%chis21d[(nt1+1):(nt1+nt2)])[-1],
                   (D1%*%chis22d[(nt1+1):(nt1+nt2)])[-1],
                   (D1%*%(chis11d+chis12d+chis21d+chis22d)[(nt1+1):(nt1+nt2)])[-1]),
         col=c(rep("Data 1 unfolded with MC 1",
                   length(taus[1,1,(nt1+1):(nt1+nt2)][-1])),
               rep("Data 1 unfolded with MC 2",
                   length(taus[1,2,(nt1+1):(nt1+nt2)][-1])),
               rep("Data 2 unfolded with MC 1",
                   length(taus[2,1,(nt1+1):(nt1+nt2)][-1])),
               rep("Data 2 unfolded with MC 2",
                   length(taus[2,2,(nt1+1):(nt1+nt2)][-1])),
               rep("Sum(Data i unf. with MC j)",
                   length(taus[2,2,(nt1+1):(nt1+nt2)][-1]))))

dChi_plot2 <- ggplot() + theme_bw() +
  geom_hline(yintercept = 0, lty = 2) +
  geom_line(data = compareDeltaChis2, 
            mapping = aes(x=tau, y=abs(dChi2), color=col)) +
  geom_point(data = compareDeltaChis2,
             mapping = aes(x=tau, y=abs(dChi2), fill=col),
             shape = 21) +
  scale_y_log10(name = expression(paste("Abs(",Delta,chi^2,")"))) +
  scale_x_continuous(name = expression(tau)) +
  labs(title = expression(paste("Plot of |",chi^2,(tau[i])-chi^2,(tau[i-1]),
                                "| for Data Unfolded vs Data Truth"))) +
  theme(axis.text.x = element_text(size = 8,angle=-30),
        plot.title = element_text(size = 12)) +
  scale_color_manual(name = "Pairings",
                     values = c("#5e81b5","#e19c24",
                                "#eb6235","#8fb032",
                                "#333333")) +
  scale_fill_manual(name = "Pairings",
                    values = c("#5e81b5","#e19c24",
                               "#eb6235","#8fb032",
                               "#888888"))

Chi2table <- tibble(MC = c(1,1,2,2,NA),
                    Data = c(1,2,1,2,NA),
                    s5Tau = c(taus[1,1,nt1+nt2+5],
                              taus[1,2,nt1+nt2+5],
                              taus[2,1,nt1+nt2+5],
                              taus[2,2,nt1+nt2+5],
                              NA),
                    s5Chi2 = c(chis11d[nt1+nt2+5],
                               chis12d[nt1+nt2+5],
                               chis21d[nt1+nt2+5],
                               chis22d[nt1+nt2+5],
                               sumChi2[nt1+nt2+5]),
                    s5Chi2m = c(chis11m[nt1+nt2+5],
                                 chis12m[nt1+nt2+5],
                                 chis21m[nt1+nt2+5],
                                 chis22m[nt1+nt2+5],
                                 sumChi2m[nt1+nt2+5]),
                    minTau = c(taus[1,1,minChi2_index],
                               taus[1,2,minChi2_index],
                               taus[2,1,minChi2_index],
                               taus[2,2,minChi2_index],
                               NA),
                    minChi2 = c(chis11d[minChi2_index],
                                chis12d[minChi2_index],
                                chis21d[minChi2_index],
                                chis22d[minChi2_index],
                                sumChi2[minChi2_index]),
                    minChi2m = c(chis11m[minChi2_index],
                                 chis12m[minChi2_index],
                                 chis21m[minChi2_index],
                                 chis22m[minChi2_index],
                                 sumChi2m[minChi2_index]))
Chi2table <- round(Chi2table,2)
```
\begin{figure}[!ht]
    \centering
```{r,echo = F, fig.align='center', fig.height=3.3}
egg::ggarrange(Chi_plot + 
                 theme(legend.position = "none",
                       axis.title.y = element_blank()), 
               Chi_plot2 + theme(axis.title.y = element_blank()), 
               left = textGrob(expression(chi^2), rot = 90, vjust = 0.25),
               nrow = 1)
```
\caption{\emph{\small Above are plots of the individual $\chi^2$s and their summation $\sum\chi^2$ with respect to selected values of $\tau$. On the left the unfolded counts were compared to their actual true counts. On the right the unfolded counts were compared to the true counts of the MC model used to unfold them. The dotted vertical pink line indicates the value of $\tau$ that minimizes $\sum\chi^2$ for the plot on the left.}}
  \label{chicomp}
\end{figure}

As far as selecting a value of $\tau$, H$\ddot{\text{o}}$cker and Kartvelishvili recommend first plotting $\vert y_i\vert$ with respect to $i$. They point out that $\bm{y}$ makes up the coefficients of the decomposition of the reconstructed counts $\bm{n}$ ($\bm{Y}$ above), with an orthogonal basis given by the columns of $\bm{U}$. It is expected that $\vert y_i\vert$ will decay exponentially with $i$ until some critical value $i=k$ in which all following coefficients will be indistinguishable from random samples from an $N(0,1)$ distribution. Once that value of $k$ has been identified the idea is to set $\tau=s_k^2$. For this paper's simulations these identifications are graphically identified in Figure \ref{TauSelect}, which shows the plot of $\vert y_i^{(0)}\vert$ and the resulting $\vert y_i^{(\tau)}\vert$. The same value of $k$ was chosen for all four combinations for consistency.

This manner of choosing the regularization parameter is just one suggestion, and while its reasoning is fairly clear there is another way that considers values of $\tau$ not restricted to squares of the singular values. Figure \ref{chicomp} features two plots concerning the sums of squares for different values of $\tau$, which were calculated by comparing the unfolded distributions to their true distributions (LEFT) and to the true distributions for the MC model used in the unfolding process (RIGHT). The Data, being also simulated, are playing the role of training sets in this capacity. The plot on the left reveals the existance of a critical value of $\tau$ above which an unfolded distribution is overfitted to the associated MC distribution, which will hide features of the original distribution that are a departure from the model being tested. The plot on the right contains no such indicators.

As the purpose of unfolding is to undo effects related to the measurement process, it would be ideal for a method to be model agnostic. As a MC simulation is used to generate the response matrix this is unfortunately impossible. However, if an unknown true model is expected to produce similar looking distributions to an assumed model, we can rely on them behaving similarly under an unfolding procedure using the same regularization parameter, as the behaviors shown in Figure \ref{chicomp} depict.

\begin{table}[!hb]
\centering
\begin{tabular}{| c | c || c | c | c || c | c | c |} 
 \hline
 MC & Data & $\tau=s_5^2$ & Data $\chi^2$ & MC $\chi^2$ & $\min\limits_{\tau}\sum\chi^2$ & Data $\chi^2$ & MC $\chi^2$ \\
 \hline\hline
 1 & 1 & $`r Chi2table$s5Tau[1]`$ & $`r Chi2table$s5Chi2[1]`$ & $`r Chi2table$s5Chi2m[1]`$ & $`r Chi2table$minTau[1]`$ & $`r Chi2table$minChi2[1]`$ & $`r Chi2table$minChi2m[1]`$ \\ 
 1 & 2 & $`r Chi2table$s5Tau[2]`$ & $`r Chi2table$s5Chi2[2]`$ & $`r Chi2table$s5Chi2m[2]`$ &  $`r Chi2table$minTau[2]`$ & $`r Chi2table$minChi2[2]`$ & $`r Chi2table$minChi2m[2]`$ \\
 2 & 1 & $`r Chi2table$s5Tau[3]`$ & $`r Chi2table$s5Chi2[3]`$ & $`r Chi2table$s5Chi2m[3]`$ & $`r Chi2table$minTau[3]`$ & $`r Chi2table$minChi2[3]`$ & $`r Chi2table$minChi2m[3]`$ \\
 2 & 2 & $`r Chi2table$s5Tau[4]`$ & $`r Chi2table$s5Chi2[4]`$ & $`r Chi2table$s5Chi2m[4]`$ & $`r Chi2table$minTau[4]`$ & $`r Chi2table$minChi2[4]`$ & $`r Chi2table$minChi2m[4]`$ \\
 \hline\hline
 \multicolumn{2}{|c||}{All} & $\sum\chi^2$ & $`r Chi2table$s5Chi2[5]`$ & $`r Chi2table$s5Chi2m[5]`$ & $\sum\chi^2$ & $`r Chi2table$minChi2[5]`$ & $`r Chi2table$minChi2m[5]`$\\
 \hline
\end{tabular}
\caption{\emph{The $\chi^2$s and their summations $\sum\chi^2$ for the listed values of $\tau$ as chosen by their respective selection methods. Data $\chi^2$ columns are a result of comparing unfolded Data to the Data's true counts. Unfolded Data from Model 1 \textbf{is never compared} to Model 2's Data true counts, and vice versa. For the MC $\chi^2$ columns the unfolded Data is compared to the true counts of the MC Model under which it was unfolded. Unfolded Data from Model 1 and Model 2 \textbf{are compared} to Model 1's MC true counts when they are unfolded using Model 1 MC and they \textbf{are compared} to Model 2's MC true counts when they are unfolded using Model 2 MC.}}
\label{tab:chi2}
\end{table}

\begin{figure}[!ht]
    \centering
```{r, echo = FALSE, fig.align='center', fig.height=3.9}
k <- 5
tau <- taus[1,1,minChi2_index]

unfplot_t <- ggplot() + theme_bw() + geom_hline(yintercept = 0, lty = 2) +
  geom_line(data = truth[c(1:60,121:180),],
            mapping = aes(x=X, y=TruthCounts, color = MC)) +
  geom_errorbar(data = hats %>% 
                 filter((MC == "MC 1" & Data == "Data 1" & Taus == tau) |
                          (MC == "MC 1" & Data == "Data 2" & Taus == tau) |
                          (MC == "MC 2" & Data == "Data 1" & Taus == tau) |
                          (MC == "MC 2" & Data == "Data 2" & Taus == tau)),
               mapping = aes(x=X,ymin=CVCounts-SD,ymax=CVCounts+SD),
               width = 0.2) +
  geom_point(data = hats %>% 
                 filter((MC == "MC 1" & Data == "Data 1" & Taus == tau) |
                          (MC == "MC 1" & Data == "Data 2" & Taus == tau) |
                          (MC == "MC 2" & Data == "Data 1" & Taus == tau) |
                          (MC == "MC 2" & Data == "Data 2" & Taus == tau)),
             mapping = aes(x=X, y=CVCounts, fill = Data), shape = 21, size = 1.25) +
  scale_fill_manual(name = "Unfolded Data",
                    labels = c("Data 1" = "Model 1",
                               "Data 2" = "Model 2"),
                    values = c("Data 1" = "#5e81b5",
                               "Data 2" = "#e19c24")) +
  scale_color_manual(name = "MC Truth",
                     labels = c("MC 1" = "Model 1",
                                "MC 2" = "Model 2"),
                     values = c("MC 1" = "#5e81b5",
                                "MC 2" = "#e19c24")) +
  scale_x_continuous(limits = c(0,30),
                     expand = c(0,0),
                     name = "X (Truth)",
                     breaks = seq(0,30,3)) +
  scale_y_continuous(name = "Counts") + theme(axis.text.x = element_text(size = 8)) +
  labs(title = bquote("SVD Unfolded ("~tau==.(tau)~")")) +
  facet_wrap(~label)

unfplot_k <- ggplot() + theme_bw() + geom_hline(yintercept = 0, lty = 2) +
  geom_line(data = truth[c(1:60,121:180),],
            mapping = aes(x=X, y=TruthCounts, color = MC)) +
  geom_errorbar(data = hats %>% 
                 filter((MC == "MC 1" & Data == "Data 1" & Taus == s11[k]^2) |
                          (MC == "MC 1" & Data == "Data 2" & Taus == s12[k]^2) |
                          (MC == "MC 2" & Data == "Data 1" & Taus == s21[k]^2) |
                          (MC == "MC 2" & Data == "Data 2" & Taus == s22[k]^2)),
               mapping = aes(x=X,ymin=CVCounts-SD,ymax=CVCounts+SD),
               width = 0.2) +
  geom_point(data = hats %>% 
                 filter((MC == "MC 1" & Data == "Data 1" & Taus == s11[k]^2) |
                          (MC == "MC 1" & Data == "Data 2" & Taus == s12[k]^2) |
                          (MC == "MC 2" & Data == "Data 1" & Taus == s21[k]^2) |
                          (MC == "MC 2" & Data == "Data 2" & Taus == s22[k]^2)),
             mapping = aes(x=X, y=CVCounts, fill = Data), shape = 21, size = 1.5) +
  scale_fill_manual(name = "Unfolded Data",
                    labels = c("Data 1" = "Model 1",
                               "Data 2" = "Model 2"),
                    values = c("Data 1" = "#5e81b5",
                               "Data 2" = "#e19c24")) +
  scale_color_manual(name = "MC Truth",
                     labels = c("MC 1" = "Model 1",
                                "MC 2" = "Model 2"),
                     values = c("MC 1" = "#5e81b5",
                                "MC 2" = "#e19c24")) +
  scale_x_continuous(limits = c(0,30),expand = c(0,0),name = "X (Truth)") +
  scale_y_continuous(name = "Counts") +
  labs(title = expression(paste("SVD Unfolded (",k==5,")"))) +
  facet_wrap(~label)
unfplot_t

#(svd(rbind(tXm1d1,sqrt(4800)*tC))$v %*% diag(svd(rbind(tXm1d1,sqrt(4800)*tC))$d) %*% t(svd(rbind(tXm1d1,sqrt(4800)*tC))$u)) %*% c(tY1,rep(0,30))
```
\caption{\emph{\small Unfolded Data superimposed on both MC Truth histograms. Data from both Model 1 and 2 had a difficult time unfolding using the opposite Model's MC response matrix.}}
  \label{unfolded}
\end{figure}

In particular, when both sets of Data are paired with the wrong MC model during unfolding, they both consistently behave similarly across the observed regions of $\tau$ values, and reach a minimum at a value of $\tau$ that aligns well with confirmatory $\chi^2$ values for the same Data when paired with the correct MC models. As a contrast to finding $k$ as depicted in Figure \ref{TauSelect}, these observations inform a method for choosing the value of $\tau$ which minimizes the combined sum of squares of the residuals between the unfolded counts of test Data sets and their true counts.

A numerical comparison of both methods' sums of squares is given in Table \ref{tab:chi2} for unfolded Data against its true counts and for unfolded data against the MC true counts from the Model under which the Data was unfolded. From the individual $\chi^2$s it is evident that the method of minimizing the total sums of squares $\sum\chi^2$ does better at minimizing Model bias while unfolding Data to estimate their true counts, and the $k=5$ method does a better job fitting unfolded Data to the MC true counts regardless of whether or not the correct Model was used, indicating a higher degree of bias. However, by a quite noticeable margin the MC $\chi^2$ columns for both methods clearly and correctly indicate which Model is more likely to have produced the Data being unfolded. 

Complementing this observation is Figure \ref{unfolded}, which depicts each of the unfolded results plotted against both MC simulations' true counts. It is clear from the plots that Data unfolded under the correct Model better approximates true counts of the MC. Which Model the unfolded Data belongs to in the mismatched unfolding cases is visually more ambiguous. Numerically, for Data from Model 1 unfolded using Model 2 MC, the $\chi^2$ when comparing to Model 1 MC true counts is `r round(chis21m1[minChi2_index],2)` (vs. `r Chi2table$minChi2m[2]` for Model 2), and for Data from Model 2 unfolded using Model 1 MC, the $\chi^2$ when comparing to Model 2 MC true counts is `r round(chis12m2[minChi2_index],2)` (vs. `r Chi2table$minChi2m[3]` for Model 1). As such, SVD unfolding continues to clearly support the correct Model.

\section{Conclusion}

The SVD Unfolding method described here has had a storied career in particle physics experiments since its introduction \cite{ALEPH:1998}\cite{CLEO:1999dln}\cite{OPAL:2002plk}\cite{KLOE:2004lnj}\cite{ALEPH:2005}\cite{MiniBooNE:2009dxl}\cite{IceCube:2010whx}\cite{ATLAS:2011juz}\cite{ATLAS:2012tjt}\cite{Belle:2012}\cite{CDF:2012ctl}\cite{ALICE:2013dpt}\cite{PierreAuger:2014}\cite{CMS:2015}\cite{DayaBay:2015}. There are however many other unfolding methods that are currently heavily implemented. One is an iterative unfolding method based on Bayes' theorem \cite{DAgostini1994}\cite{DAgostini2010}. Popular among experiments at the Large Hadron Collider is the unfolding method SPlot \cite{Pivk:2004}, which focuses on unfolding the individual contributions to a signal of interest, such as those making up Model 1 and Model 2 here. Also of note is another unfolding method based on least-squares and Tikhonov regularization called TUnfold \cite{Schmitt2012}, as well as a self-described iterative, dynamically stabilized (IDS) method of unfolding \cite{Malaescu:2009}\cite{Malaescu:2011}. In contrast to these methods is an ongoing discussion about whether unfolded histograms need to be, or should be, used to test hypotheses. Some work has been done towards exploring this question and developing a "bottom-line-test" that directly compares theoretical predictions that have been smeared and distorted by a simulated detector response to detector data that has not been unfolded \cite{Cousins:2016}. With that said, unfolding continues to be an extremely important part of particle physics analyses. It is not uncommon to consult multiple methods of unfolding for an analysis, and any such method of hypothesis testing that does not involve unfolding would at this time likely exist as another supplementary method in this way.

\section{Acknowledgements}
I am grateful to my physics advisor, Heidi Schellman, for making it possible for me to study statistics these last few years in concurrence with my physics PhD research. Through her I have been funded in part by the National Science Foundation. It is my hope that the material I've learned in pursuing a statistics degree will makeup for any misgivings she may have had along the way. I would also like to thank my statistics advisor, James Molyneux, for taking me in when my previous advisor left Oregon State University for a position elsewhere. His active engagement and endless optimism have been a guiding light towards my completion of this less traditional statistics MS project. He really unfolded away some of the negative biases and uncertainties in my own perception of my work. I would also like to thank Sarah Emerson and Sharmodeep Bhattacharyya for being my committee members as well as being excellent disseminators of statistics knowledge and wisdom. I could not have asked for better instructors for the foundational courses I took under them. Finally, I need to thank my partner, Katarina. She constantly inspires me to be a better person in ways that positively impact my relationship with others, with my work, and with myself. I love you, Kat.

\newpage

\appendix

\section{Extra Plots}

Below are some supplementary plots relevant to Section \ref{sub:reg}.

\begin{figure}[!ht]
    \centering
```{r, echo = FALSE, fig.align='center', fig.height=3}
diffdat1 <- hats %>% filter((MC == "MC 1" & Data == "Data 1" & Taus == tau) |
                  (MC == "MC 1" & Data == "Data 2" & Taus == tau) |
                  (MC == "MC 2" & Data == "Data 1" & Taus == tau) |
                  (MC == "MC 2" & Data == "Data 2" & Taus == tau)) %>%
  mutate(MCTruth = rep(c(mcTruth1$Counts,mcTruth2$Counts),2))
diffdat1$Data[which(diffdat1$Data=="Data 1")] <- "Model 1"
diffdat1$Data[which(diffdat1$Data=="Data 2")] <- "Model 2"

diffdat2 <- hats %>% filter((MC == "MC 1" & Data == "Data 1" & Taus == tau) |
                  (MC == "MC 1" & Data == "Data 2" & Taus == tau) |
                  (MC == "MC 2" & Data == "Data 1" & Taus == tau) |
                  (MC == "MC 2" & Data == "Data 2" & Taus == tau)) %>%
  mutate(DataTruth = c(dataTruth1$Counts,dataTruth1$Counts,
                       dataTruth2$Counts,dataTruth2$Counts))
diffdat2$Data[which(diffdat1$Data=="Data 1")] <- "Model 1"
diffdat2$Data[which(diffdat1$Data=="Data 2")] <- "Model 2"

areadat1 <- tibble(X = rep(c(0,rep(1:30,each=2),rep(29:1,each=2),0,0),4),
                  Y = c(rep(c(rep(mcTruth1$Counts/dmrat,each=2),
                              -rev(rep(mcTruth1$Counts/dmrat,each=2)),
                              mcTruth1$Counts[1]/dmrat,
                              rep(mcTruth2$Counts/dmrat,each=2),
                              -rev(rep(mcTruth2$Counts/dmrat,each=2)),
                              mcTruth2$Counts[1]/dmrat),2)),
                  label = rep(c("Data 1 unfolded with MC 1","Data 1 unfolded with MC 2",
                                "Data 2 unfolded with MC 1","Data 2 unfolded with MC 2"),each=121),
                  MC = c(rep("Model 1",242),rep("Model 2",242)))

areadat2 <- tibble(X = rep(c(0,rep(1:30,each=2),rep(29:1,each=2),0,0),4),
                  Y = c(rep(c(rep(dataTruth1$Counts,each=2),
                              -rev(rep(dataTruth1$Counts,each=2)),
                              dataTruth1$Counts[1]),2),
                            rep(c(rep(dataTruth2$Counts,each=2),
                                  -rev(rep(dataTruth2$Counts,each=2)),
                                  dataTruth2$Counts[1]),2)),
                  label = rep(c("Data 1 unfolded with MC 1","Data 1 unfolded with MC 2",
                                "Data 2 unfolded with MC 1","Data 2 unfolded with MC 2"),each=121),
                  `Data Truth` = c(rep("Model 1",242),rep("Model 2",242)))

ggplot() + theme_bw() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_polygon(data = areadat1,
            mapping = aes(x=X,y=sign(Y)*sqrt(abs(Y)),fill=label,color=label),
            linetype = "dotted") +
  geom_polygon(data = areadat1,
            mapping = aes(x=X,y=2*sign(Y)*sqrt(abs(Y)),fill=label,color=label),
            linetype = "dotted") +
  geom_errorbar(data = diffdat1,
                mapping = aes(x=X,ymin=CVCounts-MCTruth/dmrat-SD,
                              ymax=CVCounts-MCTruth/dmrat+SD),width=0.2) +
  geom_point(data = diffdat1,
             mapping = aes(x=X,y=CVCounts-MCTruth/dmrat,fill=label), shape = 21,size=1.5) +
  geom_point(data = diffdat1,
             mapping = aes(x=X,y=CVCounts-MCTruth/dmrat,color=label), shape = 16,size=1) +
  scale_color_manual(name = "Pairings",
                     values = c("Data 1 unfolded with MC 1"="#5e81b5",
                                "Data 1 unfolded with MC 2"="#5e81b5",
                                "Data 2 unfolded with MC 1"="#e19c24",
                                "Data 2 unfolded with MC 2"="#e19c24")) +
  scale_fill_manual(name = "Pairings",
                    values = c("Data 1 unfolded with MC 1"=alpha("#5e81b5",0.3),
                               "Data 1 unfolded with MC 2"=alpha("#e19c24",0.3),
                               "Data 2 unfolded with MC 1"=alpha("#5e81b5",0.3),
                               "Data 2 unfolded with MC 2"=alpha("#e19c24",0.3))) +
  scale_x_continuous(expand = c(0,0), breaks = seq(0,30,3),
                     name = "X (Truth)") +
  scale_y_continuous(name = TeX("$\\hat{\\mu}_i-\\mu^{MC}_j$")) +
  theme(axis.text.x = element_text(size = 8)) +
  facet_wrap( ~label,nrow=2) +
  labs(title = "Residual of Unfolded Data to MC used in SVD Unfolding")
```
\caption{\emph{\small The above is a plot of the residuals of the SVD unfolded Data with respect to the MC model used in the unfolding. The shaded regions represent 1 and 2 SD based on a per-bin Poisson model assumption for the MC's Truth counts.}}
  \label{D2Mres}
\end{figure}

\begin{figure}[!ht]
    \centering
```{r, echo = FALSE, fig.align='center', fig.height=3}
ggplot() + theme_bw() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_polygon(data = areadat2,
            mapping = aes(x=X,y=sign(Y)*sqrt(abs(Y)),fill=label,color=label),
            linetype = "dotted") +
  geom_polygon(data = areadat2,
            mapping = aes(x=X,y=2*sign(Y)*sqrt(abs(Y)),fill=label,color=label),
            linetype = "dotted") +
  geom_errorbar(data = diffdat2,
                mapping = aes(x=X,ymin=CVCounts-DataTruth-SD,
                              ymax=CVCounts-DataTruth+SD),width=0.2) +
  geom_point(data = diffdat2,
             mapping = aes(x=X,y=CVCounts-DataTruth,fill=label), shape = 21,size=1.5) +
  geom_point(data = diffdat2,
             mapping = aes(x=X,y=CVCounts-DataTruth,color=label), shape = 16,size=1) +
  scale_color_manual(name = "Pairings",
                     values = c("Data 1 unfolded with MC 1"="#5e81b5",
                                "Data 1 unfolded with MC 2"="#5e81b5",
                                "Data 2 unfolded with MC 1"="#e19c24",
                                "Data 2 unfolded with MC 2"="#e19c24")) +
  scale_fill_manual(name = "Pairings",
                    values = c("Data 1 unfolded with MC 1"=alpha("#5e81b5",0.3),
                               "Data 1 unfolded with MC 2"=alpha("#e19c24",0.3),
                               "Data 2 unfolded with MC 1"=alpha("#5e81b5",0.3),
                               "Data 2 unfolded with MC 2"=alpha("#e19c24",0.3))) +
  scale_x_continuous(expand = c(0,0), breaks = seq(0,30,3),
                     name = "X (Truth)") +
  scale_y_continuous(name = TeX("$\\hat{\\mu}_i-\\mu^{Data}_i$")) +
  theme(axis.text.x = element_text(size = 8)) +
  facet_wrap( ~label,nrow=2) +
  labs(title = "Residual of Unfolded Data to Data Truth")
```
\caption{\emph{\small The above is a plot of the residuals of the SVD unfolded Data with respect to the same Data's Truth counts. SVD unnfolded Data from Model 1 is only compared to Model 1 Data's Truth counts, and similarly for Model 2. The shaded regions represent 1 and 2 SD based on a per-bin Poisson model assumption for the Data's Truth counts.}}
  \label{D2Dres}
\end{figure}

\begin{figure}[!ht]
    \centering
```{r, echo = FALSE, fig.align='center', fig.height=3}
dChi_plot2
```
\caption{\emph{\small For adjacent tested values of $\tau$, the above plot shows the absolute value of the change in the weighted sums of squares of the residuals of the unfolded Data with respect to its true counts. The minimum value for the sum of the pairings' $\chi^2$'s indicates where the $\sum\chi^2$ shifts from decreasing with $\tau$ to increasing with $\tau$.}}
  \label{dchi2}
\end{figure}

\section{Hilbert Spaces}\label{appHilbert}

Hilbert spaces are a prominent feature in the field of
functional analysis. They see significant application in partial
differential equations, quantum mechanics, and signal processing, where
they are commonly implemented in the performance of Fourier analysis.
Mathematically they represent an extension beyond the real and complex
geometric-like vector spaces developed by earlier generalizations of
Euclidean spaces in the 19th century. Developments in real analysis at
the beginning of the 20th century lead to spaces of functions and
sequences being conceptualized as linear spaces in their own right.

As extensions of previously understood spaces they necessarily exist at
the intersection of several other important spaces that aught to be
understood beforehand. With that said, the following definitions come
from Rudin in \cite{Rudin1991}. To start, a \textbf{vector space}, as
defined here, consists of a set $X$ of vectors for which addition and
scalar multiplication are defined such that for all $x,y,z\in X$ and any
complex number $\alpha\in\mathbb{C}$
\begin{enumerate}
  \item there exists a vector in $X$ such that
    \begin{enumerate}
      \item addition is commutative: $x+y=y+x$,
      \item addition is associative: $x+(y+z)=(x+y)+z$,
    \end{enumerate}
  \item $\alpha x$ exists in $X$ such that $1x=x$, $0x=0$ (the zero vector), and multiplication is distributive:
    \begin{enumerate}
      \item $\alpha(\beta x)=(\alpha\beta x)$,
      \item $\alpha(x+y)=\alpha x+\alpha y$, and
      \item $(\alpha +\beta)x=\alpha x+\beta x$.
    \end{enumerate}
\end{enumerate}
The range of $\alpha$ above describes a complex vector space. If
$\alpha$ is restricted to the reals $\mathbb{R}$, then $X$ is considered
a real vector space. Note that vector spaces include more than just
traditional coordinate-style vectors, but also include function spaces
such as the vector space of all polynomials with degree of at most $n$,
which has the basis $\{1,x,x^2,\dots,x^{n-1},x^n\}$.

Typically associated in applications, metric spaces form a another
relevant set of spaces that has some significant overlap with the vector
spaces. A space $X$ is said to be a \textbf{metric space} if for all
$x,y\in X$ there exists an operator $d(x,y)$ that maps them to a
nonnegative real number that defines their distance from each other
within $X$. The properties of this operator are
\begin{enumerate}
  \item $0\leq d(x,y)<\infty$ for all $x$ and $y\in X$,
  \item $d(x,y)=0$ iff $x=y$,
  \item $d(x,y)=d(y,x)$ for all $x$ and $y\in X$,
  \item $d(x,z)\leq d(x,y)+d(y,z)$ for all $x$, $y$, $z\in X$.
\end{enumerate}
For a metric space $X$, the distance operator $d$ is referred to as the
metric on $X$. The intersection of the vector and metric spaces form the
set of normed spaces. As an extension of the conditions thus far, a
space $X$ is a \textbf{normed space} if $\forall x\in X$ there exists a
nonnegative real number $\vert\vert x\vert\vert$, called the
\textbf{norm} of $x$ such that
\begin{enumerate}
  \item $\vert\vert x+y\vert\vert\leq\vert\vert x\vert\vert+\vert\vert y\vert\vert\;\forall x,y\in X$,
  \item $\vert\vert\alpha x\vert\vert = \vert\alpha\vert\,\vert\vert x\vert\vert$ if $x\in X$ and $\alpha$ is a scalar,
  \item $\vert\vert x\vert\vert>0$ if $x\neq 0$.
\end{enumerate}
Such a set is said to be \textbf{complete} if every
\textbf{Cauchy sequence} in $X$ converges to a point in $X$. A Cauchy
sequence in a metric space $X$ is any sequence $\{x_n\}$ that
$\forall\varepsilon>0$ there exists an integer $N$ such that
$d(x_m,x_n)<\varepsilon$ when $m>N$ and $n>N$. A quick example of this
is the sequence defined by $x_n=\sqrt{n}$. For some starting $x_m$ and
$x_n$ where $m-n=\delta$, we have 
\begin{align}
d(x_m,x_n)
  &=\sqrt{m}-\sqrt{n}\nonumber\\
  &=\sqrt{n+\delta}-\sqrt{n}\nonumber\\
  &=(\sqrt{n+\delta}-\sqrt{n})\frac{\sqrt{n+\delta}+\sqrt{n}}{\sqrt{n+\delta}+\sqrt{n}}\nonumber\\
  &=\frac{n+\delta-n}{\sqrt{n+\delta}+\sqrt{n}}\nonumber\\
  &=\frac{\delta}{\sqrt{n}(\sqrt{1+\delta/n}+\sqrt{1})}\nonumber\\
  &<\frac{1}{\sqrt{n}}\left(\frac{\delta}{2}\right)<\varepsilon\nonumber\\
  \implies n &> \left(\frac{\delta}{2\varepsilon}\right)^2.\nonumber
\end{align}
Noting that for constant $\delta$ the limit of
$\frac{1}{\sqrt{n}}\left(\frac{\delta}{2}\right)$ as
$n\longrightarrow\infty$ is the zero vector (the point of convergence)
would also be sufficient to show that $x_n=\sqrt{n}$ is a Cauchy
sequence.

Incidentally, a normed vector space that is complete as defined here
meets the definition of a \textbf{Banach space}. An additional subset of
the normed vector spaces consists of those spaces in which for all
$x,y\in X$ there exists a real or complex number $\langle x,y\rangle$
defined by an operator called the \textbf{inner product}. For all
$x,y,z\in X$ this operation must satisfy
\begin{enumerate}
  \item $\langle x,y\rangle=\langle y,x\rangle^*$ (where the ${}^*$ represents the complex conjugate),
  \item $\langle x+y,z\rangle=\langle x,z\rangle+\langle y,z\rangle$,
  \item $\langle \alpha x,y\rangle=\alpha\langle x,y\rangle$ (for $\alpha\in\mathbb{C}$),
  \item $\langle x,x\rangle\geq0$, and
  \item $\langle x,x\rangle=0$ iff $x=0$.
\end{enumerate}

A space that satisfies these requirements forms an
\textbf{inner product space}, and the inner product defined in such a
space relates to the form of its norm, such that
$\vert\vert x\vert\vert=\langle x,x\rangle^{1/2}$. Finally, at the
intersection of Banach spaces and inner product spaces are the Hilbert
spaces. I.e. a \textbf{Hilbert space} is a complete vector space with an
inner product defined by its norm.

A commonly presented example of a Hilbert space is the $L^2$ function space, which consists
of functions that are square integrable, i.e. if
$f(x)\in L^2\implies \vert\vert f(x)\vert\vert^2=\int_\chi\vert f(x)\vert^2dx<\infty$,
where $\chi$ is the domain of $x$. The subset $L^2[-\pi,\pi]$, where
$\chi=[-\pi,\pi]$, has the well known Fourier series as a basis, which
is commonly written such that for $f(x)\in L^2[-\pi,\pi]$
\begin{align}
  f(x)=\frac{a_0}{2} + \sum_{n=1}^\infty\left[a_n\cos(nx)+b_n\sin(nx)\right],\nonumber
\end{align} 
where 
$$\begin{matrix}
  a_n=\frac{1}{\pi}\int_{-\pi}^\pi f(x)\cos(nx)dx \\ \text{and} \\ b_n=\frac{1}{\pi}\int_{-\pi}^\pi f(x)\sin(nx)dx.
\end{matrix}$$
Verification that this basis meets all the requirements
laid out here for the basis of a Hilbert Space is beyond the scope of this paper.

\begin{figure}
  \centering
```{r, echo=F, fig.height=3.6, fig.width=3.6, fig.align='center'}
venndiagram <- tibble(X0 = c( 0.0,0.0,
                             -1.3,0.9),
                      Y0 = c(-0.75, 0.75,
                              0.00,-1.05),
                      R = c(4.50,4.5,
                            2.75,2.3),
                      space = c("Vector Space",
                                "Metric Space",
                                "Inner Product Space",
                                "Banach Space")) %>%
  mutate(across(space, factor, 
                levels = c("Vector Space",
                           "Metric Space",
                           "Inner Product Space",
                           "Banach Space")))
venntext <- tibble(X = c(0.0, 1.90,
                         0.0,-2.25,
                         1.7, 0.00),
                   Y = c( 4.45, 2.10,
                         -4.45, 0.75,
                         -1.70,-0.60),
                   theta = c( 0,-35,
                              0, 50,
                             55,  0),
                   text = c("Vector Space\n(vectors)",
                            "Normed Space\n(length)",
                            "Metric Space\n(distance)", 
                            "Inner Product\nSpace\n(angle/orthogonality)",
                            "Banach Space\n(completeness)",
                            "Hilbert\nSpace"))
                           

ggplot() +
  coord_fixed() + theme_void() +
  geom_circle(data = venndiagram,
              mapping = aes(x0 = X0, y0 = Y0, r = R,
                            fill = space)) +
  geom_text(data = venntext,
            mapping = aes(x = X, y = Y, angle = theta,
                          label = text), size = 3.5) +
  theme(legend.position = "none") +
  scale_fill_manual(values = c(alpha("#b91500",0.35),
                               alpha("#45a500",0.35),
                               alpha("#ef6800",0.25),
                               alpha("#0000e0",0.25)))
  
```
  \caption{\emph{A Venn diagram representing the intersection and nesting of the spaces described in Appendix \ref{appHilbert}.}}
  \label{spaceVenn}
  \vspace{-20pt}
\end{figure}

\section{Description of Simulations} \label{simuApp}

The Cauchy processes specifically are of the form
\begin{align}
  X_{1,i}&\sim \text{Cauchy}(11,4)\nonumber\\
  X_{2,i}&\sim \text{Cauchy}(18,4)\nonumber\\
  X_{3,i}&\sim \text{Cauchy}(14,5)\nonumber
\end{align} 
The probabilities under Model 1 (in which only the first two processes take place) is $\bm{p}=\{0.3,0.7\}$. The probabilities governing Model 2 are generated from $\bm{p}$ by
\begin{align}
  \bm{p'}
    &=\bm{\Lambda}\bm{p}
    =\begin{pmatrix}0.75 & 0 \\ 0 & 0.75 \\ 0.6 & 0.1\end{pmatrix}\begin{pmatrix}0.3\\0.7\end{pmatrix}
    =\begin{pmatrix}0.225 \\ 0.525 \\ 0.25 \end{pmatrix}.\nonumber
\end{align}
The individual weighted distributions contributing to each model's physical processes are shown below in Figure \ref{IndividualDistros}.
\begin{figure}[!ht]
    \centering
```{r, echo = FALSE, fig.align='center',fig.height=3}

ggplot() + 
  theme_bw() +
  geom_line(data = fpx_truth1,
            mapping = aes(x = X, 
                          y = nsim_data*Density,
                      color = Process),
            alpha = 0.9) +
  geom_line(data = fpx_truth2,
            mapping = aes(x = X, 
                          y = nsim_data*Density,
                   linetype = Process),
            color = rep(c("#5e81b5","#e19c24","#8fb032"),each=3001),
            alpha = 0.9) +
  scale_color_manual(name = "Model 1",
                     labels = c(TeX("$X_{1,i}\\;\\sim\\;Cauchy(11,4)$"),
                                TeX("$X_{2,i}\\;\\sim\\;Cauchy(18,4)$")),
                     breaks = c("Process 1",
                                "Process 2"),
                     values = c("Process 1" = "#5e81b5",
                                "Process 2" = "#e19c24")) + 
  scale_linetype_manual(name = "Model 2",
                        labels = c(TeX("$X_{1,i}\\;\\sim\\;Cauchy(11,4)$"),
                                   TeX("$X_{2,i}\\;\\sim\\;Cauchy(18,4)$"),
                                   TeX("$X_{3,i}\\;\\sim\\;Cauchy(14,4)$")),
                        breaks = c("Process 1",
                                   "Process 2",
                                   "Process 3"),
                        values = c("Process 1" = "dashed",
                                   "Process 2" = "dashed",
                                   "Process 3" = "dashed")) +
  guides(color = guide_legend(title = "Model 1", order = 1, 
                              override.aes = list(shape = "solid",
                                                  alpha = 0.9),
                              title.vjust = unit(-0.2, "pt")),
         linetype = guide_legend(title = "Model 2", order = 2, 
                                 override.aes = list(color = c("#5e81b5","#e19c24","#8fb032"),
                                                     alpha = 0.9),
                                 title.vjust = unit(-0.2, "pt"))) +
  scale_x_continuous("X (Truth)",
                     breaks = seq(0,30,3), 
                     limits = c(0,30), 
                     expand = c(0,0)) +
  scale_y_continuous("Counts",
                     limits = c(0,fmax_count), expand = c(0,0),
                     breaks = seq(0,ceiling(fmax_count),100)) +
  labs(title = "Individual Scaled Physics Process Distributions") +
  theme(legend.text = element_text(size = 8),
        legend.title = element_text(size = 9),
        legend.justification = c("left", "top"),
        legend.box.just = "left",
        legend.spacing.y = unit(1, "pt"),
        legend.key.height = unit(10,"points"),
        legend.margin = margin(1,5,5,5,"pt"))
```
\caption{\emph{\small The figure contains the individual underlying distributions guiding each contributing physics process per model. They are weighted to reflect their relative contributions.}}
  \label{IndividualDistros}
\end{figure}

The effects of detector smearing is represented by i.i.d random variables generated by the conditional Gaussian process
$$\varepsilon_i\sim N\left(\mu(X_i),\sigma(X_i)^2\right),$$
the mean and variance of which are functions defined by 
\begin{align}
  \mu(X_i=x)&=-x^{1/4}\;\;\text{and}\nonumber\\
  \sigma(X_i=x)&=\log\left(\frac{x+10}{4}\right).\nonumber
\end{align} 
The efficiency is similarly conditional on $X_i$, and is
modeled here as a Bernoulli process with i.i.d random variables
$\epsilon_i\sim\text{Bernoulli}\big(p(X_i)\big)$, where the average
detection rate (when $\epsilon_i=1$) is a function of the form
\begin{align}
  p(X_i=x)=1-e^{-\sqrt{x}/4}.\nonumber
\end{align}

\section{Singular Value Decomposition} \label{app:svd}

The below definitions were provided by the Wikipedia page for Singular Value Decomposition \cite{wiki:svd} or by a page it directly links to.

Singular value decomposition involves the factorization of an $N\times M$ matrix $\bm{A}$ into
\begin{align}
  \bm{A}=\bm{USV}^T,\label{eq:svd}
\end{align}
where $\bm{U}$ and $\bm{V}$ are respectively $N\times N$ and $M\times M$ unitary matrices and $\bm{S}$ is an $N\times M$ rectangular diagonal matrix consisting of non-negative real numbers. 

A \textbf{unitary} matrix is any matrix in which its conjugate transpose is also its inverse, i.e. $\bm{U}^\dagger=\bm{U}^{-1}$. When the contents of a unitary matrix are all real it is also referred to as an \textbf{orthogonal} or \textbf{orthonormal} matrix. A \textbf{rectangular diagonal matrix} is simply a rectangular (not necessarily square) in which all of its off diagonal components are $0$, i.e. $S_{ij}=0$ if $i\neq j$. The matrices $\bm{U}$ and $\bm{V}$ can be thought of as two rotation matrices that sandwich a rescaling matrix $\bm{S}$.

```{r, echo=FALSE, eval=FALSE}
\section{Bin-by-bin Unfolding}

In this naive approach a multiplicative \textbf{correction factor} $C_i$ is
applied to the observed number of signal events $n_i$ for each bin to
produce the estimator of $\mu_i$ \cite{Cowan1998}, 
\begin{align}
  \hat{\mu}_i &= C_in_i.\label{eq:binest}
\end{align} 
The correction factors are determined by taking the
respective ratios of a bin's MC simulated truth signal event counts
$\mu_i^{\text{MC}}$ to its MC simulated reconstructed signal event
counts $\nu_i^{\text{MC}}$,
\begin{align}
  C_i=\frac{\mu_i^{\text{MC}}}{\nu_i^{\text{MC}}}.\label{eq:cfact}
\end{align}
The covariance matrix $\bm{\Sigma}_\mu$ of this estimator derives
naturally from Equations \eqref{eq:cov} and \eqref{eq:binest}, with
components 
\begin{align}
  \Sigma^\mu_{ij}
    &=\text{Cov}[\hat\mu_i,\hat\mu_j]\nonumber\\
    &=C_iC_j\text{Cov}[n_i,n_j]\nonumber\\
    &=C_i^2\delta_{ij}\nu_i\nonumber\\
    &=\left(\frac{\mu_i^{\text{MC}}}{\nu_i^{\text{MC}}}\right)^2\delta_{ij}\nu_i.\label{eq:bincov}
\end{align} 
The expectation value of the estimate can be calculated
easily enough as well, and with it the bias 
\begin{align}
  \text{Bias}[\hat{\mu}_i]
    &=E_i[\hat{\mu}_i]-\mu_i\nonumber\\ 
    &=C_iE[n_i]-\mu_i\nonumber\\ 
    &=\frac{\mu_i^{\text{MC}}}{\nu_i^{\text{MC}}}\nu_i-\mu_i\nonumber\\
    &=\left(\frac{\mu_i^{\text{MC}}}{\nu_i^{\text{MC}}}-\frac{\mu_i}{\nu_i}\right)\nu_i.\label{eq:binbias}
\end{align}
```

```{r, echo = F, fig.align='center',fig.height=3, eval=FALSE}
BinByBin_bins <- tibble(Model = c(rep(c("Model 1","Model 2"),each=60),
                              rep(c("Model 1","Model 2"),each=30)),
                   `Assumed Model` = rep(rep(c("Model 1","Model 2"),
                                              each=30),3),
                   LBin = rep(0:29,6), Bin = rep(1:30,6),
                   LCounts = c(((mcTruth1$Counts/mcReco1$Counts[1:30])*
                                  dataReco1$Counts[1:30])[c(1,1:29)],
                               ((mcTruth2$Counts/mcReco2$Counts[1:30])*
                                  dataReco1$Counts[1:30])[c(1,1:29)],
                               ((mcTruth1$Counts/mcReco1$Counts[1:30])*
                                  dataReco2$Counts[1:30])[c(1,1:29)],
                               ((mcTruth2$Counts/mcReco2$Counts[1:30])*
                                  dataReco2$Counts[1:30])[c(1,1:29)],
                               mcTruth1$Counts[c(1,1:29)]/dmrat,
                               mcTruth2$Counts[c(1,1:29)]/dmrat),
                   Counts = c((mcTruth1$Counts/mcReco1$Counts[1:30])*
                                dataReco1$Counts[1:30],
                              (mcTruth2$Counts/mcReco2$Counts[1:30])*
                                dataReco1$Counts[1:30],
                              (mcTruth1$Counts/mcReco1$Counts[1:30])*
                                dataReco2$Counts[1:30],
                              (mcTruth2$Counts/mcReco2$Counts[1:30])*
                                dataReco2$Counts[1:30],
                              mcTruth1$Counts/dmrat,
                              mcTruth2$Counts/dmrat),
                   Name = c(rep(c("Model 1 Data",
                                  "Model 2 Data"),each=60),
                            rep("MC",60)))



BinByBin_bins_res <- BinByBin_bins %>% 
  filter(Name == "Model 1 Data" | Name == "Model 2 Data") %>%
  mutate(`Assumed Model` = rep(rep(c("Model 1 Residuals",
                                      "Model 2 Residuals"),each=30),2),
         LCounts = (rep(c(0,mcTruth1$Counts[1:29]/dmrat,
                          0,mcTruth2$Counts[1:29]/dmrat),2) - LCounts),
         Counts = rep(c(mcTruth1$Counts/dmrat,
                        mcTruth2$Counts/dmrat),2) - Counts)


ggplot() + 
  theme_bw() + 
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_segment(data = BinByBin_bins %>% 
                 filter(Name == "MC"),
               mapping = aes(x = LBin,
                             y = Counts,
                          xend = Bin,
                          yend = Counts),
               alpha = 0.6) +
  geom_segment(data = BinByBin_bins %>% 
                 filter(Name == "MC"),
               mapping = aes(x = LBin,
                             y = LCounts,
                          xend = LBin,
                          yend = Counts),
               alpha = 0.6) +
  geom_rect(data = BinByBin_bins %>%
              mutate(ymin = Counts - 
                       rep(sqrt(c((mcTruth1$Counts^2/
                                 mcReco1$Counts[1:30]),
                              (mcTruth2$Counts^2/
                                 mcReco2$Counts[1:30]))),3),
                     ymax = Counts + 
                       rep(sqrt(c((mcTruth1$Counts^2/
                                 mcReco1$Counts[1:30]),
                              (mcTruth2$Counts^2/
                                 mcReco2$Counts[1:30]))),3),
                     xmin = LBin, xmax = Bin),
            mapping = aes(xmin=xmin,ymin=ymin,
                          xmax=xmax,ymax=ymax,
                          fill=Name),alpha=0.2) +
  geom_segment(data = BinByBin_bins %>% 
                 filter(Name == "Model 1 Data" | Name == "Model 2 Data"),
               mapping = aes(x = LBin,
                             y = Counts,
                          xend = Bin,
                          yend = Counts,
                         color = Name),
               alpha = 0.7) +
  geom_segment(data = BinByBin_bins %>% 
                 filter(Name == "Model 1 Data" | Name == "Model 2 Data"),
               mapping = aes(x = LBin,
                             y = LCounts,
                          xend = LBin,
                          yend = Counts,
                         color = Name),
               alpha = 0.7) +
  scale_x_continuous("X (Truth)",
                     breaks = seq(0,30,by=3), 
                       limits = c(0,30), 
                       expand = c(0,0)) +
  scale_color_manual(labels = c("Model 1","Model 2"),
                     breaks = c("Model 1 Data","Model 2 Data"),
                     values = c("#5e81b5","#e19c24")) +
  scale_fill_manual(labels = c("Model 1","Model 2"),
                     breaks = c("Model 1 Data","Model 2 Data"),
                     values = c(alpha("#5e81b5",0.2),
                                alpha("#e19c24",0.2),NA)) +
  guides(fill = guide_legend("Data", override.aes = list(fill=c(alpha("#5e81b5",0.2),alpha("#e19c24",0.2)))),
         color = guide_legend("Data")) +
  facet_wrap(`Assumed Model` ~ ., ncol = 2, dir = "v")
```

```{r, echo = F, fig.align='center',fig.height=3, eval=FALSE}
ggplot() + 
  theme_bw() + 
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_rect(data = BinByBin_bins_res %>%
              mutate(ymin = Counts - 
                       rep(sqrt(c((mcTruth1$Counts^2/
                                 mcReco1$Counts[1:30]),
                              (mcTruth2$Counts^2/
                                 mcReco2$Counts[1:30]))),2)/dmrat,
                     ymax = Counts + 
                       rep(sqrt(c((mcTruth1$Counts^2/
                                 mcReco1$Counts[1:30]),
                              (mcTruth2$Counts^2/
                                 mcReco2$Counts[1:30]))),2)/dmrat,
                     xmin = LBin, xmax = Bin),
            mapping = aes(xmin=xmin,ymin=ymin,
                          xmax=xmax,ymax=ymax,
                          fill=Name),alpha=0.2) +
  geom_segment(data = BinByBin_bins_res[1:120,],
               mapping = aes(x = LBin,
                             y = Counts,
                          xend = Bin,
                          yend = Counts,
                         color = Name),
               alpha = 0.7) +
  geom_segment(data = BinByBin_bins_res[1:120,],
               mapping = aes(x = LBin,
                             y = LCounts,
                          xend = LBin,
                          yend = Counts,
                         color = Name),
               alpha = 0.7) +
  scale_x_continuous("X (Truth)",
                     breaks = seq(0,30,by=3), 
                       limits = c(0,30), 
                       expand = c(0,0)) +
  scale_color_manual(labels = c("Model 1","Model 2"),
                     breaks = c("Model 1 Data","Model 2 Data"),
                     values = c("#5e81b5","#e19c24")) +
  scale_fill_manual(labels = c("Model 1","Model 2"),
                     breaks = c("Model 1 Data","Model 2 Data"),
                     values = c(alpha("#5e81b5",0.3),
                                alpha("#e19c24",0.3))) +
  guides(fill = guide_legend("Data", override.aes = 
                               list(fill=c(alpha("#5e81b5",0.2),
                                           alpha("#e19c24",0.2)))),
         color = guide_legend("Data")) +
  facet_wrap(`Assumed Model` ~ ., ncol = 2, dir = "v")
```

```{r, echo = F, fig.align='center',fig.height=3, eval=FALSE}
biases <- 
  c((mcTruth1$Counts/mcReco1$Counts[1:30] - 
       (bins_expected1 %>% filter(Treatment == "Truth") %>% pull(Counts))/
       (bins_expected1 %>% filter(Treatment == "Measured") %>% pull(Counts))) *
      (bins_expected1 %>% filter(Treatment == "Measured") %>% pull(Counts))/
      (bins_expected1 %>% filter(Treatment == "Truth") %>% pull(Counts)),
    (mcTruth2$Counts/mcReco2$Counts[1:30] - 
       (bins_expected1 %>% filter(Treatment == "Truth") %>% pull(Counts))/
       (bins_expected1 %>% filter(Treatment == "Measured") %>% pull(Counts))) *
      (bins_expected1 %>% filter(Treatment == "Measured") %>% pull(Counts))/
      (bins_expected1 %>% filter(Treatment == "Truth") %>% pull(Counts)),
    (mcTruth1$Counts/mcReco1$Counts[1:30] - 
       (bins_expected2 %>% filter(Treatment == "Truth") %>% pull(Counts))/
       (bins_expected2 %>% filter(Treatment == "Measured") %>% pull(Counts))) *
      (bins_expected2 %>% filter(Treatment == "Measured") %>% pull(Counts))/
      (bins_expected2 %>% filter(Treatment == "Truth") %>% pull(Counts)),
    (mcTruth2$Counts/mcReco2$Counts[1:30] - 
       (bins_expected2 %>% filter(Treatment == "Truth") %>% pull(Counts))/
       (bins_expected2 %>% filter(Treatment == "Measured") %>% pull(Counts))) *
      (bins_expected2 %>% filter(Treatment == "Measured") %>% pull(Counts))/
      (bins_expected2 %>% filter(Treatment == "Truth") %>% pull(Counts)),
    ((bins_expected1 %>% filter(Treatment == "Truth") %>% pull(Counts))/
       (bins_expected1 %>% filter(Treatment == "Measured") %>% pull(Counts))-
       (bins_expected2 %>% filter(Treatment == "Truth") %>% pull(Counts))/
       (bins_expected2 %>% filter(Treatment == "Measured") %>% pull(Counts))) *
      (bins_expected2 %>% filter(Treatment == "Measured") %>% pull(Counts))/
      (bins_expected2 %>% filter(Treatment == "Truth") %>% pull(Counts)),
    ((bins_expected2 %>% filter(Treatment == "Truth") %>% pull(Counts))/
       (bins_expected2 %>% filter(Treatment == "Measured") %>% pull(Counts))-
       (bins_expected1 %>% filter(Treatment == "Truth") %>% pull(Counts))/
       (bins_expected1 %>% filter(Treatment == "Measured") %>% pull(Counts))) *
      (bins_expected1 %>% filter(Treatment == "Measured") %>% pull(Counts))/
      (bins_expected1 %>% filter(Treatment == "Truth") %>% pull(Counts)))

biasSD <- c(sqrt(((mcTruth1$Counts/mcReco1$Counts[1:30])*
                    (bins_expected1 %>% 
                       filter(Treatment == "Measured") %>% 
                       pull(Counts))))/
                   (bins_expected1 %>% 
                      filter(Treatment == "Truth") %>% 
                      pull(Counts)),
            sqrt(((mcTruth2$Counts/mcReco2$Counts[1:30])*
                    (bins_expected1 %>% 
                       filter(Treatment == "Measured") %>% 
                       pull(Counts))))/
                   (bins_expected1 %>% 
                      filter(Treatment == "Truth") %>% 
                      pull(Counts)),
            sqrt(((mcTruth1$Counts/mcReco1$Counts[1:30])*
                    (bins_expected2 %>% 
                       filter(Treatment == "Measured") %>% 
                       pull(Counts))))/
                   (bins_expected2 %>% 
                      filter(Treatment == "Truth") %>% 
                      pull(Counts)),
            sqrt(((mcTruth2$Counts/mcReco2$Counts[1:30])*
                    (bins_expected2 %>% 
                       filter(Treatment == "Measured") %>% 
                       pull(Counts))))/
                   (bins_expected2 %>% 
                      filter(Treatment == "Truth") %>% 
                      pull(Counts)))


BinByBin_bias <- tibble(x = c(rep(c(0,rep(1:29,each=2),30,
                                     30,rep(29:1,each=2),0),4),
                               rep(c(0,rep(1:29,each=2),30),4)),
                         y = c(rep(biases[1:30]+biasSD[1:30],each=2),
                               rep(biases[30:1]-biasSD[30:1],each=2),
                               rep(biases[31:60]+biasSD[31:60],each=2),
                               rep(biases[60:31]-biasSD[60:31],each=2),
                               rep(biases[61:90]+biasSD[61:90],each=2),
                               rep(biases[90:61]-biasSD[90:61],each=2),
                               rep(biases[91:120]+biasSD[91:120],each=2),
                               rep(biases[120:91]-biasSD[120:91],each=2),
                               rep(biases[1:120],each=2)),
                         `Expectation Value` = 
                           c(rep("Model 1\nExpectation Value",240),
                             rep("Model 2\nExpectation Value",240),
                             rep("Model 1\nExpectation Value",120),
                             rep("Model 2\nExpectation Value",120)),
                         `MC Truth` = c(rep(c(rep("Model 1",120),
                                              rep("Model 2",120)),2),
                                        rep(c(rep("Model 1",60),
                                              rep("Model 2",60)),2)),
                         MCiEXPj = c(rep(c("MC1EXP1","MC2EXP1",
                                           "MC1EXP2","MC2EXP2"),each=120),
                                     rep(c("MC1EXP1","MC2EXP1",
                                           "MC1EXP2","MC2EXP2"),each=60)),
                         Signal = c(rep("Error",480),rep("CV",240)))

ggplot() + theme_bw() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_polygon(data = BinByBin_bias %>% filter(Signal == "Error"),
               mapping = aes(x=x, y=y, color=`MC Truth`, fill=`MC Truth`),
               linetype = "dotted") +
  geom_line(data = BinByBin_bias %>% filter(Signal == "CV"),
               mapping = aes(x=x, y=y, color=`MC Truth`)) +
  scale_color_manual(values = c("#5e81b5","#e19c24","#eb6235","#8fb032")) +
  scale_fill_manual(values = c(alpha("#5e81b5",0.3),alpha("#e19c24",0.3),
                               alpha("#eb6235",0.3),alpha("#8fb032",0.3))) +
  scale_x_continuous("X (Truth)",
                     breaks = seq(0,30,by=3), 
                       limits = c(0,30), 
                       expand = c(0,0)) +
  scale_y_continuous(
    name = TeX(r"($\frac{\mu_i^{{MC}}/\mu_i}{\nu_i^{{MC}}/\nu_i}-1$)"),
    breaks = seq(-0.15,0.15,0.05),
    labels = c("-0.15","-0.10","-0.05","0.00","0.05","0.10","0.15"),
    limits = c(-0.175,0.175), 
    expand = c(0,0)) +
  facet_wrap(~`Expectation Value`) +
  theme(axis.text.x = element_text(size = 8))

```
